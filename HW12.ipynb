{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1gw8Bht2NYYb"},"outputs":[],"source":["# Change to runtime to GPU-T4"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"HjVKFH1Z_66R","colab":{"base_uri":"https://localhost:8080/","height":188,"referenced_widgets":["e39d902d85744551a959f388ea132f60","d7a37d89c4ba4e83b7db2c898376e695","022ae34744cf44e4b7a327e85db49297","64fbf831e77d4ee0b2b0b4db133e212e","bcb71150d9f1483fb75603684dc8d897","366bdae22a4047149ea7e85abcc1fb9a","84c5e2bf81154c92ba27f33da78238e3","3c5e9d2634be4a7eb39e6883cef86df1","0940e01a7b2a4922bd37669ccafc43b0","ccc3d7a7c0be47f6a17c0f4c909c450b","8d99cd73d0e842e38e9ea56d3fb00483"]},"executionInfo":{"status":"ok","timestamp":1746242629806,"user_tz":240,"elapsed":26645,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"6431a122-c95e-4bef-9903-0c68e5e61c1e","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e39d902d85744551a959f388ea132f60"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":1}],"source":["# DONOT CHANGE THIS CODE\n","# Mount your drive\n","\n","import os\n","import gdown\n","import subprocess\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","model_name=\"gemma-transformers-1.1-2b-it-v1\"\n","\n","GOOGLE_DRIVE_PATH = os.path.join(\"/content/drive/MyDrive/\",model_name)\n","REPO_ID = \"MangalamSahai/24789HWs\"\n","if not os.path.exists(os.path.join(\"/content/drive/MyDrive/\",model_name)):\n","  os.mkdir(os.path.join(\"/content/drive/MyDrive/\",model_name))\n","\n","\n","from huggingface_hub import snapshot_download\n","\n","snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir = GOOGLE_DRIVE_PATH)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ehPrNLNHp1t3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242712965,"user_tz":240,"elapsed":80898,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"fd6b91c7-caf1-45ce-a9d0-f506f96ac338","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Running in Google Colab, installing requirements.\n","Collecting PyMuPDF\n","  Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n","Downloading pymupdf-1.25.5-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: PyMuPDF\n","Successfully installed PyMuPDF-1.25.5\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n","Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n","Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m113.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.45.5\n"]}],"source":["## DONOT CHANGE THE CODE\n","\n","import os\n","\n","if \"COLAB_GPU\" in os.environ:\n","    print(\"[INFO] Running in Google Colab, installing requirements.\")\n","    #!pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n","    !pip install PyMuPDF # for reading PDFs with Python\n","    !pip install tqdm # for progress bars\n","    !pip install sentence-transformers # for embedding models\n","    !pip install accelerate # for quantization model loading\n","    !pip install bitsandbytes # for quantizing models (less storage space)\n","    #!pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"]},{"cell_type":"markdown","metadata":{"id":"BxV7PGcNp1t9"},"source":["##To understand how we obtained embedding file. Please refer to the recitation."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"fyZf54GIp1t9","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1746242860988,"user_tz":240,"elapsed":548,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"6b4e52e0-caa2-4634-c783-c51a657ef0d7","collapsed":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number                                     sentence_chunk  \\\n","0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n","1          -38  Human Nutrition: 2020 Edition by University of...   \n","2          -37  Contents Preface University of Hawai‘i at Māno...   \n","3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n","4          -35  The Cardiovascular System University of Hawai‘...   \n","\n","   chunk_char_count  chunk_word_count  chunk_token_count  \\\n","0               308                42              77.00   \n","1               210                30              52.50   \n","2               766               114             191.50   \n","3               941               142             235.25   \n","4               998               152             249.50   \n","\n","                                           embedding  \n","0  [ 6.74242675e-02  9.02281404e-02 -5.09548886e-...  \n","1  [ 5.52156419e-02  5.92139773e-02 -1.66167244e-...  \n","2  [ 2.79801842e-02  3.39813754e-02 -2.06426680e-...  \n","3  [ 6.82566911e-02  3.81275006e-02 -8.46854132e-...  \n","4  [ 3.30264494e-02 -8.49763490e-03  9.57159605e-...  "],"text/html":["\n","  <div id=\"df-96c7b39a-34b8-4eb8-aef4-d0c971f61f39\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>sentence_chunk</th>\n","      <th>chunk_char_count</th>\n","      <th>chunk_word_count</th>\n","      <th>chunk_token_count</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-39</td>\n","      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n","      <td>308</td>\n","      <td>42</td>\n","      <td>77.00</td>\n","      <td>[ 6.74242675e-02  9.02281404e-02 -5.09548886e-...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-38</td>\n","      <td>Human Nutrition: 2020 Edition by University of...</td>\n","      <td>210</td>\n","      <td>30</td>\n","      <td>52.50</td>\n","      <td>[ 5.52156419e-02  5.92139773e-02 -1.66167244e-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-37</td>\n","      <td>Contents Preface University of Hawai‘i at Māno...</td>\n","      <td>766</td>\n","      <td>114</td>\n","      <td>191.50</td>\n","      <td>[ 2.79801842e-02  3.39813754e-02 -2.06426680e-...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-36</td>\n","      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n","      <td>941</td>\n","      <td>142</td>\n","      <td>235.25</td>\n","      <td>[ 6.82566911e-02  3.81275006e-02 -8.46854132e-...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-35</td>\n","      <td>The Cardiovascular System University of Hawai‘...</td>\n","      <td>998</td>\n","      <td>152</td>\n","      <td>249.50</td>\n","      <td>[ 3.30264494e-02 -8.49763490e-03  9.57159605e-...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96c7b39a-34b8-4eb8-aef4-d0c971f61f39')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-96c7b39a-34b8-4eb8-aef4-d0c971f61f39 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-96c7b39a-34b8-4eb8-aef4-d0c971f61f39');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-e17d8317-0e5e-4350-940d-e0e30fd7667d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e17d8317-0e5e-4350-940d-e0e30fd7667d')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-e17d8317-0e5e-4350-940d-e0e30fd7667d button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"text_chunks_and_embedding_df_load","summary":"{\n  \"name\": \"text_chunks_and_embedding_df_load\",\n  \"rows\": 1680,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349,\n        \"min\": -39,\n        \"max\": 1166,\n        \"num_unique_values\": 1136,\n        \"samples\": [\n          795,\n          918,\n          265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"The major determinants of Type 2 diabetes that can be changed are overnutrition and a sedentary lifestyle. Therefore, reversing or improving these factors by lifestyle interventions markedly improve the overall health of Type 2 diabetics and lower blood-glucose levels. In fact it has been shown that when people are overweight, losing as little as nine pounds (four kilograms) decreases blood- glucose levels in Type 2 diabetics. The Diabetes Prevention Trial demonstrated that by adhering to a diet containing between 1,200 and 1,800 kilocalories per day with a dietary fat intake goal of less than 25 percent and increasing physical activity to at least 150 minutes per week, people at high risk for Type 2 diabetes achieved a weight loss of 7 percent and significantly decreased their chances of developing Type 2 diabetes.15 The American Diabetes Association (ADA) has a website that provides information and tips for helping diabetics answer the question, \\u201cWhat Can I Eat\\u201d. In regard to carbohydrates the ADA recommends diabetics keep track of the carbohydrates they eat and set a limit. These dietary practices will help keep blood-glucose levels in the target range. Figure 18.5 Metabolic Syndrome: A Combination of Risk Factors Increasing the Chances for Chronic Disease 15.\\u00a0Knowler WC. (2002). Reduction in the Incidence of Type 2 Diabetes with Lifestyle Intervention or Metformin.\",\n          \"Scheme of a micelle formed by phospholipid s in an aqueous solution by Emmanuel Boutet /\\u00a0CC BY-SA 3.0 cholesterol so it acts as an emulsifier. It attracts and holds onto fat while it is simultaneously attracted to and held on to by water. Emulsification increases the surface area of lipids over a thousand- fold, making them more accessible to the digestive enzymes. Once the stomach contents have been emulsified, fat-breaking enzymes work on the triglycerides and diglycerides to sever fatty acids from their glycerol foundations. As pancreatic lipase enters the small intestine, it breaks down the fats into free fatty acids and monoglycerides. Yet again, another hurdle presents itself. How will the fats pass through the watery layer of mucus that coats the absorptive lining of the digestive tract?As before, the answer is bile. Bile salts envelop the fatty acids and monoglycerides to form micelles. Micelles have a fatty acid core with a water-soluble exterior.\",\n          \"Image by\\u00a0Gtirouflet / CC BY-SA 3.0 dense cortical bone is about 10 percent porous and it looks like many concentric circles, similar to the rings in a tree trunk, sandwiched together (Figure 2.27 \\u201cCortical (Compact) Bone\\u201d). Cortical bone tissue makes up approximately 80 percent of the adult skeleton. It surrounds all trabecular tissue and is the only bone tissue in the shafts of long bones. Figure 2.26 The Arrangement of Bone Tissues The two basic tissue types of bones are trabecular and cortical. This photo shows normal (left) and degraded (right) trabecular (spongy) bone. Figure 2.27 Cortical (Compact) Bone. The Skeletal System | 123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415,\n        \"min\": 121,\n        \"max\": 1831,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          421,\n          777,\n          1617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66,\n        \"min\": 9,\n        \"max\": 297,\n        \"num_unique_values\": 257,\n        \"samples\": [\n          227,\n          276,\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.84074677997317,\n        \"min\": 30.25,\n        \"max\": 457.75,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          105.25,\n          194.25,\n          404.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"[ 2.29729712e-02  2.80858967e-02  1.12715000e-02  2.38459539e-02\\n  6.08849078e-02 -2.20196079e-02 -3.04390900e-02  7.16837496e-02\\n  3.34621929e-02  2.56895032e-02  3.81384008e-02 -1.46272471e-02\\n  6.05134619e-03  3.96036990e-02 -2.14921068e-02 -2.19398756e-02\\n  3.65133472e-02 -3.25683095e-02 -2.36242507e-02  1.27335191e-02\\n -2.04192102e-02 -7.57569866e-03 -2.97014453e-02  1.34395417e-02\\n -3.89803052e-02  7.09829107e-03  8.38490427e-02  4.68919054e-03\\n -6.21600682e-03 -3.64423431e-02  6.83111176e-02  2.30760835e-02\\n  1.70159172e-02 -3.24641615e-02  2.05456968e-06 -3.15726995e-02\\n -4.76174010e-03  2.94138342e-02  9.00705997e-03  4.50910553e-02\\n  1.82500333e-02 -2.45281626e-02  2.80213878e-02  3.24000865e-02\\n  1.67630445e-02 -2.78206356e-02  2.20614839e-02  3.33284363e-02\\n  2.97171902e-02  2.26193946e-02  9.21218190e-03 -9.00363252e-02\\n  1.28435260e-02 -2.76685599e-02 -1.24296751e-02  2.76750606e-02\\n -7.83731788e-03  5.50725460e-02 -3.28327604e-02 -9.84455738e-03\\n  4.40649912e-02 -5.17247468e-02 -2.61238627e-02  2.34586280e-02\\n -3.23815495e-02  1.43910442e-02 -2.31870990e-02 -6.13956004e-02\\n  3.56797464e-02  4.42797542e-02  7.54781887e-02 -1.18830567e-02\\n  2.61459257e-02  2.20207274e-02  1.07243182e-02  9.09299869e-03\\n  2.32546125e-02 -7.01869577e-02 -4.57891934e-02  2.91911066e-02\\n -1.55159086e-02  1.12296343e-02  1.60889253e-02 -2.10474096e-02\\n  1.33295916e-03 -1.47659099e-02 -1.23751191e-02  2.28247084e-02\\n  1.66185517e-02  7.55396718e-03 -2.91870032e-02  2.15626378e-02\\n -4.81806546e-02  2.03475356e-02  2.55755950e-02 -1.04519501e-02\\n  2.39592195e-02  7.22793639e-02  2.07349043e-02 -5.32777049e-02\\n  4.41575311e-02  7.44044557e-02  8.33298713e-02 -2.73018633e-03\\n  2.66782939e-03  1.34550482e-02 -3.98521274e-02  1.67210482e-03\\n -3.05837709e-02  4.47130464e-02 -2.64749187e-03 -1.56471636e-02\\n -7.53284320e-02 -2.73020063e-02  4.55050021e-02 -1.80287682e-03\\n  5.39066736e-03 -4.53817174e-02 -4.69642878e-02 -3.81236225e-02\\n -1.64280459e-02  8.61568097e-03  3.98035981e-02 -1.25875622e-02\\n -1.80445574e-02  1.02986321e-02  4.12722863e-02 -1.31619982e-02\\n  4.11747545e-02  6.24924619e-03  4.69480641e-02 -1.32724438e-02\\n -3.39613925e-03  8.27880949e-03  6.28442178e-03  5.81035987e-02\\n -1.29940435e-02 -3.89739405e-03  2.20918935e-02  7.65071576e-03\\n -5.82268909e-02  5.13123441e-03 -2.03677192e-02 -6.17644452e-02\\n  5.85707976e-03 -2.43919939e-02  3.34543549e-02  3.82698886e-02\\n  3.51353991e-03 -1.31548736e-02 -1.13629512e-02 -1.27255134e-02\\n -2.48163124e-03  1.72944702e-02  8.27016830e-02 -3.42380162e-03\\n -5.70958778e-02  2.50502699e-03  3.03654727e-02  9.44623444e-03\\n  1.72372162e-02  7.99587555e-03 -2.43335757e-02  2.92636547e-02\\n  6.21003769e-02  7.02438236e-04  4.73045446e-02 -2.81685181e-02\\n  3.49310180e-03  5.04241772e-02  1.11960378e-02  3.41657214e-02\\n -4.17057648e-02  5.92101254e-02  2.73968335e-02 -3.56948450e-02\\n  3.93448658e-02  3.51529308e-02  8.42085201e-03 -1.98321957e-02\\n  4.33469713e-02 -1.18261045e-02  4.92545627e-02 -3.05400956e-02\\n  4.26846975e-03  4.03070636e-03 -6.49488345e-02 -7.95924570e-04\\n -3.46954390e-02 -9.99789964e-03  1.64243709e-02 -1.20536881e-02\\n  7.07974732e-02 -9.05370936e-02  2.73720417e-02  2.17130873e-02\\n -2.33670045e-02  9.12264287e-02  1.64063182e-02  3.62658389e-02\\n  1.47561124e-02 -6.63467124e-03 -5.39022982e-02 -6.76799789e-02\\n  4.55670338e-03 -2.33429801e-02  4.05053683e-02 -1.99937429e-02\\n  3.04897185e-02 -5.00026420e-02  4.43863347e-02 -2.04093903e-02\\n  6.49715960e-02 -4.67289835e-02  3.04762907e-02 -4.17238474e-02\\n  1.36049623e-02 -1.18888151e-02  1.12099862e-02  6.29553497e-02\\n -1.77913252e-02  4.15676348e-02 -1.00580053e-02  6.75371150e-04\\n  2.51111519e-02 -1.26866316e-02  1.68014988e-02 -3.28931436e-02\\n -4.90503013e-02 -5.12543647e-03  5.16201593e-02  2.13184338e-02\\n -6.63907155e-02  2.90889200e-02 -7.74796936e-04 -3.93134579e-02\\n -6.18337747e-03  1.16510903e-02 -3.34364139e-02 -2.28617378e-02\\n -9.15383454e-03 -3.65933105e-02 -3.25162262e-02 -1.53120402e-02\\n -9.30079743e-02 -3.57470699e-02 -7.29239685e-03  4.26715985e-02\\n -4.41388600e-02  6.41513467e-02 -2.29487196e-02 -5.17616123e-02\\n  1.37782358e-02  2.89340112e-02 -7.61773959e-02  1.12729799e-02\\n -7.52168596e-02  5.26861101e-03  1.53507972e-02  4.68976758e-02\\n -1.34887900e-02 -8.26931838e-03  5.36376378e-03  4.95160818e-02\\n -1.30892098e-02 -4.93161492e-02  8.89857020e-03  2.88364477e-02\\n -4.04627621e-02 -5.19260094e-02  6.67705247e-03 -2.46643461e-03\\n  2.24981792e-02  2.95400154e-02 -5.24829235e-03  1.17631555e-02\\n  3.35198268e-02 -4.02946165e-03  1.56868231e-02  2.04602368e-02\\n  8.72190893e-02  6.44629896e-02  4.14886996e-02  1.39544327e-02\\n  3.64198275e-02 -4.75281104e-02 -1.10576088e-02 -6.18624985e-02\\n  1.04863308e-02  1.40444888e-03  5.39735444e-02 -1.78427231e-02\\n  4.31339396e-03  6.45331889e-02 -4.83200699e-02  7.58985654e-02\\n  1.35629280e-02 -4.18066978e-02  1.54282255e-02 -2.14080475e-02\\n  2.15211418e-02  3.94406319e-02 -1.00536551e-02  6.46505458e-03\\n -1.72794964e-02 -1.12749003e-02 -2.73868050e-02 -3.50129902e-02\\n  8.66162498e-03 -6.33235043e-03  4.85603251e-02  3.74994241e-02\\n  2.12799814e-02 -2.77581662e-02 -3.15048848e-03 -5.67035610e-03\\n -4.97750714e-02  1.61907412e-02 -3.45674045e-02  2.42027901e-02\\n -6.08441420e-04 -1.02716545e-02 -4.54923660e-02 -1.02981711e-02\\n  1.40911583e-02  3.75180282e-02  1.68752670e-02  2.34776717e-02\\n -6.18587136e-02  3.49496827e-02  2.63891201e-02  3.26851308e-02\\n  1.95784457e-02  4.40052897e-03 -2.89963484e-02 -6.47588400e-03\\n  3.63212861e-02  7.43830502e-02 -2.42742654e-02  5.18779270e-02\\n -7.51687139e-02 -3.40245292e-02  1.70769282e-02  8.19654763e-02\\n -1.98336337e-02 -1.83938295e-02 -5.01794182e-02  2.75631640e-02\\n -4.24477682e-02  2.42094826e-02 -2.18182877e-02  1.33800218e-02\\n  3.36719789e-02  1.84938870e-03  1.08133173e-02 -6.36373088e-02\\n  4.44507152e-02  1.21700894e-02 -2.84682978e-02 -1.32247293e-02\\n  2.05366500e-02  4.44630794e-02 -6.56913072e-02 -1.55691383e-02\\n  1.03134699e-02 -3.58045660e-02 -1.38292583e-02  4.76093061e-04\\n  1.26236770e-02  1.15177697e-02 -8.35120492e-03  3.91691998e-02\\n  3.29118059e-03  1.52778924e-02 -4.11466584e-02 -2.41303574e-02\\n  2.96915621e-02  1.58117022e-02 -5.56087382e-02 -6.59467326e-03\\n  3.99093628e-02 -9.33313295e-02  1.72184650e-02  3.36729432e-03\\n -1.26988860e-02  2.48798970e-02 -8.33463483e-03 -2.17450745e-02\\n  5.75357601e-02 -3.72378267e-02  2.99197137e-02 -5.25633106e-03\\n  7.22941477e-03 -2.23791022e-02  9.92902089e-03 -4.89397943e-02\\n -3.30955125e-02 -3.47236134e-02 -3.55034024e-02  4.07063104e-02\\n  2.99441069e-02 -1.32662177e-01  3.83205079e-02 -2.97528952e-02\\n  1.57118067e-02  1.49636613e-02  3.05667035e-02  5.80923259e-03\\n  4.45674136e-02  8.68415609e-02  3.00537106e-02 -3.14172730e-02\\n -1.12733599e-02 -6.85861632e-02  8.29143077e-02  9.32687253e-04\\n  8.28287005e-03  4.60251383e-02  5.92524000e-02 -8.62023327e-03\\n -1.28702521e-02 -9.42938589e-03  2.76092310e-02  4.46677580e-02\\n -6.61543086e-02 -4.07488719e-02 -2.03002933e-02  2.83709373e-02\\n -2.50594653e-02  1.89823210e-02  6.60287142e-02  6.14814945e-02\\n -1.37754623e-03 -3.10590887e-03  2.10915934e-02  1.09830508e-02\\n -2.95005180e-02 -1.86231025e-02  8.57474133e-02  5.12545519e-02\\n  2.64492128e-02 -3.36265191e-02  3.48860510e-02 -2.85117216e-02\\n  7.39187151e-02  2.89257746e-02  3.45990323e-02 -1.44111356e-02\\n  6.03982285e-02  5.42171765e-03  1.08373733e-02  1.06092310e-02\\n -6.86236396e-02  2.03288117e-04  1.60011351e-02 -8.57141428e-03\\n -2.39313841e-02 -2.35607326e-02 -3.61981764e-02 -4.08706516e-02\\n  1.07472815e-01  3.81626049e-03  3.19191143e-02 -1.63456071e-02\\n -1.62466131e-02 -6.20278204e-03 -5.22550605e-02  5.99559210e-02\\n -5.44447228e-02 -1.31396269e-02  4.59873267e-02  4.86567691e-02\\n  2.39443686e-02  3.04191876e-02 -4.35936712e-02 -5.25394008e-02\\n -7.88886696e-02  2.50433222e-03 -3.06024663e-02 -3.53882685e-02\\n -4.16593067e-02  2.38333158e-02 -1.45164807e-03 -2.84110345e-02\\n -2.23766919e-02  3.99736091e-02 -1.79760810e-02 -2.58030556e-02\\n -2.83792131e-02  1.75174233e-02 -3.72972079e-02 -2.12674402e-03\\n -6.22427501e-02 -6.65185601e-02 -2.23804079e-02 -7.89400488e-02\\n -1.83512717e-02  5.61043695e-02  1.92721095e-02  5.66429757e-02\\n  6.06254116e-03 -2.40686052e-02  3.77018452e-02 -2.64845230e-02\\n -3.40272952e-03 -3.50038707e-02 -1.39413746e-02  1.73181354e-03\\n  2.22160127e-02  3.72691378e-02 -3.77214104e-02  4.42813803e-03\\n  1.78216752e-02 -2.63886321e-02  1.44201498e-02  5.36084129e-03\\n -1.14303408e-02  1.87312569e-02  1.34985230e-03 -4.87835286e-03\\n -3.91572108e-03 -1.70022659e-02  5.42912353e-03  6.04774803e-02\\n  1.41849415e-02  3.82715948e-02 -3.95067558e-02 -2.00121272e-02\\n -2.09616944e-02  2.74659786e-02  9.52143967e-03 -6.81963516e-03\\n -1.10616554e-02 -8.06704164e-03 -2.63257436e-02 -1.13474466e-02\\n  4.03615721e-02 -2.57180873e-02 -3.69606055e-02 -2.36007608e-02\\n  1.45106716e-02 -3.04713380e-02  2.86728740e-02 -2.93919444e-03\\n -2.37204973e-03 -8.02426413e-03 -3.76460627e-02 -2.02637110e-02\\n  2.97035687e-02 -2.50616148e-02  1.87683746e-03 -5.57896569e-02\\n -2.73329560e-02 -3.78011167e-02  1.61891244e-02 -5.38802897e-33\\n  2.96111181e-02 -9.83624719e-03  4.98379879e-02  5.77777326e-02\\n  5.10296822e-02  1.32933445e-02 -6.45377636e-02 -9.37822554e-03\\n -5.93022890e-02  7.89666101e-02 -2.94902213e-02 -2.31253002e-02\\n  2.72985399e-05 -4.59786952e-02 -6.16562590e-02 -2.65489593e-02\\n  7.99557474e-03  1.57606276e-03 -3.19293663e-02 -1.06759686e-02\\n -6.44516200e-02  3.45161185e-02  8.73177964e-03  1.91062242e-02\\n  2.99177933e-02  4.72093485e-02  1.41662303e-02 -5.85156435e-04\\n  1.50941033e-03  1.84304677e-02 -1.34501401e-02  7.77282864e-02\\n -3.04925285e-04  4.56190482e-02  1.24297431e-02  7.88352489e-02\\n -6.26035081e-03 -3.03326119e-02  2.22761407e-02  3.11824074e-03\\n  6.48971424e-02 -5.33699654e-02  1.14038214e-03  3.22002592e-03\\n -8.58834088e-02  7.39832669e-02 -2.24174149e-02 -1.98693629e-02\\n  2.02376978e-03  2.72148270e-02 -4.08967547e-02 -6.27453765e-03\\n  2.47814786e-02  4.25336882e-02 -2.33338978e-02  4.21236120e-02\\n -5.35760261e-03 -1.38929300e-02  2.34857071e-02  2.73593366e-02\\n -2.59762164e-02  3.09628043e-02 -4.41264287e-02  3.08858417e-02\\n -3.02760443e-03 -7.87444599e-03 -1.53053449e-02  1.24025848e-02\\n -1.44549301e-02 -7.57972375e-02 -7.31532797e-02 -6.25108033e-02\\n  2.19611656e-02  4.04325128e-02 -2.31300900e-03 -6.09265566e-02\\n -4.90598902e-02  1.80721302e-02 -4.54713255e-02 -4.93626110e-02\\n -9.59483627e-03 -1.43828010e-02  1.51193840e-02 -3.79985310e-02\\n -4.98143584e-02  7.65519217e-02 -3.22400853e-02 -3.91259827e-02\\n -7.93610960e-02 -3.08242645e-02  2.04933472e-02  9.77791287e-03\\n -4.97575093e-04  5.28453244e-03  4.71585430e-02  5.64918062e-03\\n  8.42629075e-02 -6.12572692e-02 -3.29317059e-03  7.30867079e-03\\n -1.97428428e-02 -1.43558551e-02  1.55156280e-03  4.72506173e-02\\n -2.23656278e-03  2.66999938e-02 -1.06265480e-02 -2.58234162e-02\\n  7.00479820e-02 -1.86981261e-02 -3.37645672e-02  2.94382703e-02\\n -3.21478657e-02 -3.84805053e-02 -5.57591282e-02  4.19322774e-02\\n  2.99479533e-02  8.49122629e-02  1.71478745e-02 -5.41984625e-02\\n -3.17496471e-02  3.15846279e-02  3.04570980e-02 -3.83366346e-02\\n -6.88592112e-03 -1.95200220e-02  3.14677991e-02 -1.13854895e-03\\n -7.36938119e-02 -1.74842868e-02  5.41744661e-03 -1.92077104e-02\\n  2.78682137e-07  4.08470184e-02 -1.53628632e-03 -1.78423189e-02\\n -1.03535233e-02 -3.08404192e-02 -6.02940517e-03  3.50857563e-02\\n -6.51571378e-02 -5.80149330e-02  4.58920673e-02  4.16008011e-02\\n -3.90204564e-02 -3.64808962e-02 -3.20155025e-02  6.35178387e-02\\n  2.17223652e-02  1.82709042e-02 -1.36412205e-02 -4.86772275e-03\\n -7.65325595e-03  4.14719339e-03  3.67409103e-02 -2.42217444e-02\\n -7.34255984e-02 -3.96182053e-02  4.99258470e-03  4.51597758e-03\\n -4.24852669e-02  6.33915886e-04 -2.84216926e-02  9.66380630e-03\\n  8.96294974e-03  4.65004295e-02 -7.67296329e-02 -2.04632115e-02\\n -7.31814653e-02 -6.51863143e-02  6.90150857e-02 -3.71697284e-02\\n  4.29911762e-02 -1.47519633e-02  1.61680002e-02 -1.68805681e-02\\n -4.27693613e-02 -3.91147472e-03 -5.42811714e-02 -4.86369655e-02\\n  4.49501313e-02  2.64735520e-02 -7.76932463e-02  4.56957985e-03\\n  2.51250882e-02 -5.15799671e-02  9.67912097e-03 -1.23087047e-02\\n  2.34425999e-02  5.72338793e-03  3.40899415e-02  2.72950362e-02\\n -3.42273489e-02 -3.70479841e-03 -5.08046616e-03  3.87148373e-02\\n -8.21772218e-03 -5.54903001e-02 -7.95374159e-04  1.96648911e-02\\n  1.80299746e-34  1.06232800e-02  7.13028107e-03 -8.23647343e-03\\n -4.72389422e-02  1.98795907e-02 -8.45584832e-03 -1.23929149e-02\\n  1.10524462e-03 -2.05322794e-04  2.58644149e-02 -6.45051524e-02]\",\n          \"[-2.89910735e-04 -4.76907492e-02  4.69394773e-02 -1.34329256e-02\\n  2.12686360e-02 -6.53087068e-03  1.84320193e-02 -5.08894678e-03\\n  4.43609543e-02 -2.93549653e-02  1.95518564e-02  5.42302988e-02\\n  1.03220148e-02 -2.42334884e-02  4.03822921e-02 -3.05171981e-02\\n -2.09650956e-02  7.83721544e-03 -1.09209083e-01 -1.38158966e-02\\n -1.22020207e-02  3.17832944e-03 -2.41586682e-03  3.28726098e-02\\n  6.40644804e-02 -3.99281606e-02 -1.83247719e-02 -3.14286165e-02\\n -4.16912213e-02 -3.85700837e-02 -1.26829343e-02 -1.41209865e-03\\n  1.31108062e-02 -9.94675606e-03  2.10254871e-06 -7.18540251e-02\\n -5.03428802e-02  2.97871716e-02 -1.69740599e-02 -2.29916032e-02\\n -1.15232810e-03 -9.93641540e-02 -1.35236466e-02 -7.95061979e-03\\n  3.42773981e-02 -5.95667362e-02 -1.08887609e-02 -1.95660256e-02\\n  7.71187916e-02  8.48048739e-03 -8.23518354e-03 -7.42352158e-02\\n  3.04884110e-02  5.15961759e-02  5.65598123e-02 -2.05483083e-02\\n -5.42607950e-03 -2.55838782e-02 -3.91943641e-02 -3.48252021e-02\\n  1.56397428e-02  5.52787483e-02  2.22797859e-02 -2.18725502e-02\\n  1.79669056e-02  6.62144572e-02  1.44304838e-02 -3.41766737e-02\\n  3.90865318e-02 -2.13876888e-02 -7.50386193e-02 -3.48831303e-02\\n -3.81169505e-02  1.10502150e-02 -4.12468053e-02  1.77212786e-02\\n  6.79142866e-03  1.09248254e-02  2.70897709e-02  1.80689208e-02\\n  6.10432960e-02  1.14617050e-02 -3.80260847e-03 -3.18495855e-02\\n  1.48276682e-03 -1.43926553e-02 -1.98453534e-02 -4.45456058e-02\\n  7.31130037e-03 -5.73883876e-02  5.91200357e-03 -3.44004631e-02\\n  4.99224756e-03  3.59584615e-02  6.44462276e-03  1.80018116e-02\\n  6.66555017e-02 -1.43214915e-04  5.16850278e-02 -2.70861341e-03\\n -8.12202320e-03 -3.59169021e-02 -2.43293568e-02  3.56825888e-02\\n  3.06037888e-02  2.07295101e-02 -2.72049010e-02  3.89792062e-02\\n -2.48995889e-03  2.48275232e-02  1.78018268e-02 -2.56232731e-02\\n  3.13616432e-02  7.06909299e-02 -3.29235606e-02  6.67960197e-03\\n  1.89117144e-03 -1.15451925e-02 -1.72486249e-02  3.75370458e-02\\n  8.91311280e-03  3.62646990e-02  3.51427384e-02  1.90210883e-02\\n -4.58955057e-02  1.19934686e-01  3.36185768e-02 -7.39446096e-03\\n -1.03651602e-02  8.12049881e-02  4.01631668e-02  1.02531221e-02\\n -4.94023971e-02 -1.05537288e-02 -2.22280361e-02  3.68791493e-03\\n -1.05225015e-02 -2.50414722e-02 -3.70037444e-02 -4.52544652e-02\\n  7.80267594e-03  3.66311371e-02  2.23718900e-02  5.00968890e-03\\n -2.80605908e-02 -1.19902072e-02  1.19607532e-02  3.20545770e-02\\n  2.95469817e-03  6.01988584e-02  2.25247932e-03  6.14434443e-02\\n -2.30831411e-02  1.14265140e-02  2.39354055e-02  1.05365543e-02\\n -1.95968766e-02 -6.21460844e-03 -3.88051420e-02  1.25449747e-02\\n  4.87881489e-02 -1.30138760e-02  3.37267518e-02 -2.83177737e-02\\n -3.69780660e-02 -1.87966954e-02  2.91273575e-02  3.03810872e-02\\n -2.06773151e-02  1.28035024e-02  3.55234668e-02  5.05655212e-03\\n  3.86969410e-02 -6.59883097e-02  6.12658076e-03  6.19648397e-02\\n -7.87231326e-03  2.65277247e-03 -5.22707403e-02  8.05527344e-03\\n  8.70954469e-02  5.17622009e-02  2.65399069e-02 -1.25501503e-03\\n  1.99715979e-02  2.72730663e-02  4.49447669e-02  3.09776962e-02\\n -3.58908772e-02  1.70547906e-02  1.96558908e-02 -1.12501793e-02\\n -5.03951497e-03 -5.86961955e-02 -9.83590959e-04  4.27694917e-02\\n -5.07055633e-02 -1.89975593e-02  4.01682733e-03  3.92593034e-02\\n  1.55548693e-03  7.46860541e-03  5.32642640e-02 -4.59342403e-03\\n -2.41951030e-02  5.40453196e-02 -7.32910186e-02 -6.24243878e-02\\n  1.83400027e-02 -8.10545012e-02 -4.10038345e-02 -5.79014830e-02\\n  5.77441463e-03 -1.38999149e-02  3.55744734e-02  6.65484229e-03\\n  3.30575407e-02  1.69161614e-02 -1.81860868e-02  3.19372416e-02\\n -1.55437961e-02  5.05150668e-02  5.82077727e-02 -2.34185979e-02\\n  5.89606073e-03 -2.15327907e-02  4.05314192e-03 -1.02185132e-02\\n -4.35367711e-02 -4.99300100e-03  1.46756461e-02 -1.71298329e-02\\n  1.82500705e-02  1.11141447e-02 -7.75681529e-03 -3.55587192e-02\\n  7.47965276e-03 -8.88304040e-03 -2.59020030e-02  6.83148857e-03\\n -4.77917828e-02 -2.46858783e-02 -3.94188752e-03  1.66137014e-02\\n  7.24267634e-03 -1.85808558e-02 -6.14708364e-02  1.26165347e-02\\n -3.51574942e-02  1.67146046e-02 -3.57484780e-02 -4.81097698e-02\\n -6.50256276e-02 -3.60029191e-02 -9.18580964e-03  4.35761511e-02\\n  6.52618706e-02 -1.07990149e-02  8.77566729e-03 -6.16241572e-03\\n -1.77497417e-02  2.54069455e-02  3.35899442e-02 -1.85800437e-02\\n -1.61337592e-02 -4.13477346e-02 -1.49374781e-02  2.46960893e-02\\n  2.41081249e-02  9.61841142e-04  3.87678668e-02  3.26361246e-02\\n  6.86734822e-03  5.00897691e-02  5.11830188e-02 -4.05327789e-02\\n -8.97689760e-02 -8.13838269e-04 -3.85215655e-02  1.96412746e-02\\n -4.87155095e-03  6.26533031e-02 -5.00203371e-02  1.36259310e-02\\n  1.98138170e-02  2.23706998e-02 -1.47624556e-02 -2.08031237e-02\\n  5.73374741e-02 -1.08853631e-01  7.27196187e-02  3.57444026e-02\\n -1.71732216e-03 -4.48276754e-03 -1.11867022e-02 -1.15359081e-02\\n -6.34977967e-03  6.09585308e-02  1.82274189e-02  8.02924335e-02\\n -6.22956501e-03  2.27490924e-02  2.69049909e-02  1.22414818e-02\\n -3.83753851e-02  5.28007969e-02 -2.88848113e-02  4.91494825e-03\\n  5.87584712e-02  4.09483258e-03  1.61685962e-02  1.02570588e-02\\n -5.05571030e-02  5.99424243e-02  2.43418068e-02 -2.55821459e-02\\n  3.37801017e-02  2.05735844e-02 -2.93751583e-02 -3.52503918e-02\\n  2.85072997e-02  2.23733932e-02 -1.63354613e-02  1.31281624e-02\\n -2.06889194e-02  1.05702206e-02  2.39639115e-02  2.64977408e-03\\n -4.38604094e-02 -2.48857718e-02 -5.12024667e-03 -4.29650396e-02\\n  3.62291187e-02  1.21394852e-02 -2.53850687e-02 -4.19798791e-02\\n -2.53918786e-02  3.80630642e-02 -1.32844523e-02 -6.39878362e-02\\n -6.17671805e-03 -2.35546101e-03  2.35471465e-02  5.53486682e-02\\n  3.34822424e-02 -6.05285168e-03  3.04024760e-02  4.34461422e-03\\n -3.28185409e-02 -8.15888401e-03 -9.93102323e-03 -4.69345376e-02\\n -2.77412869e-02 -4.12742235e-02 -5.05772792e-02 -2.53928006e-02\\n -7.41897238e-05  2.39503905e-02 -1.62669905e-02 -1.58133835e-03\\n  1.10625632e-01  1.11785065e-02  2.37951316e-02 -6.20768294e-02\\n  4.65054922e-02  4.82486039e-02  5.55227557e-03  3.66428234e-02\\n  5.40087791e-03 -1.73931941e-02 -5.17673744e-03  1.78117566e-02\\n -3.30749415e-02  3.81597057e-02  9.95533075e-03 -2.02581962e-03\\n  6.38610404e-03  4.95962463e-02 -8.83316472e-02 -6.28763228e-04\\n  8.45937710e-03  1.88548472e-02  1.00364130e-04 -1.25207026e-02\\n -7.24978046e-03 -2.52529141e-03  3.03694909e-03 -2.31710598e-02\\n  1.31589556e-02  4.16985825e-02 -9.28908773e-03 -5.84791750e-02\\n -6.46011680e-02 -9.03794989e-02  1.03680491e-02 -1.47697348e-02\\n  3.39405611e-02  2.08894573e-02 -2.78323218e-02  6.83821589e-02\\n  3.50590236e-02 -4.68555689e-02  4.72042076e-02 -3.77112441e-03\\n -5.75096253e-03  2.41767466e-02 -4.17235941e-02  2.28880756e-02\\n  4.02758233e-02 -4.91468981e-02  5.05239628e-02 -1.00680158e-01\\n -2.90744472e-02 -1.67413708e-02  1.17921848e-02 -1.26427915e-02\\n  2.48180218e-02  2.69938000e-02 -9.13031306e-03 -5.64103164e-02\\n  4.68287803e-02  3.84604670e-02  3.81848142e-02  2.33297888e-02\\n  2.69803610e-02 -3.96660902e-02 -2.58250758e-02 -6.80448487e-02\\n -7.80719274e-04 -1.27612036e-02  2.29762010e-02 -1.66850742e-02\\n -1.99029464e-02  1.26399044e-02  9.90941525e-02 -4.01808042e-03\\n  4.98764403e-02 -5.11977188e-02  1.49696544e-02  3.19837220e-02\\n  2.93051992e-02 -3.64751816e-02  3.42510156e-02 -6.13177707e-03\\n  6.81453645e-02 -2.24678498e-03 -1.98654179e-02  5.55717237e-02\\n -2.85562035e-02  3.58595587e-02 -1.89085081e-02  3.27889845e-02\\n -3.52927782e-02  4.49295454e-02  3.33935805e-02 -2.24282476e-03\\n  1.83846429e-02  4.99155652e-03  2.87550557e-02 -6.33753240e-02\\n  2.02281289e-02 -2.66907533e-04  3.94245014e-02 -1.10258274e-02\\n -3.18046138e-02 -1.13525372e-02 -2.47585289e-02 -4.52763459e-04\\n -1.15936622e-02 -9.17788537e-04 -4.71309796e-02  3.67310457e-02\\n -2.05483865e-02  7.07767438e-03 -3.64862531e-02 -6.40820637e-02\\n -5.00232242e-02 -2.14895923e-02 -2.11248789e-02  2.18097661e-02\\n -3.70296799e-02 -7.97156896e-03 -7.28307292e-04 -5.57386354e-02\\n  1.37111153e-02  3.24638449e-02 -4.11842763e-02 -8.94591399e-03\\n -2.66409963e-02 -4.38091956e-04  7.27507696e-02  3.20167057e-02\\n  7.19509600e-03 -7.69989118e-02 -8.38134252e-03 -1.57920923e-02\\n  1.48548437e-02  3.00976424e-03  1.15479948e-02  7.29491115e-02\\n -1.33349663e-02 -1.48559851e-03  7.86755513e-03  1.31217875e-02\\n -7.11150914e-02 -1.64781269e-02 -1.93814710e-02 -1.98638737e-02\\n -3.71746086e-02  6.20009191e-02  3.73834372e-02 -1.30262449e-02\\n -9.81295332e-02  4.18129601e-02  1.00712385e-02 -8.76897667e-03\\n -3.06629203e-03 -7.13661686e-02  4.43297476e-02  2.85195205e-02\\n  7.60584278e-03 -1.95184536e-02 -3.53755690e-02 -1.98184811e-02\\n  1.83212617e-03  3.23965028e-02  7.32532330e-03  1.30306305e-02\\n -6.46259710e-02  6.06432855e-02 -2.28145029e-02 -9.53818951e-03\\n -2.75356565e-02 -1.14284353e-02  1.32799949e-02 -2.13443078e-02\\n  6.87101409e-02  5.53869046e-02  2.36709360e-02 -5.58103174e-02\\n -1.48388359e-03  1.56297795e-02 -2.13469844e-02  5.28876260e-02\\n  3.98664139e-02  2.78922580e-02 -2.24801544e-02  2.37328303e-03\\n  5.89274503e-02 -5.76467775e-02 -2.99277268e-02  1.71148032e-02\\n -5.72593212e-02 -4.33231285e-03  4.52495329e-02 -5.64960070e-33\\n  7.47269094e-02  3.62142138e-02  3.95277478e-02  3.07968576e-02\\n  5.90636693e-02  3.75339501e-02 -2.31895726e-02 -4.05858383e-02\\n -1.92534924e-03  9.35129151e-02 -5.24862250e-03 -1.54407015e-02\\n  9.47384071e-03  2.04024091e-02 -5.42762764e-02 -8.57644305e-02\\n  3.89174446e-02 -2.27755848e-02  4.55556773e-02  2.87668332e-02\\n -4.85455543e-02 -6.96453229e-02 -8.42806175e-02 -2.75642015e-02\\n  7.34933466e-03 -2.87090186e-02  6.42396212e-02 -6.70448085e-03\\n -9.58653986e-02  1.88550483e-02 -3.90135646e-02 -3.27156186e-02\\n -4.77096625e-02 -2.81655490e-02 -1.86207108e-02  2.91774161e-02\\n  2.21861750e-02 -1.15162283e-02 -2.01660730e-02  1.75863020e-02\\n -2.69363285e-03 -2.08921097e-02  1.97621379e-02 -4.78858082e-03\\n -1.26683619e-02  1.35948882e-02  1.80629622e-02  3.39837112e-02\\n  2.93650609e-02 -2.20210832e-02 -1.12774428e-02  2.52639689e-02\\n -3.16103883e-02 -3.53645737e-05  7.28631169e-02  9.46151465e-03\\n  5.21957800e-02 -6.56996295e-02  9.55659822e-02  8.91741831e-03\\n -4.13986333e-02 -1.96979428e-03 -2.69607920e-02 -3.09552792e-02\\n -3.77707742e-02 -2.05183239e-03  2.14984917e-04  6.11692369e-02\\n  7.54572675e-02  3.39427218e-02 -5.28387493e-03 -8.06459859e-02\\n  1.36984903e-02 -9.93240699e-02  1.18184602e-02 -1.05019547e-02\\n -1.66479088e-02 -5.94034381e-02 -7.50455484e-02 -9.63898599e-02\\n -4.96468060e-02 -3.80980363e-03  2.93504149e-02 -1.30302366e-02\\n -1.96876731e-02  7.66343391e-03 -6.62882987e-04 -6.74138777e-03\\n -3.01071852e-02 -3.74916419e-02  8.14801380e-02  1.02742910e-02\\n -3.63987908e-02  4.70368713e-02  4.34757471e-02 -6.05085008e-02\\n  5.91272563e-02 -6.01011366e-02 -4.68130084e-03  5.12216762e-02\\n -1.28850369e-02  1.57379806e-02  1.33342706e-02  2.65763644e-02\\n -5.83724044e-02  3.47832330e-02 -1.22272791e-02  1.50706095e-03\\n -1.26263043e-02 -2.35722829e-02 -8.27669539e-03  3.78872566e-02\\n -1.11825429e-02 -2.38841549e-02 -1.60250515e-02  3.18535157e-02\\n  5.17347408e-03 -9.09908582e-03 -6.33173902e-03 -4.04079333e-02\\n -1.26681169e-02  2.52131280e-03 -4.30412777e-02  9.42817144e-03\\n -6.64179102e-02 -2.53620930e-02  3.53503413e-03  8.19246657e-03\\n  5.27810073e-04 -4.34151255e-02 -3.94068286e-03 -1.35703934e-02\\n  2.88146026e-07  4.27781008e-02  5.02584130e-03  7.52717210e-03\\n  1.53221963e-02 -3.81950592e-03 -4.28856388e-02  4.52936767e-03\\n  1.84004679e-02 -6.30369643e-03 -2.81434022e-02 -1.63980434e-03\\n  3.87146894e-04 -2.06686463e-02  3.05093341e-02  5.44758840e-03\\n  5.98083548e-02  5.66420220e-02 -2.37631034e-02  3.25634168e-03\\n  4.02556248e-02  2.62154248e-02  2.93611716e-02  4.04729322e-02\\n -3.56091261e-02 -3.55419237e-03  7.23572969e-02 -5.21490350e-02\\n  3.87458056e-02  6.35161027e-02 -3.82685214e-02 -1.84759367e-02\\n  1.70838181e-02  7.56823570e-02 -5.08912429e-02 -1.48347579e-03\\n -2.67082378e-02  6.18557027e-03 -8.99394450e-04 -1.19499853e-02\\n -7.07405107e-03  3.20798010e-02  1.72325242e-02 -1.53200990e-02\\n  7.72961415e-03 -5.85321561e-02 -3.12942429e-03  5.66900382e-03\\n -2.73432378e-02  3.62395383e-02 -1.12157129e-02 -1.35082491e-02\\n  3.58749926e-02  6.22950634e-03  2.90324874e-02 -9.97451413e-03\\n -1.87331941e-02  3.08613442e-02  2.58529987e-02 -1.21459886e-02\\n  2.73999870e-02  1.34400213e-02  8.27934500e-03 -4.50032167e-02\\n  1.50194066e-02  5.98297454e-02 -6.39654621e-02 -3.34206223e-02\\n  3.10314759e-34 -3.95918760e-04 -6.39823824e-02 -1.33804725e-02\\n -3.26808076e-03  4.86219535e-03 -3.79619971e-02  3.63107920e-02\\n  3.06082554e-02 -7.84143514e-04  5.17915264e-02 -1.50401648e-02]\",\n          \"[ 1.57728139e-02 -5.64577729e-02 -3.63888033e-03  1.73159987e-02\\n -2.66745500e-03  2.10536420e-02 -2.02101655e-02  4.30584364e-02\\n -3.53951268e-02  2.19664518e-02 -4.33511287e-02 -5.20750508e-02\\n -5.89872338e-03  2.55817883e-02  3.15636769e-02  7.17286533e-03\\n -3.72072756e-02 -1.11364517e-02 -1.11601995e-02 -3.12545337e-02\\n -3.86702456e-02 -2.43350528e-02 -4.18420956e-02 -4.27627712e-02\\n -3.75365429e-02 -1.38184447e-02  2.48206146e-02 -1.15433261e-02\\n -1.36964619e-02 -3.28309881e-03  1.13740982e-02 -1.04402518e-02\\n  5.59672639e-02 -3.27690318e-02  2.05511310e-06  8.69802199e-03\\n  5.24926791e-03 -6.02529326e-04  7.95765501e-03  8.60197842e-02\\n  1.62341874e-02  3.49696493e-04 -2.19876915e-02 -1.75113545e-03\\n  2.28333678e-02  2.16686577e-02 -1.60330608e-02  2.73146927e-02\\n -6.92030713e-02 -6.68439129e-03 -2.49029859e-03  2.01713145e-02\\n  1.47406431e-02  1.52928270e-02  6.65228488e-03  2.65217684e-02\\n -5.44198453e-02  1.34172156e-01 -1.52043123e-02  2.51244586e-02\\n -2.14611422e-02  7.70719722e-04  1.96670070e-02 -2.68957503e-02\\n  4.74147126e-02  4.20907736e-02  4.15420420e-02 -4.86595370e-02\\n  2.95606162e-02  1.16407927e-02  8.50693509e-03 -1.01046441e-02\\n  4.82071489e-02  6.38101064e-03  2.07358552e-03 -5.98762743e-02\\n -1.56382807e-02  4.81903777e-02  2.05990300e-02 -7.43976096e-04\\n  1.42410239e-02  3.69130692e-04 -3.08749732e-02  5.07627055e-03\\n -1.63760204e-02 -8.42410028e-02  3.60810347e-02  8.39669071e-03\\n  5.14656641e-02 -3.27205695e-02  7.42358039e-04 -3.66088492e-03\\n  1.47049148e-02  4.50345390e-02 -6.67315498e-02 -3.50497290e-02\\n -2.04252452e-03 -1.14344561e-03  1.14085399e-01 -3.33110057e-02\\n -5.43336533e-02 -6.90692663e-03 -1.05299480e-01  4.72107492e-02\\n -7.59746283e-02 -4.97120246e-02 -4.04516011e-02  2.93611512e-02\\n  1.07014980e-02 -3.83841433e-03  3.03368866e-02  4.20745462e-03\\n  1.17355296e-02  3.46791558e-02 -2.05858164e-02 -6.12229370e-02\\n -2.72969250e-02 -4.43289801e-03  7.09903007e-03 -2.08244193e-02\\n -6.91965455e-03  6.64384440e-02  1.79156717e-02  2.68767122e-02\\n  1.39488243e-02 -5.40009560e-03 -3.38539816e-02  1.90435927e-02\\n  9.69934836e-03 -2.47601252e-02  1.38934590e-02  3.50373574e-02\\n -1.62765086e-02  6.22384483e-03 -2.01239274e-03 -7.44358748e-02\\n  5.14437035e-02  5.49384486e-03  1.21744294e-02  3.73239219e-02\\n -1.50537109e-02 -3.23126055e-02 -1.56131070e-02 -3.34043358e-03\\n -1.84459947e-02  2.07658317e-02 -1.68162771e-02  6.05426207e-02\\n -9.68633406e-03  8.01254530e-03 -2.72264406e-02 -4.52809175e-03\\n -7.75353014e-02 -2.35011261e-02  5.10917678e-02 -3.80758867e-02\\n  1.37859508e-02 -1.74623784e-02  3.25980298e-02  5.16020320e-02\\n  2.20756009e-02 -2.48685982e-02 -5.07828332e-02 -7.71802366e-02\\n  6.98580779e-03 -1.88843627e-02 -4.38863672e-02  1.05715198e-02\\n  3.30445208e-02  3.87548818e-03  4.17918675e-02  7.42689371e-02\\n -1.62473880e-02 -2.62174979e-02 -2.07489152e-02  1.32539898e-01\\n -4.11955602e-02  8.61665234e-04  6.24621920e-02 -7.81116262e-02\\n  7.59768039e-02 -2.26889178e-02  4.39216346e-02 -2.91420631e-02\\n  3.70258503e-02 -6.29993854e-03 -1.81148071e-02  5.53550711e-03\\n  1.04619013e-02 -3.58778760e-02 -7.27115991e-03 -4.91430573e-02\\n -1.79034621e-02  2.63426341e-02  2.13690829e-02 -1.68986395e-02\\n  6.72692084e-04 -7.07057565e-02 -1.83181074e-02  2.31532566e-02\\n -3.04878112e-02 -3.46204429e-03 -5.81701845e-03 -5.40998206e-02\\n -4.43706177e-02 -4.23483402e-02 -4.37021367e-02  2.59232540e-02\\n  3.26459343e-03  2.61602327e-02 -2.89005805e-02  3.05306297e-02\\n  6.89620385e-03 -4.21663038e-02  1.18978396e-02 -1.43660037e-02\\n  1.15575418e-02 -2.52995882e-02 -4.00522910e-02  1.20739350e-02\\n -2.85350680e-02 -1.07138921e-02 -3.22781019e-02  8.96959193e-03\\n -4.03619893e-02  8.41991827e-02  1.07750827e-02 -3.76433507e-02\\n -7.40152821e-02 -4.36559580e-02  2.69117989e-02 -1.00799685e-03\\n  3.17117423e-02  5.25654964e-02 -4.80853803e-02 -5.99735826e-02\\n -8.01319331e-02  4.34510373e-02 -1.74203720e-02  4.51894887e-02\\n  1.31648295e-02 -9.69533473e-02  4.54950668e-02  4.50390577e-03\\n  4.80539836e-02 -2.98802126e-02  2.00214162e-02  2.06043832e-02\\n -1.51534937e-02 -1.61411949e-02 -3.15559916e-02 -4.95321713e-02\\n  4.67078015e-03  2.07061321e-02 -7.21533224e-02  4.37031500e-02\\n -9.53316763e-02  1.38542540e-02 -5.00775017e-02 -2.00575031e-02\\n  4.04674932e-02 -4.60864492e-02  1.04648452e-02 -2.10422873e-02\\n -1.63541567e-02  2.04278179e-03  1.52302468e-02 -7.33365677e-03\\n  6.12386083e-03  1.80253889e-02 -3.73194413e-03  4.54135388e-02\\n -1.63549334e-02  2.90952753e-02 -5.81544563e-02  1.81123242e-02\\n  4.74347919e-02  3.09087802e-02 -4.07738751e-03  3.95797528e-02\\n  6.03918172e-03  7.78303444e-02 -7.49446228e-02 -2.59119626e-02\\n  1.99973714e-02 -3.86637338e-02 -5.09800501e-02 -3.81934792e-02\\n  3.10432483e-02  6.50127158e-02  5.39996009e-03  6.04327917e-02\\n -3.42034153e-03 -2.26868819e-02  3.74620222e-02  3.57151777e-03\\n -1.46530364e-02  2.38615051e-02  1.38629163e-02  7.55867735e-02\\n  9.34519246e-03  2.70611402e-02  2.20353133e-03  3.78867891e-03\\n  1.29411439e-03  3.50290649e-02 -2.58361306e-02 -3.20287235e-02\\n  3.19325998e-02  5.17735779e-02  3.26043740e-02 -2.35525165e-02\\n -8.89074616e-03 -9.25663188e-02 -1.09475553e-02  6.18337803e-02\\n -9.19495523e-03 -1.52931651e-02 -2.32071225e-02  5.54587785e-03\\n  2.19747284e-03  4.17146422e-02  2.41861083e-02  5.54502122e-02\\n  1.29140131e-02  2.37725414e-02 -4.04298417e-02 -8.37784261e-03\\n  1.24115532e-03  3.72962430e-02 -2.39369404e-02  3.17287147e-02\\n -1.81639157e-02 -3.81305888e-02 -2.29516570e-02  1.86472125e-02\\n -2.51918212e-02 -1.11231739e-02  2.61019431e-02 -4.17887326e-03\\n  6.35215407e-03  3.05978372e-03 -2.26567276e-02  5.52211553e-02\\n  2.82943714e-02  5.34320474e-02  1.34907169e-02 -4.37364243e-02\\n  4.72911022e-04 -5.08616529e-02  2.24530231e-03  2.40628179e-02\\n -5.08143939e-02  3.10155284e-02  5.88588463e-03  6.03938997e-02\\n -6.95785042e-03  8.09021369e-02  1.35770431e-02  2.20059697e-02\\n  2.92207021e-03  3.32831107e-02 -2.36401837e-02  3.11954096e-02\\n -4.10429724e-02 -5.60786687e-02 -5.94614656e-04  2.23749820e-02\\n  2.54720799e-03 -1.74369328e-02  6.10900186e-02  5.14829457e-02\\n  2.62116641e-02  1.92390960e-02  7.50768988e-04  3.26913968e-02\\n  4.47159680e-03  4.62862989e-03 -2.28656530e-02 -1.22452769e-02\\n -1.07553555e-02  5.44747226e-02  3.25375088e-02 -8.92883632e-03\\n -2.65619420e-02  5.18130697e-02 -4.14081924e-02  1.72498990e-02\\n  2.43675858e-02 -6.26094043e-02 -1.49971270e-03  3.28977033e-02\\n -1.18482010e-02  1.05182640e-02 -1.13771232e-02  1.52540198e-02\\n -6.17332496e-02  5.94183467e-02 -1.74798481e-02  3.99899669e-02\\n -4.91301995e-03 -3.88532132e-02  4.38259244e-02  7.51819462e-04\\n -1.10452743e-02  3.96509208e-02 -5.72199374e-02  5.06886914e-02\\n -1.45355109e-02 -2.52195727e-02  4.76122275e-02 -5.30431755e-02\\n -4.65138350e-03 -1.32735260e-02  1.48337185e-02  1.04052663e-01\\n  3.78372557e-02 -1.66944489e-02  4.94301505e-02  2.64419690e-02\\n  3.87430713e-02 -2.47008689e-02  3.11886929e-02 -2.38935221e-02\\n -3.32879019e-03 -4.90695462e-02  3.12209334e-02  5.64975198e-03\\n -1.49829434e-02  4.12272103e-02 -1.28541626e-02 -4.14467268e-02\\n  2.29692366e-02  5.30691929e-02 -2.79648392e-03  9.76619944e-02\\n -3.33680725e-03 -3.79325971e-02  3.39732692e-02 -8.15867353e-03\\n  1.95176713e-02 -3.17266546e-02  2.76834685e-02  3.81082972e-03\\n  1.27146882e-03 -3.36397141e-02 -6.71808943e-02 -2.31624162e-03\\n  9.74480063e-03  7.14553520e-02  1.19599774e-02  7.30535947e-03\\n -2.38244166e-03 -4.34231386e-03  8.59754626e-03  7.95183238e-03\\n -6.88121393e-02  6.17698543e-02  2.05795504e-02  1.50338085e-02\\n -2.66217478e-02 -8.35669599e-03 -6.99697286e-02 -4.49123569e-02\\n -7.37115517e-02  3.19464058e-02 -3.07798814e-02 -7.66985044e-02\\n  1.89426709e-02  1.13014160e-02 -2.09322702e-02  4.37395684e-02\\n -3.37798335e-02 -3.42227519e-02 -1.77016798e-02 -2.69568264e-02\\n -3.63975391e-02  1.71616971e-02 -5.46425465e-04  6.61909534e-03\\n -5.62195061e-03  8.01925510e-02 -3.30427475e-02  5.89381903e-02\\n  1.32203279e-02  1.94528140e-02 -4.06433875e-03  6.62010442e-03\\n  2.55999155e-02 -2.27144100e-02  1.24016985e-01  1.62556879e-02\\n -1.74379665e-02 -1.78733207e-02  6.59452146e-03  1.21004969e-01\\n  4.74225320e-02 -9.45353508e-03  4.78293635e-02 -1.17366121e-03\\n -2.45884508e-02  4.98797453e-04 -2.94580702e-02  6.61749989e-02\\n  5.29057719e-03  4.71242182e-02 -5.08823025e-04 -4.14450187e-03\\n -3.93875204e-02 -2.48596538e-02 -1.20226685e-02  2.86598373e-02\\n  4.12970260e-02  4.82219532e-02  2.75197811e-02 -4.92078774e-02\\n -2.67136749e-02  9.12839454e-03 -5.45903407e-02 -7.02854292e-03\\n -1.63870286e-02  3.04527041e-02  2.88387425e-02  8.44170060e-03\\n  2.42968020e-03  1.03751123e-02 -4.05461993e-03  2.85729840e-02\\n -2.12687515e-02  8.71469732e-03  4.17382643e-02 -5.81028350e-02\\n -6.57838881e-02 -3.61579955e-02  8.71246029e-03  8.81481692e-02\\n  8.76042992e-03 -1.10896332e-02 -3.13027054e-02 -8.36808607e-03\\n  2.24093217e-02 -3.53497788e-02  4.64792177e-02  1.57019366e-02\\n -2.84721907e-02 -4.64542629e-03 -9.90463980e-03 -2.12735198e-02\\n  3.03749405e-02 -4.30260785e-02 -1.26876626e-02 -2.74391491e-02\\n  7.86173344e-02 -4.44015302e-02 -4.63078730e-02 -5.84436873e-33\\n  2.52901334e-02 -1.22533622e-03  2.77748099e-03 -1.93980560e-02\\n  6.36001350e-03  7.04465210e-02  5.31472862e-02  8.39251745e-03\\n -5.63005917e-02  2.05048844e-02  2.50587221e-02  1.99946109e-02\\n -9.76994867e-04  1.19377207e-02  3.38732712e-02  3.10741961e-02\\n -1.95467491e-02 -8.23809952e-02  3.13371085e-02 -6.07699994e-03\\n -4.58179228e-02 -2.34846137e-02  4.69333977e-02 -2.66870335e-02\\n  2.99764778e-02  4.57940623e-03  3.68848629e-02 -8.09239410e-03\\n -3.91403548e-02 -2.12850124e-02 -1.58816334e-02  5.18109091e-03\\n  4.49667830e-04  6.88075125e-02  1.90401971e-02  3.69306235e-03\\n -5.63388783e-03 -6.54179081e-02  2.01118906e-04 -4.79890518e-02\\n -8.07988346e-02  3.20624709e-02 -1.01095662e-02 -4.02874388e-02\\n  2.35207118e-02  2.24210862e-02 -5.00765536e-03  6.61188085e-03\\n  6.33936897e-02 -3.64622623e-02 -3.75311524e-02  6.13954151e-03\\n -1.46715362e-02 -1.08262962e-02 -3.08830980e-02  6.81238919e-02\\n -1.99181549e-02 -9.69508290e-03  2.02977508e-02  3.64061221e-02\\n  1.30685084e-02  4.41168854e-03 -9.40269157e-02  2.14329083e-02\\n -1.58167053e-02  2.95217764e-02  1.00607947e-02  8.51427391e-03\\n -5.09682670e-02 -5.59637323e-02 -5.46281338e-02  3.26886475e-02\\n -2.30248272e-03 -1.13981050e-02  4.67576943e-02 -2.70811394e-02\\n -5.65672144e-02 -7.20600132e-03  8.01948085e-02  3.82957384e-02\\n -5.66239432e-02  2.40409803e-02 -3.09104827e-04  6.30280981e-03\\n  2.36698873e-02 -1.47807449e-02  1.33694778e-03 -1.02467937e-02\\n -6.37239143e-02 -1.35340011e-02  7.17986654e-03  5.95078664e-03\\n  1.69184320e-02 -1.00788902e-02  3.31459269e-02 -1.39945187e-02\\n  2.43025329e-02 -1.34768318e-02  5.46660088e-02  3.35097574e-02\\n -4.18631593e-03  1.20559670e-02  9.96872690e-03 -8.02405830e-03\\n -2.07022242e-02 -1.35632381e-02  1.76398959e-02  4.72904788e-03\\n  2.84237582e-02  5.47406683e-03  5.50538115e-02 -3.87830101e-02\\n  2.52093542e-02 -2.19766260e-03 -1.40215801e-02 -2.50385376e-03\\n -8.00563022e-04 -4.19641584e-02 -1.19341845e-02  7.16386829e-03\\n -4.00717109e-02 -3.03082317e-02 -2.99562775e-02 -2.40452476e-02\\n  3.49001437e-02 -1.57152545e-02  1.98987331e-02  2.06106920e-02\\n -3.93997021e-02 -2.40752660e-02  9.95541923e-03  3.20616625e-02\\n  2.85939109e-07  1.87074784e-02 -1.40055991e-03 -3.99141610e-02\\n -5.14978766e-02  6.18599914e-02 -3.14667150e-02  1.45795029e-02\\n -9.48637165e-03 -4.80021276e-02 -1.23640768e-01 -7.49548431e-03\\n  4.88134511e-02  2.27913447e-02  4.93784482e-03 -2.99876612e-02\\n  3.39511037e-02 -1.36664929e-02 -5.97698502e-02  1.43400822e-02\\n  7.44066387e-03 -6.55861422e-02 -4.55697849e-02 -4.00813520e-02\\n -3.28516476e-02  4.69854334e-03  3.70964557e-02 -1.64579116e-02\\n  2.70081754e-03  3.53282616e-02 -3.39808613e-02  1.71928611e-02\\n -1.29510593e-02  3.41144986e-02  2.54638400e-02  9.78602213e-04\\n -5.00721298e-03 -6.41398877e-03 -1.76254585e-02 -2.70556696e-02\\n  5.34198470e-02 -2.46294774e-03 -5.98585606e-03  1.63509268e-02\\n -2.67965533e-02 -3.76960542e-03  7.36843678e-04  8.06827098e-03\\n -9.76282060e-02  7.08713159e-02  8.81878007e-03  3.66175249e-02\\n -5.96829504e-02 -4.19249712e-03 -4.77341749e-02  3.49609321e-03\\n -6.49406388e-02  1.75297447e-02  1.77128389e-02  3.39440480e-02\\n  9.04354751e-02 -1.21040661e-02 -1.56275537e-02 -1.67851686e-05\\n -4.02413867e-02  6.47976026e-02  4.05368023e-03  2.55564079e-02\\n  2.74016963e-34  3.57053764e-02 -1.24818478e-02 -8.30527209e-03\\n -3.16899531e-02 -1.62917208e-02 -8.44950415e-03  1.34990709e-02\\n -2.03760136e-02  9.23624169e-03  1.50190108e-03 -2.35626735e-02]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":24}],"source":["## DONOT CHANGE THE CODE\n","model_name = \"gemma-transformers-1.1-2b-it-v1\"\n","import pandas as pd\n","# Import saved file and view\n","embeddings_df_save_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\")\n","text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n","text_chunks_and_embedding_df_load.head()"]},{"cell_type":"markdown","metadata":{"id":"nJV6aK6Bp1t9"},"source":["## 1. RAG - Search and Answer(70)\n","\n","We discussed RAG briefly in the beginning but let's quickly recap.\n","\n","RAG stands for Retrieval Augmented Generation.\n","\n","Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n","\n","Let's breakdown each step:\n","* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n","* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n","* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n","\n","The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n","\n","This is an incredibly helpful tool.\n","\n","Let's say you had 1000s of customer support documents.\n","\n","You could use RAG to generate direct answers to questions with links to relevant documentation.\n","\n","Or you were an insurance company with large chains of claims emails.\n","\n","You could use RAG to answer questions about the emails with sources.\n","\n","One helpful analogy is to think of LLMs as calculators for words.\n","\n","With good inputs, the LLM can sort them into helpful outputs.\n","\n","How?\n","\n","It starts with better search."]},{"cell_type":"markdown","metadata":{"id":"X9qMf9nKp1t9"},"source":["### Similarity search\n","\n","Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n","\n","If this sounds like woo, woo. It's not.\n","\n","Perhaps searching via *meaning* is a better analogy.\n","\n","With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n","\n","Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n","\n","And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n","\n","> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",">\n",">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"F1eG0nMBp1t9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242864447,"user_tz":240,"elapsed":507,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"95cb403e-bc69-4e0e-a9da-aa1be57dda51"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1680, 768])"]},"metadata":{},"execution_count":25}],"source":["import random\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Import texts and embedding df\n","text_chunks_and_embedding_df = pd.read_csv(os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\"))\n","\n","# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n","text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n","\n","# Convert texts and embedding df to list of dicts\n","pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n","\n","# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n","embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n","embeddings.shape"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"4qWPNWFvp1t9","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1746242864563,"user_tz":240,"elapsed":55,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"40a27311-8547-4260-feea-31bdadde387c","collapsed":true},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   page_number                                     sentence_chunk  \\\n","0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n","1          -38  Human Nutrition: 2020 Edition by University of...   \n","2          -37  Contents Preface University of Hawai‘i at Māno...   \n","3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n","4          -35  The Cardiovascular System University of Hawai‘...   \n","\n","   chunk_char_count  chunk_word_count  chunk_token_count  \\\n","0               308                42              77.00   \n","1               210                30              52.50   \n","2               766               114             191.50   \n","3               941               142             235.25   \n","4               998               152             249.50   \n","\n","                                           embedding  \n","0  [0.0674242675, 0.0902281404, -0.00509548886, -...  \n","1  [0.0552156419, 0.0592139773, -0.0166167244, -0...  \n","2  [0.0279801842, 0.0339813754, -0.020642668, 0.0...  \n","3  [0.0682566911, 0.0381275006, -0.00846854132, -...  \n","4  [0.0330264494, -0.0084976349, 0.00957159605, -...  "],"text/html":["\n","  <div id=\"df-c966a4c4-be77-4a1e-9678-43c7d0b65dd5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>page_number</th>\n","      <th>sentence_chunk</th>\n","      <th>chunk_char_count</th>\n","      <th>chunk_word_count</th>\n","      <th>chunk_token_count</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-39</td>\n","      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n","      <td>308</td>\n","      <td>42</td>\n","      <td>77.00</td>\n","      <td>[0.0674242675, 0.0902281404, -0.00509548886, -...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-38</td>\n","      <td>Human Nutrition: 2020 Edition by University of...</td>\n","      <td>210</td>\n","      <td>30</td>\n","      <td>52.50</td>\n","      <td>[0.0552156419, 0.0592139773, -0.0166167244, -0...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-37</td>\n","      <td>Contents Preface University of Hawai‘i at Māno...</td>\n","      <td>766</td>\n","      <td>114</td>\n","      <td>191.50</td>\n","      <td>[0.0279801842, 0.0339813754, -0.020642668, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-36</td>\n","      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n","      <td>941</td>\n","      <td>142</td>\n","      <td>235.25</td>\n","      <td>[0.0682566911, 0.0381275006, -0.00846854132, -...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-35</td>\n","      <td>The Cardiovascular System University of Hawai‘...</td>\n","      <td>998</td>\n","      <td>152</td>\n","      <td>249.50</td>\n","      <td>[0.0330264494, -0.0084976349, 0.00957159605, -...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c966a4c4-be77-4a1e-9678-43c7d0b65dd5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-c966a4c4-be77-4a1e-9678-43c7d0b65dd5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-c966a4c4-be77-4a1e-9678-43c7d0b65dd5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-d0d7d42a-948a-4652-935e-11cb46f44779\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d0d7d42a-948a-4652-935e-11cb46f44779')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-d0d7d42a-948a-4652-935e-11cb46f44779 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"text_chunks_and_embedding_df","summary":"{\n  \"name\": \"text_chunks_and_embedding_df\",\n  \"rows\": 1680,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349,\n        \"min\": -39,\n        \"max\": 1166,\n        \"num_unique_values\": 1136,\n        \"samples\": [\n          795,\n          918,\n          265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"The major determinants of Type 2 diabetes that can be changed are overnutrition and a sedentary lifestyle. Therefore, reversing or improving these factors by lifestyle interventions markedly improve the overall health of Type 2 diabetics and lower blood-glucose levels. In fact it has been shown that when people are overweight, losing as little as nine pounds (four kilograms) decreases blood- glucose levels in Type 2 diabetics. The Diabetes Prevention Trial demonstrated that by adhering to a diet containing between 1,200 and 1,800 kilocalories per day with a dietary fat intake goal of less than 25 percent and increasing physical activity to at least 150 minutes per week, people at high risk for Type 2 diabetes achieved a weight loss of 7 percent and significantly decreased their chances of developing Type 2 diabetes.15 The American Diabetes Association (ADA) has a website that provides information and tips for helping diabetics answer the question, \\u201cWhat Can I Eat\\u201d. In regard to carbohydrates the ADA recommends diabetics keep track of the carbohydrates they eat and set a limit. These dietary practices will help keep blood-glucose levels in the target range. Figure 18.5 Metabolic Syndrome: A Combination of Risk Factors Increasing the Chances for Chronic Disease 15.\\u00a0Knowler WC. (2002). Reduction in the Incidence of Type 2 Diabetes with Lifestyle Intervention or Metformin.\",\n          \"Scheme of a micelle formed by phospholipid s in an aqueous solution by Emmanuel Boutet /\\u00a0CC BY-SA 3.0 cholesterol so it acts as an emulsifier. It attracts and holds onto fat while it is simultaneously attracted to and held on to by water. Emulsification increases the surface area of lipids over a thousand- fold, making them more accessible to the digestive enzymes. Once the stomach contents have been emulsified, fat-breaking enzymes work on the triglycerides and diglycerides to sever fatty acids from their glycerol foundations. As pancreatic lipase enters the small intestine, it breaks down the fats into free fatty acids and monoglycerides. Yet again, another hurdle presents itself. How will the fats pass through the watery layer of mucus that coats the absorptive lining of the digestive tract?As before, the answer is bile. Bile salts envelop the fatty acids and monoglycerides to form micelles. Micelles have a fatty acid core with a water-soluble exterior.\",\n          \"Image by\\u00a0Gtirouflet / CC BY-SA 3.0 dense cortical bone is about 10 percent porous and it looks like many concentric circles, similar to the rings in a tree trunk, sandwiched together (Figure 2.27 \\u201cCortical (Compact) Bone\\u201d). Cortical bone tissue makes up approximately 80 percent of the adult skeleton. It surrounds all trabecular tissue and is the only bone tissue in the shafts of long bones. Figure 2.26 The Arrangement of Bone Tissues The two basic tissue types of bones are trabecular and cortical. This photo shows normal (left) and degraded (right) trabecular (spongy) bone. Figure 2.27 Cortical (Compact) Bone. The Skeletal System | 123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415,\n        \"min\": 121,\n        \"max\": 1831,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          421,\n          777,\n          1617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66,\n        \"min\": 9,\n        \"max\": 297,\n        \"num_unique_values\": 257,\n        \"samples\": [\n          227,\n          276,\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.84074677997317,\n        \"min\": 30.25,\n        \"max\": 457.75,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          105.25,\n          194.25,\n          404.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":26}],"source":["text_chunks_and_embedding_df.head()"]},{"cell_type":"markdown","metadata":{"id":"XyH8Hd7up1t9"},"source":["Nice!\n","\n","Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"KUv4BVvhp1t9","executionInfo":{"status":"ok","timestamp":1746242868568,"user_tz":240,"elapsed":2685,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"collapsed":true},"outputs":[],"source":["device=\"cuda\"\n","from sentence_transformers import util, SentenceTransformer\n","\n","embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n","                                      device=device) # choose the device to load the model to"]},{"cell_type":"markdown","metadata":{"id":"ozJqrrX7p1t9"},"source":["Embedding model ready!\n","\n","Time to perform a semantic search.\n","\n","Let's say you were studying the macronutrients.\n","\n","And wanted to search your textbook for \"macronutrients functions\".\n","\n","Well, we can do so with the following steps:\n","1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n","2. Turn the query string in an embedding with same model we used to embed our text chunks.\n","3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n","4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts.\n","\n","Easy!\n"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"qgFJ8TuCp1t9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242868575,"user_tz":240,"elapsed":6,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"996b3014-2898-4d8e-f6b4-8f6bcf0da9e9","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: macronutrients functions\n","Time take to get scores on 1680 embeddings: 0.00018 seconds.\n"]},{"output_type":"execute_result","data":{"text/plain":["torch.return_types.topk(\n","values=tensor([0.6926, 0.6738, 0.6646, 0.6536, 0.6473], device='cuda:0'),\n","indices=tensor([42, 47, 41, 51, 46], device='cuda:0'))"]},"metadata":{},"execution_count":28}],"source":["# 1. Define the query\n","# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n","query = \"macronutrients functions\"\n","print(f\"Query: {query}\")\n","\n","# 2. Embed the query to the same numerical space as the text examples\n","# Note: It's important to embed your query with the same model you embedded your examples with.\n","query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n","\n","# 3. Get similarity scores with the dot product (we'll time this for fun)\n","from time import perf_counter as timer\n","\n","start_time = timer()\n","dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n","end_time = timer()\n","\n","print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n","\n","# 4. Get the top-k results (we'll keep this to 5)\n","top_results_dot_product = torch.topk(dot_scores, k=5)\n","top_results_dot_product"]},{"cell_type":"markdown","metadata":{"id":"gw9u6NTsp1t9"},"source":["We can get pretty far by just storing our embeddings in `torch.tensor` for now.\n","\n","However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n","\n","Let's check the results of our original similarity search.\n","\n","[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n","\n","The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n","\n","We can use those indicies to map back to our text chunks.\n","\n","First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."]},{"cell_type":"code","execution_count":29,"metadata":{"id":"lyfjL-tJp1t9","executionInfo":{"status":"ok","timestamp":1746242869127,"user_tz":240,"elapsed":5,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["# Define helper function to print wrapped text\n","import textwrap\n","\n","def print_wrapped(text, wrap_length=80):\n","    wrapped_text = textwrap.fill(text, wrap_length)\n","    print(wrapped_text)"]},{"cell_type":"markdown","metadata":{"id":"QSFfTp-7p1t-"},"source":["Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n","\n","Sounds like a lot but we can do it!"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"jQSqCePyp1t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242870210,"user_tz":240,"elapsed":16,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"39c6350c-a1e4-4d2a-a044-5d76ee2ab572"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: 'macronutrients functions'\n","\n","Results:\n","Score: 0.6926\n","Text:\n","Macronutrients Nutrients that are needed in large amounts are called\n","macronutrients. There are three classes of macronutrients: carbohydrates,\n","lipids, and proteins. These can be metabolically processed into cellular energy.\n","The energy from macronutrients comes from their chemical bonds. This chemical\n","energy is converted into cellular energy that is then utilized to perform work,\n","allowing our bodies to conduct their basic functions. A unit of measurement of\n","food energy is the calorie. On nutrition food labels the amount given for\n","“calories” is actually equivalent to each calorie multiplied by one thousand. A\n","kilocalorie (one thousand calories, denoted with a small “c”) is synonymous with\n","the “Calorie” (with a capital “C”) on nutrition food labels. Water is also a\n","macronutrient in the sense that you require a large amount of it, but unlike the\n","other macronutrients, it does not yield calories. Carbohydrates Carbohydrates\n","are molecules composed of carbon, hydrogen, and oxygen.\n","Page number: 5\n","\n","\n","Score: 0.6738\n","Text:\n","Water There is one other nutrient that we must have in large quantities: water.\n","Water does not contain carbon, but is composed of two hydrogens and one oxygen\n","per molecule of water. More than 60 percent of your total body weight is water.\n","Without it, nothing could be transported in or out of the body, chemical\n","reactions would not occur, organs would not be cushioned, and body temperature\n","would fluctuate widely. On average, an adult consumes just over two liters of\n","water per day from food and drink combined. Since water is so critical for\n","life’s basic processes, the amount of water input and output is supremely\n","important, a topic we will explore in detail in Chapter 4. Micronutrients\n","Micronutrients are nutrients required by the body in lesser amounts, but are\n","still essential for carrying out bodily functions. Micronutrients include all\n","the essential minerals and vitamins. There are sixteen essential minerals and\n","thirteen vitamins (See Table 1.1 “Minerals and Their Major Functions” and Table\n","1.2 “Vitamins and Their Major Functions” for a complete list and their major\n","functions). In contrast to carbohydrates, lipids, and proteins, micronutrients\n","are not sources of energy (calories), but they assist in the process as\n","cofactors or components of enzymes (i.e., coenzymes).\n","Page number: 8\n","\n","\n","Score: 0.6646\n","Text:\n","Learning Objectives By the end of this chapter, you will be able to: • Describe\n","basic concepts in nutrition • Describe factors that affect your nutritional\n","needs • Describe the importance of research and scientific methods to\n","understanding nutrition What are Nutrients? The foods we eat contain nutrients.\n","Nutrients are substances required by the body to perform its basic functions.\n","Nutrients must be obtained from our diet, since the human body does not\n","synthesize or produce them. Nutrients have one or more of three basic functions:\n","they provide energy, contribute to body structure, and/or regulate chemical\n","processes in the body. These basic functions allow us to detect and respond to\n","environmental surroundings, move, excrete wastes, respire (breathe), grow, and\n","reproduce. There are six classes of nutrients required for the body to function\n","and maintain overall health. These are carbohydrates, lipids, proteins, water,\n","vitamins, and minerals. Foods also contain non-nutrients that may be harmful\n","(such as natural toxins common in plant foods and additives like some dyes and\n","preservatives) or beneficial (such as antioxidants). 4 | Introduction\n","Page number: 4\n","\n","\n","Score: 0.6536\n","Text:\n","Vitamins Major Functions Water-soluble Thiamin (B1) Coenzyme, energy metabolism\n","assistance Riboflavin (B2 ) Coenzyme, energy metabolism assistance Niacin (B3)\n","Coenzyme, energy metabolism assistance Pantothenic acid (B5) Coenzyme, energy\n","metabolism assistance Pyridoxine (B6) Coenzyme, amino acid synthesis assistance\n","Biotin (B7) Coenzyme, amino acid and fatty acid metabolism Folate (B9) Coenzyme,\n","essential for growth Cobalamin (B12) Coenzyme, red blood cell synthesis C\n","(ascorbic acid) Collagen synthesis, antioxidant Fat-soluble A Vision,\n","reproduction, immune system function D Bone and teeth health maintenance, immune\n","system function E Antioxidant, cell membrane protection K Bone and teeth health\n","maintenance, blood clotting Vitamin deficiencies can cause severe health\n","problems and even death. For example, a deficiency in niacin causes a disease\n","called pellagra, which was common in the early twentieth century in some parts\n","of America. The common signs and symptoms of pellagra are known as the\n","“4D’s—diarrhea, dermatitis, dementia, and death.” Until scientists found out\n","that better diets relieved the signs and symptoms of pellagra, many people with\n","the disease ended up hospitalized in insane asylums awaiting death. Other\n","vitamins were also found to prevent certain disorders and diseases such as\n","scurvy (vitamin C), night blindness vitamin A, and rickets (vitamin D). Table\n","1.3 Functions of Nutrients Introduction | 11\n","Page number: 11\n","\n","\n","Score: 0.6473\n","Text:\n","Figure 1.1 The Macronutrie nts: Carbohydrat es, Lipids, Protein, and Water\n","Proteins Proteins are macromolecules composed of chains of subunits called amino\n","acids. Amino acids are simple subunits composed of carbon, oxygen, hydrogen, and\n","nitrogen. Food sources of proteins include meats, dairy products, seafood, and a\n","variety of different plant- based foods, most notably soy. The word protein\n","comes from a Greek word meaning “of primary importance,” which is an apt\n","description of these macronutrients; they are also known colloquially as the\n","“workhorses” of life. Proteins provide four kilocalories of energy per gram;\n","however providing energy is not protein’s most important function. Proteins\n","provide structure to bones, muscles and skin, and play a role in conducting most\n","of the chemical reactions that take place in the body. Scientists estimate that\n","greater than one-hundred thousand different proteins exist within the human\n","body. The genetic codes in DNA are basically protein recipes that determine the\n","order in which 20 different amino acids are bound together to make thousands of\n","specific proteins. Figure 1.1 The Macronutrients: Carbohydrates, Lipids,\n","Protein, and Water Introduction | 7\n","Page number: 7\n","\n","\n"]}],"source":["print(f\"Query: '{query}'\\n\")\n","print(\"Results:\")\n","# Loop through zipped together scores and indicies from torch.topk\n","for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n","    print(f\"Score: {score:.4f}\")\n","    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n","    print(\"Text:\")\n","    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n","    # Print the page number too so we can reference the textbook further (and check the results)\n","    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n","    print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"wHkRMk9qp1t-"},"source":["### Similarity measures: dot product and cosine similarity\n","\n","Let's talk similarity measures between vectors.\n","\n","Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n","\n","Two of the most common you'll across are the dot product and cosine similarity.\n","\n","They are quite similar.\n","\n","The main difference is that cosine similarity has a normalization step.\n","\n","| Similarity measure | Description | Code |\n","| ----- | ----- | ----- |\n","| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) |\n","| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n","\n","For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude.\n","\n","In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n","\n","To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n","\n","> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."]},{"cell_type":"code","execution_count":31,"metadata":{"id":"sPFnkrQ9p1t-","executionInfo":{"status":"ok","timestamp":1746242873090,"user_tz":240,"elapsed":6,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","def dot_product(vector1, vector2):\n","\n","    return torch.matmul(vector2, vector1)\n","\n","def cosine_similarity(vector1, vector2):\n","\n","    dp = dot_product(vector1, vector2).float()\n","    norm_q = torch.sqrt((vector1.float() ** 2).sum())\n","    norm_p = torch.sqrt((vector2.float() ** 2).sum())\n","\n","    return dp / (norm_q * norm_p)"]},{"cell_type":"code","execution_count":32,"metadata":{"id":"1w_ecaKzolK_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242874081,"user_tz":240,"elapsed":8,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"6bdfa911-1c84-45c4-dba9-61e91ef912c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dot product between vector1 and vector2: tensor([[ 68, 170, 113],\n","        [ 46, 113,  75],\n","        [ 54, 149,  92]])\n","Dot product between vector1 and vector3: tensor([[ 98, 291, 182],\n","        [ 84, 234, 161],\n","        [ 90, 255, 179]])\n","Dot product between vector1 and vector4: tensor([[136, 376, 254],\n","        [114, 379, 237],\n","        [130, 347, 229]])\n","Cosine similarity between vector1 and vector2: tensor([[0.1898, 0.4746, 0.3155],\n","        [0.1284, 0.3155, 0.2094],\n","        [0.1508, 0.4160, 0.2568]])\n","Cosine similarity between vector1 and vector3: tensor([[0.1584, 0.4704, 0.2942],\n","        [0.1358, 0.3782, 0.2602],\n","        [0.1455, 0.4122, 0.2893]])\n","Cosine similarity between vector1 and vector4: tensor([[0.1546, 0.4275, 0.2888],\n","        [0.1296, 0.4309, 0.2694],\n","        [0.1478, 0.3945, 0.2603]])\n"]}],"source":["# Tests for above functions\n","\n","# Example tensors\n","torch.manual_seed(42)\n","vector1 = torch.randint(low=0, high=10, size=(3,3))\n","vector2 = torch.randint(low=5, high=15, size=(3,3))\n","vector3 = torch.randint(low=10, high=20, size=(3,3))\n","vector4 = torch.randint(low=15, high=30, size=(3,3))\n","\n","# Calculate dot product\n","print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n","assert torch.equal(dot_product(vector1, vector2),torch.tensor([[ 68, 170, 113],\n","                                                      [ 46, 113,  75],\n","                                                      [ 54, 149,  92]]))\n","print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n","assert torch.equal(dot_product(vector1, vector3),torch.tensor([[ 98, 291, 182],\n","                                                         [ 84, 234, 161],\n","                                                         [ 90, 255, 179]]))\n","\n","print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n","assert torch.equal(dot_product(vector1, vector4),torch.tensor([[136, 376, 254],\n","                                                               [114, 379, 237],\n","                                                               [130, 347, 229]]))\n","\n","def test_close(a, b, eps=1e-4):\n","    # Now depending on what a, b are you can add code here\n","    # return abs(a-b)<eps # If scaler\n","    return (abs(a-b)<eps).all() # If array\n","\n","# Calculate cosine similarity\n","print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n","assert test_close(cosine_similarity(vector1, vector2),torch.tensor([[0.1898, 0.4746, 0.3155],\n","                                                                    [0.1284, 0.3155, 0.2094],\n","                                                                    [0.1508, 0.4160, 0.2568]]))\n","print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n","assert test_close(cosine_similarity(vector1, vector3),torch.tensor([[0.1584, 0.4704, 0.2942],\n","                                                                    [0.1358, 0.3782, 0.2602],\n","                                                                    [0.1455, 0.4122, 0.2893]]))\n","print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))\n","assert test_close(cosine_similarity(vector1, vector4),torch.tensor([[0.1546, 0.4275, 0.2888],\n","                                                                     [0.1296, 0.4309, 0.2694],\n","                                                                     [0.1478, 0.3945, 0.2603]]))"]},{"cell_type":"markdown","metadata":{"id":"ZFC78m7_p1t-"},"source":["Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n","\n","Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity).\n","\n","But comparing `vector1` and `vector4` the result is in the negative direction.\n","\n","This makes sense because `vector4` is the negative version of `vector1`.\n","\n","Whereas comparing `vector1` and `vector3` shows a different outcome.\n","\n","For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n","\n","However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n","\n","It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n","\n","And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n","\n","The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n","\n","So we can continue using the dot product (cosine similarity is dot product + normalization).\n","\n","With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them."]},{"cell_type":"markdown","metadata":{"id":"zI9DwA5mp1t-"},"source":["### Functionizing our semantic search pipeline\n","\n","Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."]},{"cell_type":"code","execution_count":33,"metadata":{"id":"49Y_eqcRp1t-","executionInfo":{"status":"ok","timestamp":1746242876634,"user_tz":240,"elapsed":8,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["def retrieve_relevant_resources(query: str,\n","                                embeddings: torch.tensor,\n","                                model: SentenceTransformer=embedding_model,\n","                                n_resources_to_return: int=5,\n","                                product=\"cosine\"\n","                                ):\n","    \"\"\"\n","    Embeds a query with model and returns top k scores and indices from embeddings.\n","    return scores, indices\n","    \"\"\"\n","\n","    # Embed the query\n","    # (You can refer to previous code on how to embed query. (remember to convert it to Tensor)\n","    # TODO\n","    query_embedding = model.encode(query, convert_to_tensor=True).view(-1)\n","\n","\n","    if product==\"cosine\":\n","       dot_scores = cosine_similarity(query_embedding, embeddings)\n","    if product==\"dot_product\":\n","       dot_scores = dot_product(query_embedding, embeddings)\n","    if product==\"utils\":\n","       dot_scores = util.dot_score(query_embedding, embeddings)[0]\n","\n","    #Obtain Scores and Indices for top k documents\n","    # TODO\n","    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n","\n","    return scores, indices\n","\n","\n","def print_top_results_and_scores(query: str,\n","                                 embeddings: torch.tensor,\n","                                 pages_and_chunks: list[dict]=pages_and_chunks,\n","                                 n_resources_to_return: int=5,\n","                                 product=\"cosine\"):\n","    \"\"\"\n","    Takes a query, retrieves most relevant resources and prints them out in descending order.\n","\n","    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n","    \"\"\"\n","\n","    #Obtain Scores and Indices using retrieve_relevant_resources\n","    #TODO\n","    scores, indices = retrieve_relevant_resources(query=query,\n","                              embeddings=embeddings,\n","                              n_resources_to_return=n_resources_to_return,\n","                              product=\"cosine\")\n","\n","    print(f\"Query: {query}\\n\")\n","    print(\"Results:\")\n","    # Loop through zipped together scores and indicies\n","    for score, index in zip(scores, indices):\n","        print(f\"Score: {score:.4f}\")\n","        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n","        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n","        # Print the page number too so we can reference the textbook further and check the results\n","        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n","        print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"qMvdFdlZp1t-"},"source":["Excellent! Now let's test our functions out."]},{"cell_type":"code","execution_count":34,"metadata":{"id":"o7ZfzsMwp1t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242878472,"user_tz":240,"elapsed":42,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"428253bd-b97e-4105-f6de-213db14a2e4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: symptoms of pellagra\n","\n","Results:\n","Score: 0.0122\n","Niacin deficiency is commonly known as pellagra and the symptoms include\n","fatigue, decreased appetite, and indigestion.  These symptoms are then commonly\n","followed by the four D’s: diarrhea, dermatitis, dementia, and sometimes death.\n","Figure 9.12  Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n","Page number: 565\n","\n","\n","Score: 0.0091\n","car. Does it drive faster with a half-tank of gas or a full one?It does not\n","matter; the car drives just as fast as long as it has gas. Similarly, depletion\n","of B vitamins will cause problems in energy metabolism, but having more than is\n","required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n","beware; B vitamins are not stored in the body and all excess will be flushed\n","down the toilet along with the extra money spent. B vitamins are naturally\n","present in numerous foods, and many other foods are enriched with them. In the\n","United States, B-vitamin deficiencies are rare; however in the nineteenth\n","century some vitamin-B deficiencies plagued many people in North America. Niacin\n","deficiency, also known as pellagra, was prominent in poorer Americans whose main\n","dietary staple was refined cornmeal. Its symptoms were severe and included\n","diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n","of pellagra are the result of niacin being in insufficient supply to support the\n","body’s metabolic functions.\n","Page number: 591\n","\n","\n","Score: 0.0072\n","The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n","porosity and tenderness. For those who are sensitive to gluten, it is good to\n","know that corn, millet, buckwheat, and oats do not contain the proteins that\n","make gluten. However, some people who have celiac disease also may have a\n","response to products containing oats. This is most likely the result of cross-\n","contamination of grains during harvest, storage, packaging, and processing.\n","Celiac disease is most common in people of European descent and is rare in\n","people of African American, Japanese, and Chinese descent. It is much more\n","prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n","disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n","and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n","pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n","can appear in infancy or much later in life, even Nutrition, Health and Disease\n","| 1079\n","Page number: 1079\n","\n","\n","Score: 0.0068\n","Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n","can cause the spine to curve. The increase in spine curvature not only causes\n","pain, but also decreases a person’s height. Curvature of the upper spine\n","produces what is called Dowager’s hump, also known as kyphosis. Severe upper-\n","spine deformity can compress the chest cavity and cause difficulty breathing. It\n","may also cause abdominal pain and loss of appetite because of the increased\n","pressure on the abdomen. 1090 | Nutrition, Health and Disease\n","Page number: 1090\n","\n","\n","Score: 0.0066\n","esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n","percent of the US population. An analysis of several studies published in the\n","August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n","more prevalent in people who are obese.1 The most common GERD symptom is\n","heartburn, but people with GERD may also experience regurgitation (flow of the\n","stomach’s acidic contents into the mouth), frequent coughing, and trouble\n","swallowing. There are other causative factors of GERD that may be separate from\n","or intertwined with obesity. The sphincter that separates the stomach’s internal\n","contents from the esophagus often does not function properly and acidic gastric\n","contents seep upward. Sometimes the peristaltic contractions of the esophagus\n","are also sluggish and compromise the clearance of acidic contents. In addition\n","to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n","particular foods—chocolate, garlic, spicy foods, fried foods, and tomato-based\n","foods—which worsen symptoms. Drinks containing alcohol or caffeine may also\n","worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n","of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n","a small device into the lower esophagus that measures the acidity of the\n","contents during one’s daily activities.\n","Page number: 1077\n","\n","\n"]}],"source":["query = \"symptoms of pellagra\"\n","\n","# Get just the scores and indices of top related results\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings,\n","                                              product=\"cosine\")\n","scores, indices\n","\n","# Print out the texts of the top scores\n","print_top_results_and_scores(query=query,\n","                             embeddings=embeddings)"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"4xTlGbApp1t-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242880314,"user_tz":240,"elapsed":36,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"a91012ef-8fac-49bf-e9f6-b3215c6728ea"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: symptoms of pellagra\n","\n","Results:\n","Score: 0.0122\n","Niacin deficiency is commonly known as pellagra and the symptoms include\n","fatigue, decreased appetite, and indigestion.  These symptoms are then commonly\n","followed by the four D’s: diarrhea, dermatitis, dementia, and sometimes death.\n","Figure 9.12  Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n","Page number: 565\n","\n","\n","Score: 0.0091\n","car. Does it drive faster with a half-tank of gas or a full one?It does not\n","matter; the car drives just as fast as long as it has gas. Similarly, depletion\n","of B vitamins will cause problems in energy metabolism, but having more than is\n","required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n","beware; B vitamins are not stored in the body and all excess will be flushed\n","down the toilet along with the extra money spent. B vitamins are naturally\n","present in numerous foods, and many other foods are enriched with them. In the\n","United States, B-vitamin deficiencies are rare; however in the nineteenth\n","century some vitamin-B deficiencies plagued many people in North America. Niacin\n","deficiency, also known as pellagra, was prominent in poorer Americans whose main\n","dietary staple was refined cornmeal. Its symptoms were severe and included\n","diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n","of pellagra are the result of niacin being in insufficient supply to support the\n","body’s metabolic functions.\n","Page number: 591\n","\n","\n","Score: 0.0072\n","The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n","porosity and tenderness. For those who are sensitive to gluten, it is good to\n","know that corn, millet, buckwheat, and oats do not contain the proteins that\n","make gluten. However, some people who have celiac disease also may have a\n","response to products containing oats. This is most likely the result of cross-\n","contamination of grains during harvest, storage, packaging, and processing.\n","Celiac disease is most common in people of European descent and is rare in\n","people of African American, Japanese, and Chinese descent. It is much more\n","prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n","disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n","and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n","pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n","can appear in infancy or much later in life, even Nutrition, Health and Disease\n","| 1079\n","Page number: 1079\n","\n","\n","Score: 0.0068\n","Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n","can cause the spine to curve. The increase in spine curvature not only causes\n","pain, but also decreases a person’s height. Curvature of the upper spine\n","produces what is called Dowager’s hump, also known as kyphosis. Severe upper-\n","spine deformity can compress the chest cavity and cause difficulty breathing. It\n","may also cause abdominal pain and loss of appetite because of the increased\n","pressure on the abdomen. 1090 | Nutrition, Health and Disease\n","Page number: 1090\n","\n","\n","Score: 0.0066\n","esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n","percent of the US population. An analysis of several studies published in the\n","August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n","more prevalent in people who are obese.1 The most common GERD symptom is\n","heartburn, but people with GERD may also experience regurgitation (flow of the\n","stomach’s acidic contents into the mouth), frequent coughing, and trouble\n","swallowing. There are other causative factors of GERD that may be separate from\n","or intertwined with obesity. The sphincter that separates the stomach’s internal\n","contents from the esophagus often does not function properly and acidic gastric\n","contents seep upward. Sometimes the peristaltic contractions of the esophagus\n","are also sluggish and compromise the clearance of acidic contents. In addition\n","to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n","particular foods—chocolate, garlic, spicy foods, fried foods, and tomato-based\n","foods—which worsen symptoms. Drinks containing alcohol or caffeine may also\n","worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n","of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n","a small device into the lower esophagus that measures the acidity of the\n","contents during one’s daily activities.\n","Page number: 1077\n","\n","\n"]}],"source":["query = \"symptoms of pellagra\"\n","\n","# Get just the scores and indices of top related results\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings,\n","                                              product=\"dot_product\")\n","scores, indices\n","\n","# Print out the texts of the top scores\n","print_top_results_and_scores(query=query,\n","                             embeddings=embeddings)"]},{"cell_type":"code","execution_count":36,"metadata":{"id":"jB8r8S-b_ygc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242882126,"user_tz":240,"elapsed":42,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"eb7082ec-f8f9-49fe-9eee-b245256b64c5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: symptoms of pellagra\n","\n","Results:\n","Score: 0.0122\n","Niacin deficiency is commonly known as pellagra and the symptoms include\n","fatigue, decreased appetite, and indigestion.  These symptoms are then commonly\n","followed by the four D’s: diarrhea, dermatitis, dementia, and sometimes death.\n","Figure 9.12  Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n","Page number: 565\n","\n","\n","Score: 0.0091\n","car. Does it drive faster with a half-tank of gas or a full one?It does not\n","matter; the car drives just as fast as long as it has gas. Similarly, depletion\n","of B vitamins will cause problems in energy metabolism, but having more than is\n","required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n","beware; B vitamins are not stored in the body and all excess will be flushed\n","down the toilet along with the extra money spent. B vitamins are naturally\n","present in numerous foods, and many other foods are enriched with them. In the\n","United States, B-vitamin deficiencies are rare; however in the nineteenth\n","century some vitamin-B deficiencies plagued many people in North America. Niacin\n","deficiency, also known as pellagra, was prominent in poorer Americans whose main\n","dietary staple was refined cornmeal. Its symptoms were severe and included\n","diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n","of pellagra are the result of niacin being in insufficient supply to support the\n","body’s metabolic functions.\n","Page number: 591\n","\n","\n","Score: 0.0072\n","The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n","porosity and tenderness. For those who are sensitive to gluten, it is good to\n","know that corn, millet, buckwheat, and oats do not contain the proteins that\n","make gluten. However, some people who have celiac disease also may have a\n","response to products containing oats. This is most likely the result of cross-\n","contamination of grains during harvest, storage, packaging, and processing.\n","Celiac disease is most common in people of European descent and is rare in\n","people of African American, Japanese, and Chinese descent. It is much more\n","prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n","disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n","and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n","pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n","can appear in infancy or much later in life, even Nutrition, Health and Disease\n","| 1079\n","Page number: 1079\n","\n","\n","Score: 0.0068\n","Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n","can cause the spine to curve. The increase in spine curvature not only causes\n","pain, but also decreases a person’s height. Curvature of the upper spine\n","produces what is called Dowager’s hump, also known as kyphosis. Severe upper-\n","spine deformity can compress the chest cavity and cause difficulty breathing. It\n","may also cause abdominal pain and loss of appetite because of the increased\n","pressure on the abdomen. 1090 | Nutrition, Health and Disease\n","Page number: 1090\n","\n","\n","Score: 0.0066\n","esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n","percent of the US population. An analysis of several studies published in the\n","August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n","more prevalent in people who are obese.1 The most common GERD symptom is\n","heartburn, but people with GERD may also experience regurgitation (flow of the\n","stomach’s acidic contents into the mouth), frequent coughing, and trouble\n","swallowing. There are other causative factors of GERD that may be separate from\n","or intertwined with obesity. The sphincter that separates the stomach’s internal\n","contents from the esophagus often does not function properly and acidic gastric\n","contents seep upward. Sometimes the peristaltic contractions of the esophagus\n","are also sluggish and compromise the clearance of acidic contents. In addition\n","to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n","particular foods—chocolate, garlic, spicy foods, fried foods, and tomato-based\n","foods—which worsen symptoms. Drinks containing alcohol or caffeine may also\n","worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n","of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n","a small device into the lower esophagus that measures the acidity of the\n","contents during one’s daily activities.\n","Page number: 1077\n","\n","\n"]}],"source":["query = \"symptoms of pellagra\"\n","\n","# Get just the scores and indices of top related results\n","scores, indices = retrieve_relevant_resources(query=query,\n","                                              embeddings=embeddings,\n","                                              product=\"utils\")\n","scores, indices\n","\n","# Print out the texts of the top scores\n","print_top_results_and_scores(query=query,\n","                             embeddings=embeddings)"]},{"cell_type":"markdown","metadata":{"id":"xEkVj3O06-Hu"},"source":["##Try 3 different queries for dot and cosine similarity and comment on differences in top k results if any. Also try product=\"utils\", does the score and final query change?"]},{"cell_type":"markdown","metadata":{"id":"yJUmwErOp1t_"},"source":["### Loading an LLM locally\n","\n","We will do it for `gemma-2b-it`\n","\n","\n","The Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n","\n","To load our LLM, we're going to need a few things:\n","1. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-2b-it`.2. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n","3. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n","\n","\n","\n","\n","Let's do it!"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"iA-6HRpSjoR_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242885778,"user_tz":240,"elapsed":4,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"295316d4-5aa5-4bdc-f8f2-8fc389c63d81"},"outputs":[{"output_type":"stream","name":"stdout","text":["model_id set to: /content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1\n"]}],"source":["import os\n","# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n","\n","model_id = os.path.join(\"/content/drive/MyDrive/\",model_name)\n","print(f\"model_id set to: {model_id}\")"]},{"cell_type":"code","execution_count":38,"metadata":{"id":"nJFS5kKFp1t_","colab":{"base_uri":"https://localhost:8080/","height":535,"referenced_widgets":["864e532029864e5b9cc164df1d4a390d","e6e406332a934913b66a89d8d3874764","4835b396934d4cd79d3c9e58adf51a89","92960dd91f784936bb68957af8eb2ff0","bd3ffcbd17414f7da83b0fd57fc666b1","a2de276b8f6d4475ba40a7fe9cc969db","a0d0f5fecafa4577bc03f30719b8ff2d","e0f8ad477d444faf9167802f7510fd47","fa91a3ae28324f018ba6a50c00da948b","b6cdfb7c6a124ad3abbae41a31dec9f2","1abece0922ad4dbba211c4c87816b3d7"]},"executionInfo":{"status":"ok","timestamp":1746242891567,"user_tz":240,"elapsed":5085,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"59385c6b-9ca9-4264-94b6-712d9c980253"},"outputs":[{"output_type":"stream","name":"stdout","text":["[INFO] Using attention implementation: sdpa\n","[INFO] Using model_id: /content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"864e532029864e5b9cc164df1d4a390d"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["GemmaForCausalLM(\n","  (model): GemmaModel(\n","    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n","    (layers): ModuleList(\n","      (0-17): 18 x GemmaDecoderLayer(\n","        (self_attn): GemmaAttention(\n","          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n","          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n","        )\n","        (mlp): GemmaMLP(\n","          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n","          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n","          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n","          (act_fn): PytorchGELUTanh()\n","        )\n","        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n","        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n","      )\n","    )\n","    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n","    (rotary_emb): GemmaRotaryEmbedding()\n","  )\n","  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",")"]},"metadata":{},"execution_count":38}],"source":["import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from transformers.utils import is_flash_attn_2_available\n","\n","# 1. Create quantization config for smaller model loading (optional)\n","# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n","# For models that require 4-bit quantization (use this if you have low GPU memory available)\n","\n","attn_implementation = \"sdpa\"\n","print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n","\n","# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n","#model_id = \"google/gemma-7b-it\"\n","model_id = model_id # (we already set this above)\n","print(f\"[INFO] Using model_id: {model_id}\")\n","\n","# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n","\n","# 4. Instantiate the model\n","llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n","                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n","                                                 low_cpu_mem_usage=False, # use full memory\n","                                                 attn_implementation=attn_implementation) # which attention version to use\n","\n","llm_model.to(\"cuda\")"]},{"cell_type":"markdown","metadata":{"id":"AOfjjXbCp1t_"},"source":["Parameters Of The LLM Model"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"DvioL7Iwp1t_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242895893,"user_tz":240,"elapsed":7,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"11698b05-7cd1-4f6e-f201-21c4f5a32c1b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'model_mem_bytes': 5012345344, 'model_mem_mb': 4780.15, 'model_mem_gb': 4.67}"]},"metadata":{},"execution_count":39}],"source":["def get_model_num_params(model: torch.nn.Module):\n","    return sum([param.numel() for param in model.parameters()])\n","\n","get_model_num_params(llm_model)\n","\n","def get_model_mem_size(model: torch.nn.Module):\n","    \"\"\"\n","    Get how much memory a PyTorch model takes up.\n","\n","    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n","    \"\"\"\n","    # Get model parameters and buffer sizes\n","    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n","    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n","\n","    # Calculate various model sizes\n","    model_mem_bytes = mem_params + mem_buffers # in bytes\n","    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n","    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n","\n","    return {\"model_mem_bytes\": model_mem_bytes,\n","            \"model_mem_mb\": round(model_mem_mb, 2),\n","            \"model_mem_gb\": round(model_mem_gb, 2)}\n","\n","get_model_mem_size(llm_model)"]},{"cell_type":"markdown","metadata":{"id":"PzSBE1MZp1t_"},"source":["### Generating text with our LLM\n","\n","We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n","\n","The tokenized input comes from passing a string of text to our `tokenizer`.\n","\n","It's important to note that you should use a tokenizer that has been paired with a model.\n","\n","Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n","\n","For some LLMs, there's a specific template you should pass to them for ideal outputs.\n","\n","For example, the `gemma-2b-it` model has been trained in a dialogue fashion (instruction tuning).\n","\n","In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model."]},{"cell_type":"code","execution_count":40,"metadata":{"id":"p9g0eidzp1t_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746242898120,"user_tz":240,"elapsed":4,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"c0574caf-1d84-4258-eca7-381755d1f9b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Input text:\n","What are the macronutrients, and what roles do they play in the human body?\n","\n","Prompt (formatted):\n","<bos><start_of_turn>user\n","What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n","<start_of_turn>model\n","\n"]}],"source":["input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n","print(f\"Input text:\\n{input_text}\")\n","\n","# Create prompt template for instruction-tuned model\n","dialogue_template = [\n","    {\"role\": \"user\",\n","     \"content\": input_text}\n","]\n","\n","# Apply the chat template\n","prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n","                                       tokenize=False, # keep as raw text (not tokenized)\n","                                       add_generation_prompt=True)\n","print(f\"\\nPrompt (formatted):\\n{prompt}\")"]},{"cell_type":"markdown","metadata":{"id":"uPjI7UHbp1t_"},"source":["Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n","\n","Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n","\n","We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n","\n","We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."]},{"cell_type":"markdown","metadata":{"id":"vxr2mBU2p1uA"},"source":["> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Erk7RqqIp1uA","executionInfo":{"status":"ok","timestamp":1746242899877,"user_tz":240,"elapsed":6,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["## Feel free to play with the questions\n","\n","# Nutrition-style questions generated with GPT4\n","gpt4_questions = [\n","    \"What are the macronutrients, and what roles do they play in the human body?\",\n","    \"How do vitamins and minerals differ in their roles and importance for health?\",\n","    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n","    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n","    \"Explain the concept of energy balance and its importance in weight management.\"\n","]\n","\n","# Manually created question list\n","manual_questions = [\n","    \"How often should infants be breastfed?\",\n","    \"What are symptoms of pellagra?\",\n","    \"How does saliva help with digestion?\",\n","    \"What is the RDI for protein per day?\",\n","    \"water soluble vitamins\"\n","]\n","\n","query_list = gpt4_questions + manual_questions"]},{"cell_type":"markdown","metadata":{"id":"KHILOHcpp1uA"},"source":["### Augmenting our prompt with context items\n","\n","What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n","\n","In essence, we start with a base prompt and update it with context text.\n","\n","Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n","\n","We'll apply the dialogue and chat template to our prompt before returning it as well.\n","\n","> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."]},{"cell_type":"code","execution_count":62,"metadata":{"id":"ZBzmx3CIp1uA","executionInfo":{"status":"ok","timestamp":1746243298978,"user_tz":240,"elapsed":7,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["def prompt_formatter(query: str,\n","                     context_items: list[dict]) -> str:\n","    \"\"\"\n","    Augments query with text-based context from context_items.\n","    \"\"\"\n","    # Join context items into one dotted paragraph\n","    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n","\n","    # Create a base prompt with examples to help the model\n","    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n","    # We could also write this in a txt file and import it in if we wanted.\n","    base_prompt = \"\"\"Based on the following context items, please answer the query.\n","Give yourself room to think by extracting relevant passages from the context before answering the query.\n","Don't return the thinking, only return the answer.\n","Make sure your answers are as explanatory as possible.\n","Use the following examples as reference for the ideal answer style.\n","\\nExample 1:\n","Query: What are the fat-soluble vitamins?\n","Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n","\\nExample 2:\n","Query: What are the causes of type 2 diabetes?\n","Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n","\\nExample 3:\n","Query: What is the importance of hydration for physical performance?\n","Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n","\\nNow use the following context items to answer the user query:\n","{context}\n","\\nRelevant passages: <extract relevant passages from the context here>\n","User query: {query}\n","Answer:\"\"\"\n","\n","    # Update base prompt with context items and query\n","    base_prompt = base_prompt.format(context=context, query=query)\n","\n","    # Create prompt template for instruction-tuned model\n","    dialogue_template = [\n","        {\"role\": \"user\",\n","        \"content\": base_prompt}\n","    ]\n","\n","    # Apply the chat template\n","    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n","                                          tokenize=False,\n","                                          add_generation_prompt=True)\n","    return prompt"]},{"cell_type":"markdown","metadata":{"id":"n6uJHH28p1uA"},"source":["Looking good! Let's try our function out."]},{"cell_type":"code","execution_count":63,"metadata":{"id":"Z-ly04eop1uA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746243300535,"user_tz":240,"elapsed":3,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"4d560fd2-9feb-4146-854f-5e604538de4e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: How does saliva help with digestion?\n","<bos><start_of_turn>user\n","What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n","<start_of_turn>model\n","\n"]}],"source":["import random\n","\n","query = random.choice(query_list)\n","print(f\"Query: {query}\")\n","\n","# Create prompt template for instruction-tuned model\n","dialogue_template = [\n","    {\"role\": \"user\",\n","     \"content\": input_text}\n","]\n","\n","# Apply the chat template\n","prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n","                                       tokenize=False, # keep as raw text (not tokenized)\n","                                       add_generation_prompt=True)\n","\n","print(prompt)"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"HHeJZwhSp1uA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746243308085,"user_tz":240,"elapsed":6606,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"c47a4fbe-c1e4-4e74-baf0-6323fd2270f8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: How does saliva help with digestion?\n","RAG answer:\n","<bos>**Macronutrients** are the three main types of nutrients that provide the body with energy, building blocks for tissues, and support various bodily functions.\n","\n","**1. Protein:**\n","\n","- Essential for building and repairing tissues\n","- Forms enzymes, hormones, and antibodies\n","- Plays a role in muscle strength and recovery\n","- Helps transport oxygen and nutrients throughout the body\n","\n","**2. Carbohydrates:**\n","\n","- The primary source of energy for the body\n","- Provides energy to fuel activities and cell growth\n","- Forms the structure and energy stores of cells\n","- Supplies glucose, which is the body's primary energy source\n","\n","**3. Fat:**\n","\n","- Essential for energy storage and insulation\n","- Stores energy and provides a reserve of fuel\n","- Forms cell membranes and hormones\n","- Plays a role in insulation and temperature regulation\n","\n","**Role of Macronutrients in the Human Body:**\n","\n","**Energy Production:**\n","\n","- Provide the body with the energy it needs to function.\n","- Fuel cellular processes and organ function.\n","\n","**Tissue Repair and Growth:**\n","\n","- Repair and replace damaged tissues.\n","- Build and maintain muscles, bones, and other organs.\n","\n","**Immune Function:**\n","\n","- Produce white blood cells that fight infection.\n","- Support the production of antibodies and other immune\n"]}],"source":["input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","# Generate an output of tokens\n","outputs = llm_model.generate(**input_ids,\n","                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n","                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n","                             max_new_tokens=256) # how many new tokens to generate from prompt\n","\n","# Turn the output tokens into text\n","output_text = tokenizer.decode(outputs[0])\n","\n","print(f\"Query: {query}\")\n","print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"]},{"cell_type":"markdown","metadata":{"id":"GKU7tJcrp1uA"},"source":["Our RAG pipeline is complete!\n","\n","We just Retrieved, Augmented and Generated!\n","\n","And all on our own local GPU!\n","\n","How about we functionize the generation step to make it easier to use?\n","\n","We can put a little formatting on the text being returned to make it look nice too.\n","\n","And we'll make an option to return the context items if needed as well."]},{"cell_type":"code","execution_count":65,"metadata":{"id":"2sXb3FiFp1uA","executionInfo":{"status":"ok","timestamp":1746243308087,"user_tz":240,"elapsed":1,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}}},"outputs":[],"source":["def ask(query,\n","        temperature=0.7,\n","        max_new_tokens=512,\n","        format_answer_text=True,\n","        return_answer_only=True):\n","    \"\"\"\n","    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n","    \"\"\"\n","\n","    # Get just the scores and indices of top related results\n","    scores, indices = retrieve_relevant_resources(query=query,\n","                                                  embeddings=embeddings)\n","\n","    # Create a list of context items\n","    context_items = [pages_and_chunks[i] for i in indices]\n","\n","    # Add score to context item\n","    for i, item in enumerate(context_items):\n","        item[\"score\"] = scores[i].cpu() # return score back to CPU\n","\n","    # Format the prompt with context items\n","    prompt = prompt_formatter(query=query,\n","                              context_items=context_items)\n","\n","    # Tokenize the prompt\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n","\n","    # Generate an output of tokens\n","    outputs = llm_model.generate(**input_ids,\n","                                 temperature=temperature,\n","                                 do_sample=True,\n","                                 max_new_tokens=max_new_tokens)\n","\n","    # Turn the output tokens into text\n","    output_text = tokenizer.decode(outputs[0])\n","\n","    if format_answer_text:\n","        # Replace special tokens and unnecessary help message\n","        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n","\n","    # Only return the answer without the context items\n","    if return_answer_only:\n","        return output_text\n","\n","    return output_text, context_items"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"a9CaBYNLp1uA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746243391209,"user_tz":240,"elapsed":1604,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"c8337806-23b8-409b-d118-315edac49d0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Query: What are symptoms of pellagra?\n","Answer:\n","\n","**Symptoms of pellagra include:**  - Diarrhea - Dermatitis - Dementia - Loss of\n","appetite - Pale, fatty, loose stools - Gastrointestinal upset - Abdominal pain -\n","Weight loss - Failure to grow and thrive in children\n","Context items:\n"]},{"output_type":"execute_result","data":{"text/plain":["[{'page_number': 565,\n","  'sentence_chunk': 'Niacin deficiency is commonly known as pellagra and the symptoms include fatigue, decreased appetite, and indigestion. \\xa0These symptoms are then commonly followed by the four D’s: diarrhea, dermatitis, dementia, and sometimes death. Figure 9.12 \\xa0Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565',\n","  'chunk_char_count': 308,\n","  'chunk_word_count': 43,\n","  'chunk_token_count': 77.0,\n","  'embedding': array([ 2.01436747e-02,  8.51660687e-03,  3.45250294e-02,  3.82414348e-02,\n","          6.64332435e-02,  3.54784019e-02, -4.80622128e-02,  4.61080950e-03,\n","          3.34787590e-04,  1.60651766e-02,  3.92620452e-02,  6.21891469e-02,\n","         -1.17300265e-02, -5.21841682e-02,  2.41388697e-02,  5.63627146e-02,\n","         -1.34784402e-02,  3.53664602e-03,  2.67130211e-02,  2.86840610e-02,\n","         -1.54797956e-02,  1.54494308e-02,  1.49387112e-02, -1.47909895e-02,\n","         -3.16866860e-02, -4.39014062e-02,  1.52432267e-02,  1.12246827e-03,\n","          5.22842584e-03, -5.35612814e-02,  8.66692886e-03,  9.63806920e-03,\n","         -4.63740677e-02, -7.12042004e-02,  1.79701919e-06, -3.09904087e-02,\n","          2.07887986e-03, -1.88569967e-02,  4.02884372e-02,  4.72819395e-02,\n","          5.95259070e-02, -3.28605175e-02,  8.70731920e-02, -1.59079097e-02,\n","          6.93424465e-03,  7.41906138e-03, -1.26511697e-02,  5.19636050e-02,\n","          2.12589800e-02, -3.04629020e-02,  9.05239489e-03,  1.39545174e-02,\n","         -1.18368864e-02,  5.35098137e-03,  5.21608219e-02, -5.92644587e-02,\n","         -3.14331576e-02, -7.59684434e-03, -3.61897945e-02, -2.20621899e-02,\n","          4.66010235e-02, -2.06967350e-02,  1.24066500e-02, -3.30125988e-02,\n","         -6.01206757e-02,  4.06160317e-02, -5.23116030e-02, -8.65481123e-02,\n","         -2.68469378e-02,  1.61186550e-02, -5.58957346e-02,  3.64177711e-02,\n","          3.33710425e-02, -7.73112057e-03, -5.90388756e-03, -3.91221233e-02,\n","          2.13320069e-02, -1.52866729e-02, -1.06347324e-02,  1.24292402e-02,\n","          6.51582852e-02,  4.88588726e-03,  2.55723693e-03, -4.13905121e-02,\n","          9.67014879e-02, -9.65873525e-03,  3.41327898e-02, -3.93719636e-02,\n","         -3.25458571e-02, -3.78897227e-02, -7.52406102e-03,  1.86681431e-02,\n","          2.51598898e-02, -6.46342859e-02, -1.18215121e-02, -6.89716195e-04,\n","         -3.54469307e-02,  3.39496806e-02,  7.08408430e-02, -8.08711424e-02,\n","          1.16460666e-03, -2.57595908e-02, -7.43604302e-02,  7.04141147e-03,\n","         -2.25696135e-02, -4.50206138e-02, -7.01002823e-03,  2.91246106e-03,\n","         -2.54207235e-02, -8.29225965e-03,  1.96902938e-02,  1.21010188e-02,\n","         -4.49008606e-02,  3.18548903e-02,  9.53408331e-03, -1.09731862e-02,\n","          3.08105387e-02, -1.28596590e-03, -8.72647688e-02,  8.19300022e-03,\n","         -3.13223107e-03,  5.79931624e-02,  5.81145510e-02, -3.28667723e-02,\n","         -1.06409742e-02,  1.11036927e-01,  1.80809312e-02,  1.48574429e-04,\n","         -4.66622831e-03, -1.50046786e-02,  4.09693159e-02, -8.02068114e-02,\n","         -7.54609555e-02,  1.79497246e-02, -2.03563329e-02,  1.25280605e-03,\n","          7.05576837e-02,  1.99154597e-02, -4.59272340e-02,  1.59874316e-02,\n","          2.96790730e-02, -8.70421808e-03, -8.02215412e-02, -7.11355032e-03,\n","          3.38942483e-02, -3.93788284e-03, -8.17733910e-03, -9.08293203e-02,\n","         -7.85331056e-03,  1.82975400e-02,  1.59088876e-02, -4.42294031e-03,\n","         -5.25261369e-03,  2.22098175e-02,  2.06334586e-03, -2.00773533e-02,\n","          7.29211867e-02,  1.63355134e-02,  4.55578491e-02, -4.80309175e-03,\n","          1.40801147e-02,  5.55891870e-03,  2.84422375e-02, -7.31042819e-03,\n","          2.55806651e-02,  8.65328759e-02,  5.12012579e-02,  2.35733874e-02,\n","         -2.09905114e-02,  1.03960903e-02,  2.21131742e-02, -2.07281369e-03,\n","          3.74787077e-02, -1.82760227e-02, -4.51723300e-03,  2.10492201e-02,\n","          4.00412753e-02, -8.87901615e-03, -2.28812099e-02, -1.15745207e-02,\n","          1.31076658e-02, -2.62623224e-02, -1.66508660e-03, -1.01253204e-02,\n","         -5.57024851e-02, -1.09032271e-02,  5.82777895e-02,  6.43359590e-03,\n","         -1.22463023e-02, -2.00624391e-02,  8.00025091e-02,  3.45112495e-02,\n","         -3.33243832e-02, -1.18235452e-02, -2.81366389e-02,  5.08574545e-02,\n","          1.87496922e-03,  1.52046222e-03,  5.61473332e-02, -8.30861256e-02,\n","          3.65505554e-02,  2.43380927e-02,  1.29598845e-02, -2.24182922e-02,\n","         -5.47432154e-02,  1.53932991e-02, -3.37853692e-02,  6.11509494e-02,\n","         -2.45477688e-02,  1.37223499e-02, -4.23752777e-02, -2.79862303e-02,\n","          3.80876213e-02, -1.24806222e-02,  5.25672697e-02,  9.08590201e-03,\n","         -1.23057049e-02, -1.54168345e-02, -4.40402366e-02,  5.92003092e-02,\n","         -2.88868044e-03, -3.49905565e-02, -3.63882259e-02,  8.66273418e-03,\n","          1.23011135e-02, -3.66288908e-02,  3.23691429e-03,  3.51931639e-02,\n","         -5.03924228e-02,  2.23754998e-02,  4.04795408e-02,  2.21229307e-02,\n","         -4.67053279e-02, -1.15362473e-03, -2.75404621e-02, -8.44398513e-02,\n","          5.33348657e-02, -5.40810488e-02, -4.61473683e-05,  6.29030317e-02,\n","          1.44508919e-02,  1.74656026e-02, -9.05289054e-02, -3.50806229e-02,\n","          3.37347277e-02,  1.02167539e-02, -1.49566634e-02,  1.98988672e-02,\n","         -5.53820282e-02,  6.52909465e-03,  5.71482740e-02,  5.90622388e-02,\n","          9.72692948e-03, -1.34699317e-02, -4.99808304e-02,  6.83579370e-02,\n","         -3.54464240e-02, -3.82957123e-02, -8.03678781e-02, -7.38356784e-02,\n","         -1.49323056e-02,  6.54804334e-02,  2.85303518e-02,  2.14109700e-02,\n","         -3.02362777e-02,  1.66035593e-02,  2.02648295e-03,  2.62575410e-02,\n","         -2.18783133e-03,  4.70235348e-02,  2.18754727e-03, -6.35180064e-03,\n","         -2.22385488e-03, -1.72878001e-02, -4.18303870e-02, -6.21711761e-02,\n","         -1.63925979e-02,  7.09179696e-03,  2.34528352e-02,  2.89171026e-03,\n","          1.64546277e-02, -1.36233186e-02, -4.58153151e-03, -9.20590851e-03,\n","          2.70369072e-02, -1.92067288e-02,  7.81404227e-03, -4.47864011e-02,\n","          4.37837914e-02, -8.16387124e-03, -4.78716614e-03,  1.91938672e-02,\n","          2.03399621e-02,  2.59422660e-02,  1.10674789e-02, -1.86734945e-02,\n","         -1.25435337e-01,  1.34915831e-02, -3.00527047e-02,  1.56404004e-02,\n","         -1.76228378e-02,  5.53855002e-02, -1.05688283e-02,  9.49536450e-03,\n","         -2.27637719e-02,  4.13049944e-02,  1.46645345e-02, -6.49071708e-02,\n","         -1.13932081e-02,  1.71074271e-02,  4.06161323e-02,  2.05646083e-02,\n","         -1.37343919e-02, -2.49084434e-03, -3.08647882e-02, -1.14909699e-02,\n","          2.07857750e-02,  3.47385667e-02,  7.87836537e-02, -4.72479500e-02,\n","          2.50543226e-02, -1.70875918e-02, -2.18701810e-02, -5.52592427e-02,\n","         -7.69798271e-03, -9.31966305e-03,  7.54030002e-03,  2.99097523e-02,\n","         -8.42841044e-02,  3.14248800e-02, -3.23967636e-02,  4.90640625e-02,\n","         -3.86804417e-02,  2.55261194e-02,  2.02362221e-02, -2.28986423e-02,\n","          2.36052852e-02,  1.51032200e-02, -9.84392501e-03,  1.72018558e-02,\n","          1.08009139e-02,  2.33890414e-02,  2.03199107e-02,  2.97367852e-02,\n","          2.23247129e-02,  8.30422156e-03, -5.09448498e-02, -2.70952135e-02,\n","          1.96284950e-02,  2.71339826e-02, -5.78475259e-02, -1.45325037e-02,\n","          1.17504112e-02,  1.92179959e-02, -1.19939325e-02, -2.98389159e-02,\n","          2.08984185e-02, -5.14276065e-02, -2.32847892e-02,  2.11720858e-02,\n","         -1.27897486e-02,  5.38795575e-06, -3.48040052e-02, -1.45353572e-02,\n","         -1.06814541e-02,  4.09045182e-02, -5.59289828e-02, -7.59745808e-03,\n","          5.48423976e-02, -2.23280657e-02, -6.06751889e-02,  1.04750460e-02,\n","          1.37677370e-02, -3.02010104e-02, -6.38577249e-03, -1.65960118e-02,\n","         -3.00829345e-03,  2.16916688e-02, -8.37258324e-02, -5.18354252e-02,\n","          1.46310125e-02,  3.20744067e-02, -1.55870961e-02, -4.04053088e-03,\n","          4.10246029e-02,  1.44132860e-02, -2.17044037e-02,  3.60128586e-03,\n","          6.67641684e-02, -1.29332803e-02, -5.65078668e-02,  6.32132590e-02,\n","         -1.14415111e-02,  2.91307326e-02,  1.57162193e-02,  2.62897019e-03,\n","          1.98654998e-02, -2.37804167e-02,  4.03905436e-02,  5.25472574e-02,\n","          3.44017707e-02, -2.20389050e-02,  3.23821828e-02,  2.93509811e-02,\n","          5.46972863e-02, -2.05581803e-02, -4.78017181e-02,  1.00533357e-02,\n","          7.19388872e-02, -2.70631146e-02,  1.44625707e-02, -4.46009375e-02,\n","          4.69546486e-03,  2.49617845e-02,  3.44661325e-02,  1.94465946e-02,\n","         -3.95101097e-06, -4.69028354e-02,  3.05446219e-02, -6.32204488e-02,\n","          9.65563767e-03, -8.05628020e-03, -7.86782876e-02,  3.84534011e-03,\n","          5.24742715e-03, -4.73955311e-02,  4.84081218e-03,  2.06521899e-02,\n","         -1.38298254e-02,  4.43490259e-02,  3.41871791e-02, -4.06741910e-02,\n","          3.25535275e-02,  1.58720557e-02, -1.39159178e-02,  4.24312055e-03,\n","          4.11047190e-02,  3.63599434e-02,  7.47574791e-02, -2.27853227e-02,\n","          3.28443684e-02, -1.40861096e-02,  3.09938062e-02, -6.77025318e-03,\n","         -8.22199916e-04, -1.37036927e-02, -6.90173125e-04,  8.21405873e-02,\n","          4.80418205e-02,  3.73897702e-02, -1.70116108e-02,  1.49296047e-02,\n","          3.43731325e-03,  1.68242827e-02,  5.26401997e-02,  9.50832106e-03,\n","         -2.88762916e-02, -5.09082600e-02,  1.25213722e-02, -5.24219908e-02,\n","         -1.25193107e-03,  8.56769830e-03, -1.05213737e-02, -4.46834527e-02,\n","          1.45932231e-02,  2.18121801e-02, -1.48399817e-02,  6.31801859e-02,\n","         -4.01628064e-03, -1.59620959e-02,  9.06392559e-03,  5.80499619e-02,\n","          1.55726466e-02, -9.91623197e-03, -3.68816704e-02, -5.00595905e-02,\n","         -1.95246246e-02,  5.63495271e-02,  1.18385633e-05,  5.06634638e-02,\n","         -3.77443209e-02, -4.05137874e-02, -4.59714234e-03, -2.86676269e-02,\n","          9.14281351e-04, -2.57623959e-02, -4.45773639e-02, -2.15496402e-02,\n","         -1.99548826e-02, -2.83306148e-02,  2.86509711e-02,  5.87536655e-02,\n","          3.59714963e-02, -1.29653430e-02, -5.47382087e-02, -1.52523974e-02,\n","         -6.02128357e-02,  1.37535660e-02,  3.29765677e-02, -5.55829657e-03,\n","          4.85451184e-02, -2.01627593e-02, -5.24508161e-03, -6.20030388e-02,\n","         -3.25007141e-02,  1.45936767e-02, -2.67954711e-02,  2.22904235e-02,\n","         -2.54664067e-02, -2.83304173e-02, -3.19305770e-02, -9.62628238e-03,\n","          1.52522074e-02,  1.99754210e-03, -1.26420539e-02, -7.38942027e-02,\n","         -5.34991622e-02,  1.06475083e-02,  2.79105902e-02,  1.46741401e-02,\n","          7.33273849e-03,  2.30873656e-02, -2.56804824e-02,  2.41360907e-02,\n","         -6.56245127e-02,  2.82795299e-02,  2.79036481e-02, -6.19795220e-03,\n","         -7.21653551e-02,  4.43677008e-02, -2.80656144e-02,  1.44343646e-02,\n","         -2.04881635e-02, -1.76303368e-02,  4.93760295e-02, -5.04416996e-04,\n","         -4.38343361e-02,  4.97557707e-02, -4.22871783e-02, -1.83215383e-02,\n","          1.97321102e-02,  2.02805866e-02, -2.38434430e-02,  1.08509557e-02,\n","         -1.50376037e-02,  4.62516062e-02, -6.95999488e-02, -5.86223826e-02,\n","         -4.15806629e-04, -8.06974769e-02,  1.31996656e-02, -3.88159752e-02,\n","         -5.00450283e-02,  1.95961520e-02,  1.62257440e-03, -4.70355257e-33,\n","          4.84969728e-02, -5.44050485e-02,  2.61682980e-02,  7.70137310e-02,\n","          3.11205536e-02,  4.11386900e-02, -2.70451214e-02, -4.62202430e-02,\n","         -8.83982982e-03,  4.00649682e-02, -1.58100501e-02,  7.91575834e-02,\n","          3.17819463e-03,  2.17699595e-02,  1.74506716e-02, -3.87910986e-03,\n","          3.73214372e-02, -1.08173760e-02,  1.53364278e-02,  4.94681224e-02,\n","         -3.94949764e-02, -2.71271169e-02, -3.89453815e-03,  4.29055579e-02,\n","          4.65665273e-02, -8.87710750e-02,  5.02847172e-02, -3.62409018e-02,\n","          2.13094931e-02, -3.14339548e-02,  1.88883673e-02, -2.80346954e-03,\n","         -6.02734881e-03,  5.45887798e-02, -3.72069404e-02,  1.70942098e-02,\n","          6.32082969e-02,  1.12507148e-02,  3.15774977e-02,  1.48692401e-02,\n","          2.23735161e-02, -5.00843488e-03, -2.36868821e-02,  3.03903725e-02,\n","          5.86783960e-02,  4.24007811e-02, -3.58621255e-02, -6.07762299e-03,\n","         -1.96222216e-03,  6.09048568e-02, -1.04437573e-02,  2.42489278e-02,\n","          3.77100743e-02, -4.76063974e-02,  6.64104475e-03,  1.77524351e-02,\n","          1.15110986e-02, -1.11377135e-01,  4.70479690e-02,  7.91039690e-03,\n","         -2.78663426e-03,  4.54687374e-03,  1.00460453e-02, -1.68605167e-02,\n","         -4.35589924e-02, -1.96927916e-02,  8.95319209e-02, -4.59860457e-04,\n","          2.51219887e-02,  1.46659855e-02, -4.48159017e-02, -7.38603845e-02,\n","         -4.91571315e-02, -2.42758058e-02, -5.71921729e-02,  2.42923200e-02,\n","          5.33325151e-02,  6.09982722e-02, -1.90914217e-02,  5.81811853e-02,\n","         -2.28015911e-02, -3.88596999e-03,  2.51713526e-02, -4.59096730e-02,\n","          6.66780514e-04,  2.21222844e-02, -2.22523697e-02,  1.57655980e-02,\n","          3.41596231e-02, -3.34178396e-02,  8.29144791e-02, -6.68444764e-03,\n","         -3.31523083e-02,  1.06170513e-02, -4.00758581e-03,  8.18344578e-02,\n","         -3.26026999e-03,  3.42674665e-02,  1.59552712e-02, -2.30626781e-02,\n","          4.51319814e-02, -2.65569752e-03, -1.48039721e-02, -1.06644742e-02,\n","         -1.99099295e-02, -1.15731955e-02,  4.02657427e-02,  6.87926635e-03,\n","          6.35881275e-02, -3.21904756e-02, -3.19159701e-02,  2.96865348e-02,\n","          1.65222138e-02,  4.87334654e-02, -3.87803540e-02,  1.13892015e-02,\n","          8.98753013e-03,  1.38958544e-02,  1.35010350e-02,  7.82186445e-03,\n","          4.30424605e-03,  1.04804551e-02,  4.49315012e-02, -4.70428653e-02,\n","         -5.95107265e-02, -3.49524729e-02, -5.59733547e-02,  1.02736816e-01,\n","         -1.79780591e-02, -2.58237235e-02,  3.16798240e-02, -6.50785863e-03,\n","          2.69196278e-07, -1.25177167e-02,  2.57803611e-02, -2.20475961e-02,\n","         -6.71318695e-02, -6.11812714e-03, -2.18150523e-02, -7.85978232e-03,\n","         -1.35719525e-02, -3.26612890e-02,  6.35978580e-02,  2.51659341e-02,\n","         -5.12026623e-02, -2.79564094e-02,  6.21845834e-02,  8.64494033e-03,\n","          5.82561493e-02,  4.70088003e-03,  1.94973741e-02, -6.72624819e-03,\n","         -1.27117718e-02, -2.45906953e-02, -3.20351049e-02, -6.87071234e-02,\n","         -1.67586356e-02, -2.07691523e-03, -5.46417274e-02, -2.88479030e-02,\n","          3.73909287e-02, -1.31121939e-02, -4.70074713e-02,  2.76269894e-02,\n","          5.19062094e-02,  8.97821784e-03,  3.78280319e-02, -4.25370224e-02,\n","         -6.48046136e-02,  8.17744900e-03, -4.91942354e-02, -2.74507503e-04,\n","          1.96939427e-02,  3.64412484e-03, -1.02822827e-02,  1.15628764e-02,\n","          7.07469322e-03, -4.77050394e-02,  1.25684952e-02, -1.37362145e-02,\n","          1.06817763e-02,  3.62327769e-02,  1.01788864e-02,  9.56159178e-03,\n","          7.59956241e-03,  1.04538947e-02, -2.66773459e-02, -4.66744900e-02,\n","          3.29680671e-03, -4.18838207e-03, -3.71142812e-02,  8.67770761e-02,\n","          7.56789278e-03, -7.97840394e-03, -1.11822197e-02, -2.97499374e-02,\n","         -8.10784772e-02, -1.39397169e-02, -1.67733375e-02, -8.31955299e-03,\n","          1.85181526e-34, -3.81010436e-02, -1.66416559e-02,  2.94149332e-02,\n","         -5.49318865e-02, -4.51126397e-02,  4.11632545e-02, -3.28921638e-02,\n","          2.11090385e-03, -4.12831679e-02, -3.07366531e-02, -2.21453421e-02]),\n","  'score': tensor(0.0115)},\n"," {'page_number': 591,\n","  'sentence_chunk': 'car. Does it drive faster with a half-tank of gas or a full one?It does not matter; the car drives just as fast as long as it has gas. Similarly, depletion of B vitamins will cause problems in energy metabolism, but having more than is required to run metabolism does not speed it up. Buyers of B-vitamin supplements beware; B vitamins are not stored in the body and all excess will be flushed down the toilet along with the extra money spent. B vitamins are naturally present in numerous foods, and many other foods are enriched with them. In the United States, B-vitamin deficiencies are rare; however in the nineteenth century some vitamin-B deficiencies plagued many people in North America. Niacin deficiency, also known as pellagra, was prominent in poorer Americans whose main dietary staple was refined cornmeal. Its symptoms were severe and included diarrhea, dermatitis, dementia, and even death. Some of the health consequences of pellagra are the result of niacin being in insufficient supply to support the body’s metabolic functions.',\n","  'chunk_char_count': 1047,\n","  'chunk_word_count': 172,\n","  'chunk_token_count': 261.75,\n","  'embedding': array([ 1.18444962e-02,  1.16567723e-02,  6.21234672e-03,  1.90166105e-02,\n","          6.06579147e-02,  2.90020406e-02, -3.70180048e-02,  3.67560387e-02,\n","          1.07013136e-02, -4.31997981e-03,  5.36062755e-02,  5.10990769e-02,\n","         -8.57106596e-03, -2.01537739e-02, -4.21769265e-03,  6.06491044e-02,\n","         -2.60459110e-02,  1.37570938e-02,  3.66020910e-02,  3.59740481e-02,\n","         -2.25367900e-02,  2.66634151e-02,  2.23607123e-02, -1.43629871e-02,\n","         -6.08302727e-02, -2.39346288e-02,  7.23184645e-02, -3.25098308e-03,\n","          1.30033987e-02, -9.00078639e-02,  2.12378614e-02,  1.39582288e-02,\n","         -3.87164461e-03, -9.15227458e-02,  2.19386993e-06, -1.86175928e-02,\n","          1.22987106e-02,  2.76233582e-03, -3.30520011e-02,  8.24959055e-02,\n","          7.98611641e-02, -1.23178679e-02,  6.06004037e-02,  1.59579851e-02,\n","         -2.13973895e-02, -2.15457357e-03, -6.77812286e-03,  1.25223794e-03,\n","         -4.23293300e-02, -2.45496519e-02,  1.91401560e-02,  4.26450893e-02,\n","         -1.53336860e-02,  3.46431546e-02,  7.13221878e-02, -4.06150147e-02,\n","         -2.06049550e-02,  1.59691572e-02, -4.05417150e-03,  6.22088206e-04,\n","          1.99244963e-03, -1.22812157e-02,  1.25143677e-03, -4.37341910e-03,\n","         -3.46296318e-02,  7.10358918e-02, -3.70867327e-02, -4.87738289e-02,\n","          8.87189712e-03,  4.05474789e-02, -6.29986599e-02,  2.42261942e-02,\n","          6.04709573e-02,  2.17967331e-02, -4.07399572e-02, -5.75022958e-02,\n","          9.58325062e-03, -3.01274448e-03, -3.71157564e-02,  1.50681669e-02,\n","          8.39454755e-02, -2.80296952e-02,  2.46602967e-02, -1.03417598e-02,\n","          4.58675139e-02,  4.54214327e-02,  4.88429004e-03, -1.80983730e-02,\n","         -4.14963998e-02, -5.11379540e-02, -4.77414317e-02, -6.33938238e-03,\n","          2.73912940e-02, -4.17364836e-02,  3.17337438e-02, -8.17056838e-03,\n","         -1.65353958e-02,  2.79847793e-02,  8.35654661e-02, -9.68036726e-02,\n","          3.09677050e-03, -2.18292675e-03, -9.32131857e-02, -1.24038048e-02,\n","         -2.44756117e-02, -6.78617135e-02, -8.39760993e-03,  2.88502295e-02,\n","         -2.27838866e-02,  1.37119535e-02, -1.09271314e-02,  2.86272876e-02,\n","         -3.86260473e-03,  2.37362087e-03,  5.80315525e-03,  9.37026273e-03,\n","         -2.76787225e-02, -1.52059570e-02, -9.57137048e-02, -1.74161680e-02,\n","         -6.35600463e-02,  3.76707651e-02,  5.11575229e-02, -1.13524431e-02,\n","         -1.14689879e-02,  9.15272683e-02,  2.70712432e-02, -1.56898256e-02,\n","          9.12504457e-03, -2.86040138e-02,  1.95134822e-02, -8.25801492e-02,\n","         -2.75838766e-02,  4.48396849e-03,  2.39477376e-03, -4.78581467e-04,\n","          5.62134795e-02, -2.98357499e-03, -6.22236058e-02,  6.60387473e-03,\n","          1.13899270e-02, -8.88699945e-03, -8.70712101e-02, -1.39518473e-02,\n","          2.56529059e-02,  2.16633435e-02,  7.49683240e-03, -5.30431606e-02,\n","          2.41030920e-02,  4.43869457e-02, -5.13947047e-02, -1.98769867e-02,\n","          3.95543240e-02,  4.62127365e-02,  3.30057777e-02, -2.26131603e-02,\n","          5.25231920e-02, -2.82527674e-02,  6.75178543e-02, -3.56035456e-02,\n","          6.99012540e-03, -1.40522355e-02,  1.92368205e-03, -2.03620940e-02,\n","          2.51096878e-02,  5.06967753e-02,  5.32545932e-02, -1.55249506e-03,\n","          1.30335428e-02,  9.00881365e-03,  2.81657316e-02, -2.10220776e-02,\n","          2.30309390e-03,  1.72589347e-03,  1.92814432e-02, -4.37548291e-03,\n","         -2.24420801e-02,  7.65061588e-04, -1.37947854e-02,  5.98588772e-03,\n","         -4.59470926e-03, -4.47268551e-03, -2.64033582e-03, -1.16997780e-02,\n","         -8.95953253e-02, -2.19613183e-02,  5.31286262e-02,  4.45509665e-02,\n","         -1.85878742e-02, -1.70800034e-02,  5.48224784e-02,  2.33413205e-02,\n","          2.34968923e-02, -4.59942296e-02,  9.59697831e-03,  2.04527993e-02,\n","         -1.28424764e-02,  2.39974856e-02,  4.60487679e-02, -4.75898981e-02,\n","          2.04733741e-02,  5.14555946e-02, -1.56338457e-02, -3.18094008e-02,\n","         -2.41402499e-02,  3.59297656e-02, -4.16801795e-02,  4.66355011e-02,\n","          1.92605015e-02, -4.37927246e-02, -5.44931516e-02, -2.12936811e-02,\n","          3.99917476e-02, -9.82682873e-03,  3.89921777e-02, -3.61818112e-02,\n","         -8.41491017e-03, -1.59933381e-02, -4.21996638e-02,  1.03559613e-01,\n","          1.52200228e-02, -4.73163016e-02, -1.71960946e-02, -2.88567152e-02,\n","          2.21521482e-02, -1.33124432e-02,  2.43292041e-02, -3.59760597e-03,\n","         -6.47308603e-02, -1.71863344e-02,  2.86536068e-02,  2.13335697e-02,\n","         -2.38982104e-02, -1.71887726e-02, -3.11995558e-02, -3.34200971e-02,\n","          2.15023831e-02, -4.04146081e-03,  2.68881116e-03,  6.92666620e-02,\n","         -9.89996130e-04,  3.51329381e-03, -7.17470422e-02, -4.46868986e-02,\n","         -2.48646829e-02,  3.11696213e-02, -3.43918093e-02,  1.32981734e-02,\n","         -5.51400110e-02,  1.63395517e-02,  4.28345278e-02,  3.94972079e-02,\n","          9.52988956e-03, -3.30098526e-04, -4.26959395e-02,  4.63051572e-02,\n","         -4.67287078e-02, -2.61939894e-02, -5.89254536e-02,  7.76295969e-03,\n","          3.57764587e-02,  3.34200449e-02,  2.91858194e-03, -2.31695687e-03,\n","         -4.84267063e-02, -1.18450234e-02,  6.87344372e-02,  3.00786719e-02,\n","         -8.90839379e-03,  2.67612524e-02, -5.82224410e-03,  3.95364361e-03,\n","          1.84136815e-02,  1.99869205e-03, -3.86850871e-02, -2.74318121e-02,\n","          3.18611274e-03,  4.12719697e-02,  2.83420570e-02, -1.73197389e-02,\n","          2.89489725e-03, -3.64894560e-03,  1.27919698e-02,  1.17002632e-02,\n","          1.84390116e-02, -2.07332931e-02, -4.27251197e-02,  2.27634446e-03,\n","          4.61309291e-02, -3.34046036e-02,  5.80925457e-02,  2.05299258e-02,\n","          2.01679002e-02,  4.10322919e-02, -4.51739272e-03,  7.89262913e-03,\n","         -6.59511983e-02,  5.24030719e-03, -3.70413740e-03, -2.42514517e-02,\n","         -1.83855295e-02,  3.50294858e-02, -3.34950499e-02,  5.45849698e-03,\n","         -2.06807628e-02,  5.28659038e-02, -2.22354173e-03, -7.77556300e-02,\n","         -1.38166798e-02,  8.08828883e-03,  1.72016546e-02,  2.32081041e-02,\n","          4.27095741e-02,  3.98687832e-02, -3.82241122e-02, -2.31705401e-02,\n","         -6.84916554e-03, -8.45982786e-03,  5.67877553e-02,  5.24219545e-03,\n","          8.94395052e-04, -4.16806117e-02, -1.99225754e-03, -2.78529748e-02,\n","         -1.22691989e-02, -1.69738091e-03, -6.48545194e-03,  3.96176390e-02,\n","         -1.39942795e-01,  4.74585332e-02, -2.74089612e-02, -6.40012743e-03,\n","         -1.17334640e-02,  4.33929414e-02,  4.28712890e-02, -1.78059260e-03,\n","          8.67100805e-03,  1.71495765e-03, -3.27484757e-02,  1.90727003e-02,\n","         -1.31540908e-03, -1.78135782e-02, -9.18128935e-04, -1.38710700e-02,\n","          3.02474890e-02, -1.17822019e-02, -2.35467386e-02,  8.08448822e-04,\n","         -5.85973496e-03,  1.18571138e-02, -5.65626845e-02, -9.24692664e-04,\n","          5.39197493e-03,  4.44797464e-02,  1.10587534e-02, -4.85246144e-02,\n","          5.09182736e-03, -6.99098185e-02, -1.26439929e-02, -7.32341968e-03,\n","          4.70309556e-02,  2.99620628e-02, -3.55501706e-03, -4.55858372e-02,\n","          3.76813561e-02,  3.47118489e-02, -5.49620911e-02,  1.51415085e-02,\n","          1.87003221e-02, -6.06368063e-03, -3.27326655e-02,  1.75877158e-02,\n","         -6.15530163e-02, -3.92294209e-03, -2.14551613e-02,  2.59300750e-02,\n","          4.15515229e-02,  3.50829884e-02, -4.87921946e-02, -3.81483659e-02,\n","          4.79379557e-02, -1.86515898e-02,  2.85862703e-02, -2.35138880e-03,\n","          1.27598969e-02,  2.42927112e-02,  2.31210305e-03, -2.47830059e-02,\n","          3.28928903e-02, -4.07116413e-02, -1.04620820e-02,  5.89480847e-02,\n","          1.04197590e-02,  5.39721474e-02, -1.40121905e-03,  7.36936321e-03,\n","          3.25539075e-02, -1.59847662e-02,  3.73374596e-02,  4.88743149e-02,\n","          1.83285959e-02, -7.50302672e-02,  2.09720545e-02,  3.09170242e-02,\n","          2.98509896e-02, -4.42874320e-02, -5.64554706e-02, -3.94437015e-02,\n","          6.29979670e-02, -4.05172072e-02,  3.26867290e-02, -2.97762938e-02,\n","         -1.27418442e-02, -3.73539366e-02,  1.03375994e-01, -2.07188330e-03,\n","          3.64062726e-03, -2.03160252e-02,  2.78255753e-02, -6.30679056e-02,\n","         -4.24298681e-02, -8.93253833e-03, -4.61854339e-02, -6.58495585e-03,\n","         -2.36385521e-02, -4.23449501e-02,  4.94633685e-04,  3.01651750e-03,\n","         -2.14829370e-02,  1.67437680e-02,  1.61280334e-02, -4.34928481e-03,\n","          4.45778854e-02, -4.22311909e-02, -1.59242712e-02,  3.39233242e-02,\n","          4.01039310e-02, -4.01716083e-02,  4.76349294e-02, -8.33068136e-03,\n","          2.42669806e-02,  3.23260413e-03,  5.76930866e-02, -3.21395397e-02,\n","          3.21987495e-02, -7.24964915e-03, -3.63314012e-03,  8.34180489e-02,\n","          5.44170886e-02,  4.17106561e-02, -1.69475488e-02,  4.79998291e-02,\n","         -4.11097147e-02,  2.26609502e-02,  5.21868244e-02,  2.07053460e-02,\n","         -6.16508052e-02, -7.19163492e-02,  8.84736516e-03, -1.95796192e-02,\n","          5.47408778e-03,  1.24133853e-02,  3.69516350e-02, -4.45786044e-02,\n","          3.85754160e-04,  6.12417758e-02,  1.03834989e-02,  2.97388975e-02,\n","         -2.93458123e-02, -3.02363727e-02, -2.94052064e-02,  7.46345147e-02,\n","          7.07391184e-03, -7.39689995e-06, -3.76068093e-02,  8.26267898e-03,\n","         -3.06838211e-02,  9.33617761e-04,  2.55461573e-03,  5.70086576e-02,\n","         -3.82053629e-02,  4.21845540e-03, -6.48367032e-03, -4.71307081e-04,\n","         -1.77656151e-02, -1.11404173e-02, -2.99858171e-02, -3.71042080e-02,\n","         -5.67649538e-03, -1.60887484e-02, -5.63370716e-03,  3.21499147e-02,\n","         -7.88179226e-04,  3.89504021e-05, -1.37553038e-02,  2.56832708e-02,\n","         -4.84770872e-02,  7.96666648e-03,  1.63696818e-02, -3.16521861e-02,\n","          7.27780685e-02, -1.41034182e-02,  2.36744098e-02, -4.09665741e-02,\n","         -1.54854888e-02,  5.94976470e-02, -2.30434872e-02,  7.33561292e-02,\n","         -1.67232379e-02, -5.05123101e-02, -4.42183837e-02, -2.90985452e-03,\n","         -9.60751809e-03,  3.00936084e-02, -1.28222965e-02, -1.10312914e-02,\n","         -4.38243598e-02, -7.17353970e-02,  2.87771542e-02,  2.18262598e-02,\n","         -4.42714430e-04, -7.06447056e-03,  5.26475534e-03,  3.57527956e-02,\n","         -2.04629079e-02,  4.51463945e-02, -5.20954898e-04, -8.03815015e-03,\n","         -4.99979369e-02,  5.43175638e-02, -2.25747898e-02, -1.22743882e-02,\n","         -2.81581637e-02, -2.50446871e-02,  1.30022503e-02,  3.36271562e-02,\n","         -2.97642555e-02,  5.75959124e-02, -2.00640573e-03, -1.10718366e-02,\n","          3.44474502e-02, -5.34584513e-03,  1.07406639e-02,  2.09157020e-02,\n","          2.90202983e-02,  7.86209106e-03, -6.11882620e-02, -4.44183536e-02,\n","          2.76606139e-02, -1.19668052e-01, -9.19826422e-03, -8.30678828e-03,\n","         -7.86803141e-02, -2.95801684e-02,  2.18306985e-02, -5.95882478e-33,\n","          3.18266526e-02, -7.87683427e-02,  3.31198350e-02,  6.92715719e-02,\n","          5.33978119e-02,  4.74158823e-02, -2.30867080e-02, -1.56274624e-02,\n","          2.47579184e-03,  3.47058102e-02, -1.76133886e-02,  2.85486076e-02,\n","          1.17863491e-02,  1.13541577e-02,  2.01284466e-03, -1.30565194e-02,\n","          1.72865726e-02, -2.36728955e-02,  1.47207947e-02,  2.62802187e-02,\n","         -3.87545787e-02,  9.71585326e-03, -1.35467425e-02,  7.22947419e-02,\n","          5.87555021e-02, -6.25118911e-02,  5.57548478e-02, -6.66933209e-02,\n","         -1.84467027e-03, -5.69690904e-03,  3.43243964e-02,  1.56692844e-02,\n","          2.30974369e-02,  2.93206293e-02, -4.09728624e-02,  5.76832797e-03,\n","          6.48464561e-02,  2.40120366e-02, -1.55448690e-02,  9.80796013e-03,\n","         -4.17305194e-02, -1.78737342e-02, -3.18879867e-03,  3.83111946e-02,\n","          4.03144620e-02,  6.20360859e-02, -4.45533767e-02, -1.79096144e-02,\n","         -1.53540317e-02,  6.88684285e-02,  1.44459940e-02,  1.20353121e-02,\n","          2.72738412e-02, -1.97492018e-02,  1.26132183e-02,  7.35224262e-02,\n","          4.71034739e-03, -9.99235809e-02,  6.66729435e-02, -1.54729858e-02,\n","          2.68851686e-02,  6.64585503e-03,  2.05318239e-02,  9.95445531e-03,\n","         -3.26510482e-02, -3.52954268e-02,  6.18247502e-02,  2.80144773e-02,\n","         -1.75802428e-02,  2.93189585e-02, -3.64402048e-02, -5.36166281e-02,\n","         -6.22980185e-02,  7.12697254e-03, -4.81724367e-03, -1.25219468e-02,\n","          2.81744474e-03,  8.64900947e-02, -3.96894254e-02,  1.91641692e-02,\n","         -4.93246876e-02, -1.34311710e-02,  2.26779673e-02, -3.39464918e-02,\n","         -5.10167368e-02,  6.00047521e-02, -1.27675394e-02,  4.96150665e-02,\n","          2.79734116e-02, -3.21167968e-02,  6.03232235e-02, -1.41566973e-02,\n","         -1.24477642e-02,  9.06573376e-04,  7.03669619e-03,  5.67904338e-02,\n","          2.07250360e-02, -1.73599813e-02, -6.10787608e-03, -3.03681716e-02,\n","         -1.34519758e-02,  4.53036278e-02, -1.81717742e-02, -1.16182659e-02,\n","         -4.34858613e-02,  2.34006345e-02,  4.82455119e-02, -3.47190090e-02,\n","          1.87432002e-02, -2.15813387e-02, -5.49471341e-02,  2.40284503e-02,\n","          2.89777238e-02,  1.56197818e-02, -5.11091761e-03,  5.39694307e-03,\n","          2.05735886e-03,  4.86506671e-02,  3.27441320e-02,  2.27575935e-02,\n","         -1.99917555e-02, -1.24959378e-02,  4.50092852e-02, -1.74307171e-02,\n","         -5.56204095e-02, -3.85597907e-02,  1.29718203e-02,  6.79138303e-02,\n","         -3.52450609e-02, -1.03341276e-02,  3.45656760e-02, -3.75390821e-03,\n","          3.00867271e-07,  5.52483788e-03,  2.72795148e-02, -8.56389175e-04,\n","         -7.24244192e-02,  1.16493914e-03,  7.60216732e-03, -4.69624177e-02,\n","         -1.39517114e-02, -1.78321730e-02,  3.01347002e-02,  4.88415919e-02,\n","         -2.93126144e-02, -4.94633317e-02,  2.63821948e-02,  3.56691144e-02,\n","          9.14342776e-02, -2.28819307e-02,  9.68290027e-03, -1.34942681e-02,\n","         -5.08503430e-03, -3.15079987e-02, -2.68655196e-02, -9.18726549e-02,\n","         -4.70484905e-02,  6.38386607e-03, -3.66588943e-02, -1.03500849e-02,\n","         -3.63933109e-02, -2.08425969e-02, -6.05867691e-02,  1.93018001e-02,\n","          6.96154907e-02, -6.56083645e-03,  1.81419235e-02, -3.99962626e-02,\n","         -6.67625442e-02, -4.65235598e-02, -4.32775058e-02, -2.37559117e-02,\n","         -1.51262889e-02, -6.32780604e-04,  1.09292055e-02,  1.22320391e-02,\n","          2.73808483e-02, -4.43265252e-02,  2.82378048e-02, -5.23656942e-02,\n","         -1.22237448e-02,  2.62468606e-02,  1.18103558e-02, -1.41017372e-02,\n","          3.66939120e-02, -2.62923203e-02, -8.87147803e-03, -4.53229733e-02,\n","          1.74141973e-02,  1.48263657e-02, -1.78705789e-02,  9.42277610e-02,\n","          2.76215188e-02, -1.35056507e-02, -1.21954838e-02, -4.57070768e-02,\n","         -1.55056100e-02, -4.83514443e-02,  5.27427830e-02, -2.76629534e-02,\n","          2.65745454e-34, -2.08405666e-02, -3.64427790e-02,  2.94195898e-02,\n","         -7.25442246e-02, -3.05305235e-02,  5.17590530e-02, -1.70974166e-03,\n","         -1.92621462e-02, -5.88275008e-02, -4.69687954e-02, -2.89492998e-02]),\n","  'score': tensor(0.0087)},\n"," {'page_number': 1079,\n","  'sentence_chunk': 'The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its porosity and tenderness. For those who are sensitive to gluten, it is good to know that corn, millet, buckwheat, and oats do not contain the proteins that make gluten. However, some people who have celiac disease also may have a response to products containing oats. This is most likely the result of cross-contamination of grains during harvest, storage, packaging, and processing. Celiac disease is most common in people of European descent and is rare in people of African American, Japanese, and Chinese descent. It is much more prevalent in women and in people with Type 1 diabetes, autoimmune thyroid disease, and Down and Turner syndromes. Symptoms can range from mild to severe and can include pale, fatty, loose stools, gastrointestinal upset, abdominal pain, weight loss and, in children, a failure to grow and thrive. The symptoms can appear in infancy or much later in life, even Nutrition, Health and Disease | 1079',\n","  'chunk_char_count': 1008,\n","  'chunk_word_count': 164,\n","  'chunk_token_count': 252.0,\n","  'embedding': array([-6.64326251e-02, -2.21385667e-03,  5.92324324e-03,  3.61854695e-02,\n","          5.60895465e-02,  1.86096057e-02, -5.71276397e-02,  5.20170154e-03,\n","          7.48443231e-02,  2.38871630e-02,  3.13964300e-02,  1.96020212e-02,\n","         -2.52566058e-02, -2.21625417e-02, -3.52608711e-02,  9.13255587e-02,\n","         -1.57936551e-02,  1.56345610e-02, -3.37289087e-02, -4.23163874e-03,\n","         -3.20668928e-02,  8.67997389e-03,  4.54692245e-02,  2.59598177e-02,\n","         -3.58006894e-03, -4.04105596e-02,  2.58153211e-02, -3.24412696e-02,\n","          2.25738436e-02,  2.46383324e-02,  1.50372542e-03, -1.94025226e-02,\n","         -2.72075552e-02, -5.70092835e-02,  1.89436685e-06,  1.08708926e-02,\n","         -2.65722852e-02,  2.25750450e-02, -3.92919546e-03,  1.36278626e-02,\n","         -2.84242574e-02, -3.97910699e-02, -2.19251830e-02,  1.29112266e-02,\n","          2.22825035e-02, -3.45190130e-02,  2.48207673e-02,  1.81014556e-02,\n","         -1.64507683e-02, -4.23717545e-03,  1.68819856e-02, -2.15877444e-02,\n","          3.28137213e-03, -7.97742512e-03,  1.16317451e-01,  2.33973917e-02,\n","         -5.05083054e-02, -3.13303918e-02,  5.98623045e-03, -4.50045206e-02,\n","         -1.91262383e-02, -1.69710349e-02,  1.05192894e-02,  3.68244275e-02,\n","         -8.41323808e-02,  1.47961611e-02, -1.31042174e-03, -9.56990197e-03,\n","          4.00039181e-02,  5.15898615e-02, -1.25973020e-02,  3.27234827e-02,\n","          2.66028009e-02,  2.40340866e-02,  8.22898559e-03,  1.38945375e-02,\n","          3.68293040e-02,  1.65260083e-03, -2.33521126e-03, -9.57498606e-03,\n","          3.63176838e-02, -1.74193718e-02, -1.99262872e-02,  2.99632200e-03,\n","          3.40184080e-03,  9.73242223e-02, -6.77033840e-03, -7.80257210e-03,\n","         -4.58285511e-02, -6.32318482e-02,  3.58330570e-02, -3.94266918e-02,\n","         -6.59626573e-02, -3.11155468e-02, -4.12967661e-03, -2.83722207e-03,\n","         -1.73436236e-02,  1.33008761e-02,  2.03191657e-02, -3.11826821e-02,\n","         -1.59654487e-03,  1.63837783e-02, -1.79423727e-02,  9.59970150e-03,\n","         -3.06892190e-02,  3.94537151e-02,  5.80378016e-03,  6.08653715e-03,\n","          4.14675660e-02,  3.88001166e-02, -8.00082739e-03,  2.14654598e-02,\n","         -9.57674626e-03, -1.64306571e-03,  2.65014283e-02,  1.21444706e-02,\n","         -3.85542447e-03,  9.84708127e-03, -4.21721600e-02, -1.36437751e-02,\n","         -1.10398665e-01, -2.70875972e-02,  3.19967829e-02,  3.07907872e-02,\n","         -3.08728013e-02,  4.25869748e-02,  6.46654190e-03, -5.35378978e-03,\n","          7.87336472e-03, -4.87290770e-02, -1.18850954e-02, -4.22748066e-02,\n","         -3.20908912e-02,  8.26629326e-02, -2.16816994e-03, -3.44432965e-02,\n","          1.34535395e-02,  1.82955060e-02,  4.61438932e-02, -1.30080860e-02,\n","         -8.79924465e-03,  1.47054577e-02, -3.90030481e-02, -2.57637072e-02,\n","          3.32062207e-02,  7.73938969e-02,  3.20588686e-02, -7.18467589e-03,\n","         -2.28001792e-02,  2.04023141e-02, -8.49242061e-02,  4.51029278e-03,\n","          6.02652244e-02,  3.92741226e-02, -1.55934589e-02, -1.24323163e-02,\n","          6.18113391e-02, -4.82565607e-04, -4.96408716e-03,  3.35329808e-02,\n","          3.73809435e-03,  2.35716384e-02,  2.40390152e-02,  1.32530425e-02,\n","         -5.66640049e-02,  1.07472995e-02,  5.18378168e-02,  2.82624941e-02,\n","          3.15762423e-02,  3.61186340e-02,  5.34531325e-02, -1.00153536e-02,\n","          1.51607813e-02, -3.85169052e-02,  4.01078351e-02,  4.41012345e-02,\n","          6.61009643e-03, -2.36799568e-02, -5.28727695e-02,  2.74992641e-02,\n","          4.98077460e-02,  4.52505164e-02,  6.00851290e-02, -1.94083061e-02,\n","         -3.49980146e-02,  1.57993529e-02,  4.35280763e-02, -8.12475849e-03,\n","          1.46430060e-02, -5.15535998e-04, -2.58451533e-02,  3.16204354e-02,\n","          1.64353102e-02, -7.05224415e-03,  1.12166721e-02,  3.21557000e-02,\n","          2.69565755e-03,  3.86169218e-02,  3.92716490e-02, -9.31078866e-02,\n","         -7.38019822e-03,  1.03638638e-02,  5.14562614e-02, -8.23660567e-02,\n","          2.91834641e-02,  1.44317569e-02, -1.30919348e-02, -9.03331582e-03,\n","         -1.62556563e-02, -1.97227560e-02,  2.54103579e-02, -2.07285490e-02,\n","         -1.61298476e-02,  2.96662115e-02,  3.05089843e-03,  1.31714912e-02,\n","         -2.68874429e-02,  8.32812022e-03, -3.92578216e-03,  1.29859094e-02,\n","         -4.66279350e-02, -4.93892580e-02,  4.83567230e-02,  1.21968500e-02,\n","          1.77931618e-02, -9.53110028e-03,  1.30467108e-02, -4.48391512e-02,\n","          2.33024843e-02,  3.20777521e-02,  3.00483219e-02,  7.57482601e-03,\n","         -6.17923550e-02,  1.60361594e-03, -6.12087660e-02,  6.36622403e-03,\n","         -2.21277513e-02, -8.75651557e-03, -2.34739259e-02,  1.82363801e-02,\n","         -6.26530275e-02, -3.49110141e-02, -8.68006498e-02, -6.04116777e-03,\n","          4.48260903e-02, -6.27818378e-03,  7.08247907e-03,  7.66419666e-03,\n","         -7.93338716e-02,  4.05093506e-02, -5.09731099e-02, -6.97234133e-03,\n","          3.29639353e-02, -1.38025880e-02, -9.59928893e-03, -3.57539579e-02,\n","         -2.67492142e-03, -1.91205703e-02, -9.82575119e-03, -7.54863909e-03,\n","          1.79689955e-02,  4.72533666e-02,  2.25397684e-02, -2.27177236e-02,\n","         -4.30235639e-02,  2.46138149e-03, -6.22445671e-03,  7.34607968e-03,\n","          8.08855891e-03, -4.37501492e-03,  1.52702455e-03, -4.43217024e-04,\n","         -2.56180186e-02, -2.52876431e-02, -2.07744390e-02, -3.58477980e-02,\n","          6.68372288e-02,  4.33819816e-02,  1.72474273e-02, -2.42778435e-02,\n","         -4.54377271e-02,  4.81666885e-02,  3.32502066e-03,  4.72053234e-03,\n","          2.62789037e-02,  1.31917857e-02, -7.37411622e-03, -3.22309248e-02,\n","          3.72579917e-02, -9.25364625e-03,  1.72630197e-03,  3.21696661e-02,\n","          1.65409464e-02,  4.04735282e-02, -4.30397764e-02, -4.37600538e-03,\n","         -2.33691297e-02,  2.05417001e-03, -2.22631302e-02,  8.42420906e-02,\n","          3.52759683e-03,  7.95017555e-03,  1.21032400e-02,  3.90807986e-02,\n","          2.62876451e-02,  1.48084490e-02, -2.47846805e-02,  1.67448707e-02,\n","         -1.64994150e-02,  1.96974706e-02,  4.34285291e-02,  2.84560565e-02,\n","         -3.10326647e-03, -4.14884686e-02, -7.09726755e-03,  1.30846901e-02,\n","         -1.56874452e-02, -2.57135034e-02,  7.86183253e-02, -2.59680524e-02,\n","          2.51362938e-02, -1.70261494e-03, -3.55230048e-02, -2.46748216e-02,\n","         -2.16074996e-02,  1.32209426e-02,  2.68722288e-02,  2.48634927e-02,\n","         -7.81361833e-02, -2.35897885e-03,  7.75572844e-03,  9.81699303e-03,\n","          2.23264061e-02,  1.64473914e-02, -1.86901749e-03, -3.17417867e-02,\n","         -1.59984473e-02,  3.43679376e-02,  2.43940242e-02, -2.62289145e-03,\n","          5.53632295e-03,  6.25632424e-03,  1.37501927e-02,  4.75971848e-02,\n","          3.07139079e-03,  9.24969465e-03, -3.46784592e-02, -3.98246460e-02,\n","         -2.36803293e-02,  1.84140485e-02, -6.77443147e-02, -2.44420767e-02,\n","          4.34282087e-02,  2.20559258e-02, -1.10020293e-02, -3.39505114e-02,\n","          4.16753180e-02, -4.61601131e-02, -3.27344351e-02, -1.05858035e-02,\n","         -1.61782969e-02,  2.30488516e-02, -6.94645196e-02, -3.29919793e-02,\n","          5.82422093e-02,  1.86753683e-02, -8.62242561e-03, -5.67826107e-02,\n","          8.02612752e-02, -1.24759292e-02, -4.24889177e-02,  8.90853070e-03,\n","          7.81793147e-03, -1.32987695e-02, -1.39938658e-02,  6.36593159e-03,\n","          3.21113095e-02,  1.29772974e-02, -3.79292555e-02, -2.46940386e-02,\n","          3.23932022e-02, -2.48265620e-02, -1.72144291e-03,  2.95823114e-03,\n","         -4.35674191e-02, -8.02634936e-03, -7.57052526e-02,  3.62343118e-02,\n","         -5.87333972e-03, -7.82219972e-03, -1.38771292e-02,  6.03957847e-03,\n","         -1.05181530e-01,  1.09299232e-04,  6.62692115e-02,  4.56362627e-02,\n","          1.20318239e-03, -4.27177623e-02, -1.20801991e-02,  2.88505144e-02,\n","         -1.75634231e-02, -5.50057441e-02,  6.70899590e-03,  7.20439181e-02,\n","          5.67188784e-02,  3.61443348e-02, -4.34128605e-02, -4.86177206e-02,\n","          1.39339725e-02, -5.76412901e-02, -1.78056248e-02, -1.92341767e-02,\n","          1.88421886e-02, -2.11487636e-02,  8.26660171e-03,  1.20453872e-02,\n","          9.10576805e-03, -1.02140475e-02,  1.77487694e-02, -5.87837547e-02,\n","          5.62713929e-02, -6.85231236e-04, -2.53984649e-02,  2.06005108e-02,\n","         -3.64052914e-02,  1.34913996e-03, -1.34263020e-02, -9.65427887e-03,\n","         -1.87450424e-02, -2.99822143e-03,  1.06572937e-02, -2.47543454e-02,\n","          1.68259908e-02, -1.27460314e-02,  6.90293759e-02,  5.74206524e-02,\n","          6.84181675e-02,  2.30354778e-02, -6.23248797e-03,  4.67220061e-02,\n","          3.61772627e-02, -3.86318006e-03, -4.36501056e-02, -1.78630035e-02,\n","          2.59366930e-02, -1.57707818e-02,  1.51985360e-03,  5.65017387e-03,\n","          6.72109723e-02,  6.08622208e-02, -3.65965925e-02, -1.06242942e-02,\n","          1.51888197e-02,  1.85662117e-02, -1.71097752e-03,  3.14409621e-02,\n","          4.01580371e-02, -8.51354524e-02, -1.53440339e-02, -2.92555895e-03,\n","         -3.89654003e-02, -5.45870699e-03,  2.80284174e-02, -3.57526727e-02,\n","         -1.34408381e-02,  1.52447987e-02, -3.76835242e-02,  4.13733833e-02,\n","         -1.47155649e-03, -1.99676049e-03, -5.73486239e-02,  4.29265723e-02,\n","          1.69869754e-02,  3.72540876e-02, -1.03736687e-02,  1.63881890e-02,\n","         -6.03834242e-02,  1.65870450e-02,  8.96575209e-03,  3.26949023e-02,\n","         -6.15810454e-02, -5.84980883e-02, -1.84891783e-02, -5.05322702e-02,\n","         -1.14374040e-02,  5.41335531e-03,  1.96729470e-02, -3.10322102e-02,\n","         -1.59438662e-02, -3.19790952e-02,  2.52734739e-02,  3.62120406e-03,\n","          3.85012478e-02, -4.41877618e-02, -2.58937776e-02,  2.04193816e-02,\n","          2.72783008e-03,  4.92966957e-02, -1.01853825e-01,  2.51120329e-02,\n","          4.44200300e-02, -4.23125364e-02,  2.59565208e-02,  2.53757294e-02,\n","         -2.43898053e-02, -2.64448207e-02, -1.28887240e-02,  3.52027752e-02,\n","          3.33079770e-02,  2.34492552e-02,  4.77548316e-02,  1.04591288e-02,\n","         -3.89545131e-03,  7.92965889e-02, -3.34549211e-02, -6.37925640e-02,\n","         -6.72842339e-02, -8.57141707e-03,  2.89599095e-02, -8.01783893e-03,\n","         -3.85054876e-03, -1.02831225e-03, -3.33639123e-02,  1.15697626e-02,\n","         -2.96163205e-02,  4.38645445e-02,  1.30933188e-02, -1.82828456e-02,\n","         -1.58792511e-02,  4.78913188e-02, -1.55573003e-02, -1.63843166e-02,\n","         -5.28006740e-02, -4.88019735e-02,  6.33965246e-04,  2.61172429e-02,\n","          8.17780644e-02,  1.88547582e-03, -1.31252473e-02,  1.29619949e-02,\n","          8.85353517e-03, -1.01380376e-02,  1.39171109e-02,  3.50940637e-02,\n","          3.26899812e-02,  1.29630761e-02, -4.38526534e-02,  2.29428373e-02,\n","          6.23800009e-02, -2.87025841e-03,  4.05203970e-03, -1.53147522e-02,\n","         -1.90393459e-02, -4.27371822e-02,  2.46131048e-02, -5.38458807e-33,\n","         -2.55577024e-02, -3.67560051e-02,  1.67563092e-02,  3.38485800e-02,\n","          8.74503404e-02,  7.02852756e-02,  3.13296020e-02, -3.79156657e-02,\n","         -1.59179810e-02,  5.21480814e-02, -5.17031848e-02,  5.77013232e-02,\n","         -6.55628880e-03,  2.93866917e-02, -6.60690591e-02, -3.49952653e-02,\n","          3.29452828e-02, -1.48813482e-02,  1.15654934e-02, -1.14513980e-02,\n","         -2.58188099e-02, -6.29132763e-02, -6.09102100e-02, -1.75897069e-02,\n","          8.54788497e-02, -5.66027090e-02,  6.67336807e-02, -7.95466751e-02,\n","         -9.63603240e-03,  1.93619896e-02, -1.64564035e-03, -1.09831220e-03,\n","          5.45770675e-02,  7.21650496e-02, -3.08141280e-02,  2.05615675e-03,\n","          9.15022288e-03,  2.18770951e-02, -7.41887540e-02, -1.42872641e-02,\n","          1.39682721e-02,  3.06270253e-02, -6.43441826e-02,  3.82054076e-02,\n","         -1.91426036e-04,  3.46929356e-02, -1.42032374e-02,  1.71755292e-02,\n","         -5.15570268e-02,  4.19627763e-02,  3.70156439e-03,  4.76410100e-03,\n","          4.71033789e-02,  2.67195255e-02, -5.14346687e-03,  5.71937412e-02,\n","          1.78015735e-02, -6.80885985e-02,  1.16154961e-01,  4.60636653e-02,\n","          2.67381892e-02, -7.05542741e-03, -1.18617364e-03, -2.35085320e-02,\n","         -1.93623248e-02, -2.08029356e-02, -4.00323756e-02,  6.16153628e-02,\n","         -2.83016525e-02, -2.79777795e-02, -7.23716989e-02, -1.03983425e-01,\n","         -6.05324991e-02, -9.26108100e-03,  7.80624337e-03, -6.66272081e-03,\n","          3.12162526e-02,  1.62928756e-02, -5.54244891e-02, -1.76460240e-02,\n","         -5.65437134e-03,  8.88363924e-03, -4.44593839e-03, -6.18577749e-03,\n","         -1.15419710e-02,  4.17539813e-02,  2.82772426e-02, -2.59705563e-03,\n","          5.44837350e-03, -5.55630513e-02,  4.95972000e-02, -1.06214099e-01,\n","         -2.15621591e-02,  3.09692025e-02,  7.89820328e-02, -1.68118421e-02,\n","          5.87124415e-02, -3.21991332e-02,  1.66930736e-03,  5.04982211e-02,\n","          3.94614413e-02,  3.28671522e-02,  3.24755441e-03, -3.77390049e-02,\n","         -5.24530187e-02,  4.31666560e-02,  3.68128605e-02,  5.40915923e-03,\n","         -3.31310146e-02, -4.73643318e-02, -7.48448120e-03,  5.57228737e-02,\n","          4.77671361e-04,  1.09336376e-02, -1.71212219e-02, -1.96028799e-02,\n","          3.35858241e-02,  2.85613611e-02,  6.05569929e-02,  8.25261511e-03,\n","          5.82338572e-02, -6.18591234e-02, -1.38365338e-02, -9.29808393e-02,\n","         -2.49568727e-02, -4.18373719e-02, -7.41152384e-04,  5.50149307e-02,\n","         -2.83596739e-02, -2.91609913e-02,  2.31510997e-02, -1.82705335e-02,\n","          2.78825524e-07,  4.84804809e-02, -1.11483987e-02,  2.00862940e-02,\n","         -8.11025314e-03,  9.73448856e-04, -3.62392738e-02,  7.88504407e-02,\n","          4.27444167e-02, -6.11369126e-02,  1.09444521e-01,  8.45588464e-03,\n","          4.77188965e-03, -3.32502164e-02,  7.68244173e-03,  1.04173888e-02,\n","          5.27712032e-02,  2.21078061e-02,  3.43233868e-02, -2.09541246e-03,\n","          2.83842981e-02,  6.92449585e-02, -1.54591361e-02, -8.15437436e-02,\n","         -2.56864466e-02,  1.25686238e-02, -3.37084159e-02, -3.57852839e-02,\n","         -3.70061696e-02, -9.06435121e-03, -9.13764611e-02, -3.29992548e-02,\n","         -1.52473934e-02,  4.33018990e-03,  1.32345939e-02, -3.31202857e-02,\n","         -9.87866055e-03, -8.60140193e-03, -1.04391083e-01, -1.22287944e-02,\n","         -2.38258466e-02,  1.89043283e-02, -2.38258820e-02,  1.58485118e-02,\n","         -1.04094362e-02, -1.66350137e-02,  4.62699234e-02, -4.25799303e-02,\n","          2.00202838e-02,  9.26369280e-02, -5.65071814e-02, -1.09606888e-02,\n","          1.62604749e-02,  1.55352084e-02, -6.36990070e-02,  1.72504578e-02,\n","         -1.46895321e-02, -1.85454730e-02, -4.42504399e-02,  2.59833392e-02,\n","         -7.32877329e-02, -2.34075896e-02,  2.66167149e-02,  5.01750363e-03,\n","         -1.66471321e-02,  2.85596005e-03,  2.91380994e-02, -5.01928292e-02,\n","          2.77849740e-34, -4.29821126e-02,  7.87063241e-02, -1.32186804e-03,\n","         -5.62513247e-02,  7.64833950e-03,  2.40294095e-02, -3.33216712e-02,\n","          2.34534759e-02,  1.74410157e-02, -2.19959412e-02, -1.96086746e-02]),\n","  'score': tensor(0.0077)},\n"," {'page_number': 1090,\n","  'sentence_chunk': 'Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it can cause the spine to curve. The increase in spine curvature not only causes pain, but also decreases a person’s height. Curvature of the upper spine produces what is called Dowager’s hump, also known as kyphosis. Severe upper-spine deformity can compress the chest cavity and cause difficulty breathing. It may also cause abdominal pain and loss of appetite because of the increased pressure on the abdomen. 1090 | Nutrition, Health and Disease',\n","  'chunk_char_count': 524,\n","  'chunk_word_count': 86,\n","  'chunk_token_count': 131.0,\n","  'embedding': array([ 2.52677593e-02,  4.56459261e-02,  1.34791210e-02,  9.88827553e-04,\n","          5.57550006e-02,  1.98626034e-02, -3.59463841e-02,  5.59148006e-02,\n","         -2.04613265e-02,  1.96834505e-02,  3.33012305e-02,  7.06745610e-02,\n","          2.41481559e-03, -3.97070982e-02,  2.67823134e-02, -5.95306139e-03,\n","         -3.23891174e-03,  4.10845689e-03,  3.71918082e-02, -1.21674743e-02,\n","         -2.42468584e-02,  3.17772962e-02,  1.89284999e-02, -2.43543414e-03,\n","          6.28962135e-03, -1.22416550e-02, -1.32905273e-02,  3.66305821e-02,\n","         -1.60819329e-02,  9.10368282e-03,  1.98913887e-02,  5.28351404e-03,\n","         -7.56285852e-03, -2.92758588e-02,  2.09937161e-06,  1.51513079e-02,\n","         -8.19235221e-02, -1.15339383e-02, -9.24444497e-02,  1.82868261e-02,\n","          2.95388922e-02, -1.96383763e-02,  7.40240421e-03, -4.90699336e-02,\n","          1.76239237e-02,  1.79482729e-03, -5.40805422e-03,  8.66584927e-02,\n","          1.05025470e-02, -1.41705200e-02,  2.24527251e-03, -3.79936607e-03,\n","         -4.90759648e-02, -3.07606300e-03,  9.55942571e-02, -6.48778081e-02,\n","         -3.75003032e-02,  3.56897227e-02, -5.60660958e-02,  5.52631915e-03,\n","         -5.06841764e-03, -4.88569774e-02, -3.69486469e-03,  1.63809564e-02,\n","         -4.81905751e-02,  4.79256772e-02,  8.56497698e-03, -7.47044059e-03,\n","          5.72953699e-03,  4.14545275e-02, -5.98330610e-02, -1.38185676e-02,\n","          3.01685892e-02, -2.74786241e-02,  4.26274538e-02, -6.27294183e-02,\n","          3.85805368e-02, -1.36719551e-02,  3.47812362e-02, -4.95916270e-02,\n","          6.64230287e-02, -1.19882612e-03, -1.26433047e-02, -1.70120504e-02,\n","          1.04241781e-02, -9.85860080e-02,  3.00201830e-02, -3.03160343e-02,\n","          4.29370664e-02, -3.50798443e-02,  2.25183088e-02,  2.06660889e-02,\n","         -9.77129955e-03, -1.91871282e-02, -2.61404309e-02,  6.58095675e-03,\n","         -3.15555558e-02, -1.98579356e-02,  7.63375610e-02, -8.32459405e-02,\n","          3.85334156e-02,  2.36440208e-02,  1.11260097e-02, -2.49651708e-02,\n","          8.51395167e-03,  8.59826878e-02, -4.20764834e-02,  7.11800233e-02,\n","         -1.75812393e-02,  7.97143131e-02,  7.64477579e-03,  1.13277766e-03,\n","         -1.28477737e-02, -3.72184366e-02, -4.79495339e-02,  2.93754935e-02,\n","          1.94530524e-02,  9.04534571e-03,  3.15585993e-02,  1.55574279e-02,\n","          5.61023736e-03,  4.45481315e-02, -1.41461287e-02,  5.33155911e-02,\n","          1.88731924e-02,  3.92292365e-02,  2.28662472e-02,  3.39189405e-03,\n","          1.12395529e-02,  3.63597684e-02,  3.70526277e-02, -4.13064174e-02,\n","         -2.90439129e-02,  5.64039908e-02,  3.70984711e-02, -5.87487258e-02,\n","         -2.93683633e-03,  4.04363871e-02,  6.80602193e-02, -5.75995306e-03,\n","         -3.36444117e-02,  1.60824601e-02,  1.31181665e-02, -1.80545021e-02,\n","          2.54281890e-02,  3.27094668e-03, -2.74762176e-02, -3.64528061e-03,\n","         -1.67258326e-02,  8.30961973e-04, -1.13179907e-01, -3.71137857e-02,\n","         -3.73447463e-02, -2.03981418e-02,  6.40463978e-02, -2.30012462e-02,\n","         -8.97950213e-03, -3.15184775e-03,  3.54129896e-02, -1.20799858e-02,\n","          2.06194576e-02,  3.65776895e-03, -7.12767169e-02,  3.49237281e-03,\n","          1.37846954e-02,  2.09146999e-02,  9.75831412e-03,  5.23378886e-02,\n","         -4.56372164e-02,  7.27836322e-03, -2.71984953e-02,  2.78926976e-02,\n","          3.42865400e-02,  2.39907634e-02, -1.92077458e-02,  1.20748915e-02,\n","         -9.34634265e-03,  6.00316040e-02, -7.21389055e-02, -5.93795367e-02,\n","         -1.68162361e-02,  3.23536880e-02, -1.93044101e-03, -4.22285423e-02,\n","          2.58468594e-02,  5.28082205e-03,  1.66048948e-02, -2.87867966e-03,\n","          1.11496188e-02, -4.01857495e-02, -1.58867948e-02,  2.13430617e-02,\n","         -2.51843594e-02,  9.53812711e-03, -2.99543212e-03,  1.05485760e-01,\n","         -2.35994849e-02,  2.68388931e-02,  1.34366006e-02, -2.60608830e-02,\n","         -3.23560238e-02,  3.15207280e-02, -7.36162961e-02,  8.34716426e-04,\n","          1.93206631e-02,  1.38973044e-02, -3.91764380e-02,  1.37707507e-02,\n","         -5.08362576e-02, -2.49726605e-02, -3.69724669e-02, -1.97306201e-02,\n","         -3.17350402e-02, -8.88454262e-03,  3.67591940e-02,  7.71159902e-02,\n","          2.78386716e-02, -4.74612974e-02, -4.64394800e-02, -2.37818360e-02,\n","         -3.50135565e-02, -6.95201829e-02, -7.23688491e-03,  1.10516194e-02,\n","         -7.18328431e-02,  4.29076552e-02, -1.32488171e-02,  1.71915647e-02,\n","         -2.91214567e-02, -3.13714668e-02, -5.77847380e-03,  4.20502312e-02,\n","         -1.63164139e-02,  4.40972950e-03, -8.40877369e-02,  3.25840078e-02,\n","         -5.02189212e-02, -2.72475909e-02,  1.93434339e-02,  6.02506474e-02,\n","          1.79055016e-02, -9.73539501e-02, -6.39446219e-03, -2.84710173e-02,\n","         -1.91701320e-03, -7.89356008e-02,  2.76408345e-02,  3.57186496e-02,\n","         -5.32179177e-02,  5.47554679e-02, -6.13031983e-02, -3.16763893e-02,\n","          2.74951085e-02,  1.17817596e-02,  1.75307989e-02, -2.07442921e-02,\n","         -4.38859351e-02, -2.89510265e-02, -8.76526069e-03, -8.39928538e-02,\n","          1.63453557e-02,  1.59503296e-02,  2.98554935e-02,  2.17394400e-02,\n","         -3.91085912e-03,  4.57084402e-02, -4.62126806e-02, -2.60319281e-02,\n","          2.40092091e-02,  1.98626705e-02, -5.91391958e-02, -8.37011263e-03,\n","          4.10270365e-03,  4.84053558e-03, -6.95302561e-02, -2.19985917e-02,\n","         -2.92065367e-02,  3.67812552e-02,  1.84231270e-02,  1.76147595e-02,\n","         -3.01527679e-02,  1.24085322e-01, -3.37796025e-02, -3.29488330e-02,\n","         -1.52780488e-02,  5.05923294e-04,  3.14446948e-02, -4.77396976e-03,\n","          6.54819384e-02,  4.01339829e-02,  2.13586893e-02,  1.74932089e-02,\n","          4.37113282e-04, -5.87182269e-02, -3.35276835e-02,  1.29429111e-02,\n","          2.79287696e-02,  1.40521666e-02,  5.01857810e-02,  1.48456648e-03,\n","         -2.43867096e-03,  1.89603977e-02,  1.69946849e-02,  2.77253706e-02,\n","         -4.30817157e-02, -9.22971033e-03, -2.84602046e-02, -5.84311150e-02,\n","          1.05112912e-02,  1.25794094e-02,  2.91905962e-02, -1.19457720e-03,\n","         -1.69521179e-02, -6.49105012e-02,  3.00824866e-02,  3.14759389e-02,\n","          4.23914241e-03, -4.47319783e-02, -1.29667930e-02, -3.44311033e-04,\n","          6.19520340e-03,  7.75357932e-02, -2.57221051e-02,  2.03709267e-02,\n","         -3.91547605e-02,  2.87691448e-02,  1.10465400e-01, -1.23576540e-03,\n","          1.04025193e-02,  3.81267071e-03,  3.68261673e-02,  2.89955232e-02,\n","          2.32380368e-02,  2.41326205e-02, -3.42320204e-02, -6.92191347e-03,\n","         -1.66278146e-03,  1.61187723e-03, -4.10616538e-03,  3.41235660e-02,\n","          5.68778662e-04, -3.08668911e-02, -1.49299121e-02,  7.78076127e-02,\n","         -2.22562929e-03, -5.58791077e-03, -9.98689514e-03,  1.52210779e-02,\n","          2.64978781e-02, -5.86516857e-02, -4.93495204e-02, -2.13743327e-03,\n","         -3.66717428e-02,  5.67400418e-02, -5.49891498e-03, -6.90692291e-02,\n","          1.76582560e-02,  4.55141217e-02, -3.30976509e-02, -3.74503928e-04,\n","         -4.99582179e-02, -3.80257471e-03, -2.23900210e-02, -1.56888668e-03,\n","          3.90517600e-02,  4.05543894e-02, -1.22899711e-02, -5.56089059e-02,\n","          1.94079932e-02, -5.89921176e-02, -6.33777976e-02,  1.64964274e-02,\n","         -2.73759868e-02,  3.57441828e-02, -2.24257279e-02, -1.61215253e-02,\n","         -9.02259839e-04, -2.74478439e-02,  1.27179530e-02, -4.64737322e-03,\n","          3.21342386e-02,  4.39637825e-02, -3.33619565e-02, -3.64471925e-03,\n","         -1.15266128e-03,  1.89696904e-02,  2.38045994e-02,  4.00985107e-02,\n","         -1.73539214e-04,  1.97103936e-02, -4.62282002e-02,  6.47430420e-02,\n","          2.05586329e-02,  1.16571523e-02,  3.59440073e-02,  4.73569669e-02,\n","         -1.74716655e-02,  2.72501763e-02,  2.93477047e-02,  4.79592271e-02,\n","         -1.30982250e-02, -4.01311554e-02,  3.17666717e-02, -5.12446463e-03,\n","         -4.58411593e-03, -2.06873976e-02, -8.54094923e-02,  2.28482876e-02,\n","          7.23265531e-03, -6.04164228e-02, -5.14176972e-02,  4.69482578e-02,\n","          8.38059559e-03,  1.20627563e-02, -7.63190072e-03,  3.46492715e-02,\n","          1.24329561e-03, -6.53250888e-02, -1.32486569e-02, -1.02421865e-02,\n","          8.84454623e-02, -2.35847645e-02, -4.55786437e-02, -3.39120440e-02,\n","          1.74971670e-02, -3.70673500e-02,  2.10896619e-02,  6.27231831e-03,\n","          1.35365753e-02,  2.24304907e-02,  3.61060910e-02, -4.17738333e-02,\n","         -1.43471779e-02,  5.85953444e-02,  2.24126782e-02,  4.57516015e-02,\n","          5.67416698e-02, -4.78866734e-02,  2.45912578e-02, -1.66096836e-02,\n","          9.97717753e-02, -4.73854691e-02, -3.30259129e-02,  3.62115018e-02,\n","          5.26068881e-02, -3.07047889e-02, -1.11531066e-02, -3.17519121e-02,\n","         -4.25414816e-02,  8.68198797e-02, -3.97100346e-03, -1.11412741e-02,\n","         -1.60172954e-02,  2.98290793e-02, -7.01830795e-05,  2.64310893e-02,\n","         -9.26280767e-02,  1.36651797e-02,  1.94005165e-02,  2.74232868e-03,\n","         -3.77240442e-02, -1.63185596e-02,  2.18304228e-02, -3.00390292e-02,\n","          4.34191572e-03,  7.39536062e-02,  3.24572087e-03, -4.62840637e-03,\n","          1.11464746e-02,  7.88306221e-02, -4.20681015e-02, -4.16058174e-04,\n","         -1.07243219e-02, -9.12590045e-03, -4.05043103e-02, -3.25506926e-02,\n","         -2.83234403e-03, -6.40427098e-02, -7.74229243e-02, -4.51335125e-03,\n","          6.43650303e-03, -4.64354083e-02, -3.74998222e-03, -3.97723280e-02,\n","          3.86475474e-02, -3.46102228e-04,  1.95122771e-02,  1.59579590e-02,\n","         -4.24923655e-03,  4.26496658e-03,  2.01157164e-02, -3.40919830e-02,\n","          5.74858161e-03, -3.11061908e-02, -4.94341925e-02,  2.37540323e-02,\n","          5.42952120e-03,  8.38271379e-02,  3.96388024e-03,  1.02594150e-02,\n","          3.42485905e-02, -2.88281050e-02, -4.42530494e-03, -5.61900735e-02,\n","         -1.67920981e-02, -2.35569719e-02,  1.62868537e-02, -5.84331788e-02,\n","          4.58016247e-03, -1.86258499e-02,  3.02513409e-03,  2.92231347e-02,\n","          7.50010088e-02,  6.99660927e-02,  4.48196828e-02, -7.95619115e-02,\n","         -3.49651538e-02, -9.10241436e-03, -4.91035655e-02,  2.47088857e-02,\n","         -2.32074428e-02,  2.09299996e-02, -6.57613799e-02, -1.36737935e-02,\n","         -6.05981573e-02,  3.03748958e-02, -3.53948325e-02,  2.26256009e-02,\n","          9.64488275e-03,  4.45205893e-04,  7.71393999e-02,  1.21442908e-02,\n","         -2.01163758e-02, -2.68969163e-02,  3.76537489e-03, -1.14372987e-02,\n","         -1.16402705e-04, -4.84171808e-02,  2.59598135e-03, -1.48390457e-02,\n","          4.11208533e-02, -2.66246195e-03, -4.76897648e-03,  3.05332169e-02,\n","          1.12652136e-02,  2.75518019e-02,  6.18465766e-02, -1.68123655e-02,\n","         -4.04407568e-02, -6.61001652e-02, -5.33034429e-02, -2.73844190e-02,\n","         -2.39618309e-02,  3.56848165e-03,  2.00314075e-02, -5.53021787e-33,\n","          8.19430500e-03, -2.05891486e-02, -1.56005949e-03,  2.82918401e-02,\n","          2.75181849e-02,  7.98333362e-02, -2.31103729e-02, -1.78047232e-02,\n","         -1.49703156e-02,  4.57550846e-02, -1.67949554e-02, -1.14035476e-02,\n","          1.67063903e-02, -2.56183408e-02, -3.96299921e-03, -1.76505316e-02,\n","          3.11641451e-02, -2.66328286e-02,  7.24615715e-03,  2.21836455e-02,\n","         -1.68646183e-02, -3.56016979e-02,  2.56407131e-02, -2.72122733e-02,\n","          4.16290238e-02, -5.02842441e-02,  6.91905059e-03, -3.46749052e-02,\n","         -1.73484199e-02,  1.74713763e-03, -4.76389565e-03, -3.06914989e-02,\n","         -1.43733835e-02,  1.55092624e-03, -2.72642486e-02, -7.96955265e-03,\n","          2.54031154e-03,  2.55986378e-02,  1.90636851e-02,  3.46378833e-02,\n","          3.15942988e-02, -2.36881077e-02, -1.82265546e-02,  8.09642393e-03,\n","          7.48755550e-03,  4.19227360e-03,  1.96193764e-03, -1.17175523e-02,\n","          5.89985261e-03, -4.36621867e-02,  3.33559699e-02, -2.30972972e-02,\n","         -1.63778998e-02, -5.56735769e-02, -2.99853701e-02, -6.75951391e-02,\n","          2.81333439e-02, -7.37189595e-03,  8.14692602e-02,  1.40422594e-03,\n","         -2.01255758e-03,  2.14659330e-02, -2.14056578e-02, -2.20942721e-02,\n","          7.10617891e-03, -4.98236082e-02, -7.46771246e-02, -4.64178203e-03,\n","         -5.31445667e-02, -1.91098135e-02, -1.76128615e-02,  7.65774101e-02,\n","          4.81170714e-02, -5.88778779e-02,  3.67425047e-02, -6.11153319e-02,\n","          4.29246835e-02,  2.05619000e-02,  8.15455522e-03,  2.73771887e-03,\n","          2.25101784e-02,  3.18042864e-03, -1.99485160e-02, -3.61070316e-03,\n","          2.96551362e-02, -3.67140584e-02,  4.92540374e-03,  3.04851606e-02,\n","         -6.18105195e-02, -5.89138642e-03,  4.41838130e-02, -5.89252310e-03,\n","         -1.68243516e-02, -8.57224315e-03,  5.12665249e-02,  3.57171358e-03,\n","          3.18397358e-02, -6.81433687e-03,  9.96976718e-03,  2.84236260e-02,\n","          6.91652745e-02,  2.87329890e-02,  3.02453060e-02,  5.69964200e-03,\n","          1.10078650e-02,  1.31093161e-02,  1.60856210e-02, -1.31719178e-02,\n","          4.32236046e-02,  1.77597478e-02,  2.29942482e-02,  7.28647644e-03,\n","          6.09127358e-02,  4.74097729e-02,  7.34581752e-03, -1.89836677e-02,\n","         -4.06418927e-02,  1.56721696e-02,  1.37330568e-03,  9.91674364e-02,\n","         -4.24773619e-02,  1.79546012e-04, -3.47786210e-02,  3.76352631e-02,\n","         -7.45034516e-02, -7.54706329e-03,  5.19717205e-03,  2.49274187e-02,\n","         -4.88397777e-02, -4.40320373e-02,  1.79231390e-02,  1.77809820e-02,\n","          2.83909969e-07, -4.80549317e-03,  7.58282989e-02,  6.12138323e-02,\n","         -5.72482646e-02,  5.57962805e-02,  5.20715229e-02,  2.20830794e-02,\n","         -5.54755516e-03,  4.35179594e-04,  3.13835144e-02, -3.42954393e-03,\n","          4.15880680e-02,  1.20900050e-02,  3.70143875e-02,  2.29135132e-03,\n","         -2.46085320e-03, -2.87754871e-02,  3.09552941e-02, -2.16042120e-02,\n","          3.81323956e-02, -4.17386182e-02, -5.67035340e-02, -5.78978173e-02,\n","         -4.15181778e-02, -2.96478309e-02,  3.34112681e-02, -4.16625626e-02,\n","          1.81927474e-03,  4.83019277e-02, -1.26219047e-02,  5.97607829e-02,\n","          1.07572246e-02, -1.05820643e-02, -7.24187912e-03, -3.31029035e-02,\n","         -4.58162688e-02, -6.47280924e-03, -3.49313579e-02, -1.33871259e-02,\n","         -1.00558445e-01,  4.79497649e-02,  3.66285397e-03,  2.54162960e-02,\n","         -2.50843577e-02,  3.00672613e-02,  5.69541492e-02,  3.43475640e-02,\n","         -4.12217006e-02,  1.07904254e-02,  2.45501101e-02,  3.84805910e-02,\n","         -2.13364717e-02,  1.31164677e-02, -3.77881527e-03,  1.48532242e-02,\n","         -4.58223280e-03, -2.27131154e-02, -4.94862609e-02,  3.28713171e-02,\n","         -6.54815929e-03, -6.00857427e-03,  6.43172413e-02,  3.23193409e-02,\n","         -4.88236360e-03,  2.63138488e-03,  4.17331606e-02, -2.39798222e-02,\n","          2.75885769e-34, -9.65112168e-03,  3.41306664e-02, -1.85319199e-03,\n","         -7.17376620e-02, -8.36076960e-03, -3.47110927e-02,  1.06046339e-02,\n","          3.61416675e-03, -1.59389544e-02, -4.86350507e-02, -3.89015228e-02]),\n","  'score': tensor(0.0074)},\n"," {'page_number': 1077,\n","  'sentence_chunk': 'esophagus and cause irritation. It is estimated that GERD affects 25 to 35 percent of the US population. An analysis of several studies published in the August 2005 issue of Annals of Internal Medicine concludes that GERD is much more prevalent in people who are obese.1 The most common GERD symptom is heartburn, but people with GERD may also experience regurgitation (flow of the stomach’s acidic contents into the mouth), frequent coughing, and trouble swallowing. There are other causative factors of GERD that may be separate from or intertwined with obesity. The sphincter that separates the stomach’s internal contents from the esophagus often does not function properly and acidic gastric contents seep upward. Sometimes the peristaltic contractions of the esophagus are also sluggish and compromise the clearance of acidic contents. In addition to having an unbalanced, high-fat diet, some people with GERD are sensitive to particular foods—chocolate, garlic, spicy foods, fried foods, and tomato-based foods—which worsen symptoms. Drinks containing alcohol or caffeine may also worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency of recurring symptoms. A more proper diagnosis can be made when a doctor inserts a small device into the lower esophagus that measures the acidity of the contents during one’s daily activities.',\n","  'chunk_char_count': 1362,\n","  'chunk_word_count': 210,\n","  'chunk_token_count': 340.5,\n","  'embedding': array([ 2.70217806e-02, -3.83477323e-02,  1.56746041e-02, -3.91171388e-02,\n","          7.53519917e-03,  1.66651998e-02, -3.98133928e-03,  4.69807386e-02,\n","          1.23318806e-01, -1.12976134e-02,  1.62594281e-02, -2.98251491e-02,\n","          1.75872154e-03, -5.27708679e-02, -1.98795516e-02,  5.45181334e-04,\n","         -3.81452441e-02,  9.99750942e-03,  2.73882411e-02, -1.59627311e-02,\n","         -1.99941695e-02,  1.83432561e-03, -5.47511131e-02,  3.81027125e-02,\n","          5.91423654e-04, -4.29776832e-02,  2.98900325e-02, -6.62425086e-02,\n","          5.39378226e-02, -6.83864430e-02,  3.54621187e-02,  4.71732579e-02,\n","         -5.78983724e-02,  1.53193409e-02,  1.93190681e-06, -2.49875151e-02,\n","         -1.60074066e-02,  5.77086918e-02, -4.19689603e-02, -2.40833331e-02,\n","         -1.54035259e-03,  7.75952963e-03, -6.39004924e-04,  1.42524596e-02,\n","         -8.26541428e-03, -5.18266521e-02,  2.70797182e-02,  3.31102200e-02,\n","          4.42009680e-02, -3.07451263e-02,  1.42941559e-02, -1.14409342e-01,\n","         -2.66402382e-02,  3.31820827e-03,  7.41938278e-02,  6.07344732e-02,\n","          1.99017022e-02, -5.05718142e-02, -2.06167921e-02,  3.09991762e-02,\n","         -1.02324076e-02, -2.56627984e-02,  2.98161618e-02,  1.05676338e-01,\n","          1.51660424e-02,  4.95001785e-02, -1.99616812e-02, -7.03487312e-03,\n","          5.12840077e-02,  2.99802832e-02, -2.59781498e-02,  1.81664117e-02,\n","          1.87407192e-02,  1.95749593e-03, -5.68897761e-02, -5.89800207e-03,\n","          7.41547570e-02, -3.89480740e-02, -6.82248324e-02, -1.02784124e-03,\n","          5.87935932e-02,  4.53348644e-03,  2.25021411e-03, -5.24593852e-02,\n","         -2.65305955e-02,  4.27813195e-02,  6.13285182e-03, -2.96479762e-02,\n","          2.35166512e-02, -3.63325365e-02, -4.36702147e-02, -3.50492559e-02,\n","          2.14271899e-03, -2.80494522e-02, -3.66316549e-02,  3.62627283e-02,\n","          3.97919565e-02, -4.24593650e-02,  5.04038259e-02, -4.63709533e-02,\n","         -1.97894741e-02, -7.48180877e-03, -8.28381162e-03,  1.50214871e-02,\n","         -4.71470580e-02,  3.71150039e-02, -1.35343960e-02,  8.79655629e-02,\n","         -1.17656030e-02,  3.83027345e-02, -2.01635994e-02, -1.18047148e-02,\n","         -2.14309841e-02,  2.15736981e-02, -2.04803050e-02, -3.61538678e-02,\n","         -3.82424034e-02, -5.66838644e-02, -3.93397249e-02,  4.79697064e-02,\n","         -4.68695872e-02, -1.35634691e-04,  2.49347873e-02,  3.81546393e-02,\n","         -1.05199292e-02,  4.38123457e-02,  3.92980389e-02, -6.36973325e-03,\n","         -3.35560297e-03,  4.80801947e-02,  4.36875708e-02, -2.47033369e-02,\n","         -2.83280946e-02,  2.27317419e-02,  2.49169096e-02, -3.77167873e-02,\n","         -2.99317576e-02,  3.43140103e-02, -2.61814557e-02, -3.41573097e-02,\n","         -8.78803059e-03, -2.76944553e-03, -6.42197381e-04,  5.42972377e-03,\n","          4.33242843e-02,  2.49086097e-02,  4.89015505e-03, -4.00274396e-02,\n","          7.74133950e-03,  3.42258774e-02, -8.81405082e-03, -9.54144076e-03,\n","          3.65907103e-02,  3.02537587e-02,  2.66799908e-02,  5.75206708e-03,\n","         -3.42515111e-02,  1.11022801e-03, -1.14691574e-02, -3.42756025e-02,\n","          2.66558141e-03,  1.83892995e-02,  1.45847453e-02,  2.57736463e-02,\n","         -2.68076435e-02,  4.17982489e-02,  7.32793584e-02,  3.88336629e-02,\n","         -1.10541107e-02, -9.50288214e-03,  3.80871422e-03,  4.52790894e-02,\n","          4.56116535e-02,  5.37467655e-03,  2.88154352e-02, -1.10592414e-02,\n","         -5.31355711e-03,  1.43118426e-02, -3.49989198e-02, -1.82626117e-02,\n","          3.91314141e-02,  7.52567500e-02,  1.50432726e-02, -3.38000581e-02,\n","         -4.77231815e-02, -4.91683325e-03,  2.81647383e-03,  6.55348599e-02,\n","         -5.40672876e-02, -1.32116135e-02, -2.30597332e-02,  6.96287379e-02,\n","          4.16434668e-02,  2.73464510e-04,  2.37364247e-02, -1.21492809e-02,\n","         -4.40848842e-02,  4.54753935e-02,  9.03544389e-03, -6.99001690e-03,\n","         -2.59519015e-02, -1.60052697e-03,  4.33481075e-02, -2.50060670e-02,\n","          3.45951058e-02, -1.66398566e-02, -7.39339646e-03,  2.93672737e-03,\n","         -1.89812202e-02, -8.77650082e-02, -4.46082242e-02, -2.18522549e-02,\n","         -6.40737591e-03,  2.01254375e-02, -1.05275428e-02,  5.18881045e-02,\n","          5.07550798e-02,  4.58774231e-02,  1.84906647e-02,  7.45488256e-02,\n","         -8.42990912e-03, -2.85418313e-02,  6.94043189e-02, -1.41158337e-02,\n","         -3.09609957e-02,  5.15540829e-03, -7.39067746e-03, -2.48689707e-02,\n","         -1.42445778e-02, -5.20587899e-02,  1.58971362e-02, -1.13285601e-03,\n","         -1.03399433e-01,  2.67579034e-02, -2.25025062e-02,  4.02271003e-02,\n","         -5.92265604e-03, -1.14953667e-02, -1.93761401e-02,  1.01907082e-01,\n","         -7.17811212e-02,  2.21039988e-02, -9.24695954e-02,  1.57308523e-02,\n","         -5.83981983e-02, -5.02604172e-02,  2.65647601e-02,  5.91969211e-03,\n","         -6.97621703e-02,  7.62763023e-02, -3.92323732e-02,  1.93227809e-02,\n","         -3.23386863e-02,  1.28127495e-02,  1.37294689e-02, -5.60731115e-03,\n","          1.37376729e-02,  4.16575931e-02, -5.77417947e-02, -6.69546276e-02,\n","         -3.20351273e-02,  3.18596736e-02,  1.46714486e-02,  7.05615617e-03,\n","         -1.58827286e-02, -3.85532118e-02,  1.00937765e-02, -4.68134396e-02,\n","         -2.72008544e-03,  1.33709027e-03, -2.65616681e-02, -2.90679955e-03,\n","         -2.18395493e-03,  4.83962297e-02, -2.16799532e-03, -1.28351040e-02,\n","         -3.60919163e-02,  3.78829092e-02,  3.38558890e-02,  1.88115276e-02,\n","         -3.30498815e-03,  1.19146965e-02, -2.72938833e-02,  2.89956946e-02,\n","          1.19720344e-02,  4.49469499e-03, -3.39418626e-03, -6.28107712e-02,\n","          3.00550237e-02, -3.61933559e-02,  6.39744103e-02,  1.93046462e-02,\n","          2.20466703e-02, -1.62980501e-02, -2.23190784e-02,  1.06585044e-02,\n","          2.01212186e-02,  5.78609593e-02, -1.98823377e-03,  3.56211700e-02,\n","         -3.43136974e-02,  2.82752216e-02,  9.48209967e-03,  1.39092552e-02,\n","         -3.78057472e-02,  5.25222393e-03, -1.00636585e-02, -2.37301961e-02,\n","          1.02054130e-03,  6.59561753e-02,  2.48849727e-02, -8.27869773e-03,\n","         -3.44562642e-02, -4.36334051e-02,  1.87299959e-02, -2.00550463e-02,\n","         -4.28461656e-02, -3.43893543e-02,  3.77287269e-02, -3.25038247e-02,\n","          4.25849139e-04,  4.12546806e-02, -2.20190920e-02, -4.38767634e-02,\n","         -4.66489494e-02, -3.29813100e-02,  5.29866433e-03, -2.18354259e-02,\n","          9.96296015e-03, -2.02380978e-02,  1.97049566e-02,  1.98172573e-02,\n","          3.01887728e-02,  4.34099026e-02, -1.52086830e-02,  9.88455769e-03,\n","         -2.06795223e-02,  2.74470542e-02, -6.54092953e-02, -7.16866925e-03,\n","          3.33237909e-02, -4.43142094e-03, -6.89381547e-03,  1.58348195e-02,\n","          1.27766430e-02, -4.26220037e-02,  5.74221648e-02, -1.88866106e-03,\n","         -1.19937975e-02,  3.71325575e-02, -2.50075366e-02, -2.58022286e-02,\n","          2.43667774e-02, -2.51314230e-02, -2.43845247e-02, -4.43312302e-02,\n","          4.38506976e-02,  1.21372603e-02, -2.50215791e-02,  1.00992424e-02,\n","         -2.49512098e-03,  6.87430352e-02, -1.06926048e-02, -1.95397511e-02,\n","          5.11207292e-03,  3.50245424e-02,  5.67443995e-03, -4.40713950e-02,\n","          4.46579978e-02, -5.69193289e-02, -1.66706685e-02,  2.37683076e-02,\n","         -5.33327237e-02,  3.91846374e-02,  5.22231963e-03, -5.41549642e-03,\n","          4.09486517e-03,  1.15666859e-04, -1.33039765e-02, -4.12693247e-02,\n","         -5.31420344e-04, -4.34975922e-02,  2.59916522e-02, -4.94749285e-03,\n","         -3.11312154e-02, -1.35178855e-02,  2.12855898e-02, -4.15644497e-02,\n","         -2.74078436e-02,  3.06901126e-03,  7.25039281e-03,  2.08024606e-02,\n","         -2.78721079e-02, -1.25386538e-02,  6.50135279e-02,  1.18315835e-02,\n","          1.55729149e-02, -8.88994783e-02,  2.61562541e-02,  6.22535646e-02,\n","          3.26663591e-02, -1.22113205e-01, -8.11192906e-04,  3.29332240e-02,\n","          1.10581992e-02, -4.33732234e-02, -5.97192012e-02,  1.10623874e-02,\n","          1.00958131e-01, -2.45245546e-02,  5.69292866e-02, -3.64610814e-02,\n","          2.48644371e-02, -1.36920400e-02,  4.29261513e-02,  3.36565971e-02,\n","          7.70662958e-03, -4.27972786e-02,  5.57842571e-03, -5.77209070e-02,\n","          1.93264280e-02,  1.82908326e-02,  5.03421947e-02,  1.05541795e-02,\n","         -1.62573066e-02, -2.56907791e-02,  1.26661058e-03,  3.66748944e-02,\n","          1.27469860e-02, -3.80741507e-02,  3.88243012e-02,  5.94356516e-03,\n","          2.59677470e-02,  5.01816384e-02,  4.30804193e-02,  5.08374758e-02,\n","          6.10364787e-02,  1.97768640e-02,  4.63724090e-03,  5.84004968e-02,\n","          4.72761318e-02, -4.41588312e-02, -7.59902352e-04,  9.54205450e-03,\n","          5.78004122e-02, -2.19122432e-02,  4.41941479e-03,  8.54150057e-02,\n","         -2.51088347e-02,  5.21350168e-02,  1.82376280e-02, -1.10584814e-02,\n","         -1.64337382e-02, -1.39507167e-02,  5.59683107e-02, -2.08990667e-02,\n","          3.02678309e-02,  1.38864890e-02,  1.55432876e-02, -3.69760767e-03,\n","          1.04188239e-02,  9.51664615e-03,  3.59618142e-02, -5.77086732e-02,\n","          3.32833007e-02, -5.44406846e-03, -7.90620074e-02,  1.98496552e-03,\n","          2.34198887e-02,  8.71502142e-03, -5.95514961e-02,  6.12366712e-03,\n","         -2.84544434e-02,  8.86634830e-03, -3.55523229e-02,  1.51671935e-02,\n","         -3.33107449e-02, -2.33157035e-02, -9.07930806e-02,  5.33433212e-03,\n","          4.16791886e-02,  1.97160635e-02, -2.08395179e-02, -5.30772284e-02,\n","          6.82820240e-03,  2.54718252e-02,  2.87804194e-02,  4.70737293e-02,\n","         -3.26046310e-02,  5.71463164e-03,  3.35557200e-02, -5.59310280e-02,\n","         -1.29895313e-02, -2.79286597e-02, -2.54597757e-02,  3.02532781e-02,\n","         -3.19995731e-02,  3.85220349e-02, -2.11132728e-02, -4.70292047e-02,\n","          5.36682568e-02,  4.26126551e-03,  1.61094684e-02, -5.16411364e-02,\n","          1.86563167e-03, -4.04909030e-02,  1.95944160e-02,  4.92163934e-02,\n","          3.72439101e-02,  2.00747047e-03,  4.38340306e-02,  2.11478770e-02,\n","          1.04528124e-04,  5.40992729e-02, -1.73008051e-02, -8.80164430e-02,\n","         -2.81632715e-03, -2.06907298e-02,  1.51080284e-02, -1.33044366e-02,\n","         -1.11877797e-02, -1.67716779e-02, -4.16301489e-02, -1.40785538e-02,\n","          1.89336427e-02,  2.71006934e-02, -3.81695554e-02,  9.98237752e-04,\n","          8.28160811e-03,  1.91177316e-02, -3.67182754e-02, -5.23976609e-02,\n","         -7.84501340e-03, -4.96397391e-02,  1.40574323e-02, -3.30539681e-02,\n","          1.83754880e-02,  1.69349369e-02, -1.50387147e-02, -7.25036711e-02,\n","         -1.34943919e-02, -3.96872051e-02, -3.64940204e-02,  4.05539386e-02,\n","          1.47882011e-02,  2.09033713e-02,  7.71835772e-03,  2.16525495e-02,\n","         -3.18904296e-02, -6.19599260e-02, -3.45897116e-02,  2.43747458e-02,\n","         -9.14950818e-02, -1.25900493e-03,  1.06886579e-02, -5.01315390e-33,\n","         -1.03732338e-02,  1.27562927e-02,  2.79693305e-02,  5.04126726e-03,\n","          6.10335842e-02,  1.53106274e-02, -3.17388475e-02,  1.55391032e-02,\n","         -2.74699880e-03,  6.35283142e-02, -2.82733180e-02, -2.76987329e-02,\n","          2.38857977e-02,  9.14989971e-03, -3.36745456e-02, -5.74642085e-02,\n","          8.17291215e-02,  3.95217240e-02, -8.00354127e-03,  2.73501799e-02,\n","         -3.65348160e-02, -5.09556159e-02, -6.91084936e-02, -9.67170671e-02,\n","         -2.13295892e-02, -1.11834286e-02,  4.94349003e-02, -4.14122939e-02,\n","         -1.67513210e-02,  5.85033232e-03,  8.99986131e-04,  5.90686640e-03,\n","          1.27878785e-02, -1.06551006e-01, -2.03814581e-02,  3.56015749e-02,\n","         -9.34123900e-03, -2.38187388e-02, -1.53745357e-02, -2.09171511e-02,\n","          3.40367146e-02,  1.13433003e-02,  1.06750224e-02,  4.15799469e-02,\n","          1.86144412e-02, -2.75170878e-02,  2.46972442e-02,  1.88420285e-02,\n","          4.39365059e-02, -2.98438733e-03, -5.27352095e-03,  2.91167330e-02,\n","          3.40045244e-02,  3.05184089e-02, -1.30264414e-02,  3.27732936e-02,\n","          1.43335983e-02, -1.48011381e-02,  4.15125638e-02, -3.24570872e-02,\n","         -4.88501117e-02, -3.12288236e-02, -5.65916449e-02, -1.67724341e-02,\n","         -9.46464762e-03,  9.54916887e-03, -3.72844934e-02,  1.57966539e-02,\n","         -1.63697507e-02,  2.49342974e-02, -8.69573131e-02, -3.46590653e-02,\n","         -4.41002175e-02, -6.54439256e-02,  3.58093195e-02, -2.19691880e-02,\n","          3.59513201e-02,  5.34921810e-02, -9.93068591e-02, -3.23889777e-02,\n","         -1.46301379e-02, -2.69573685e-02, -1.20684924e-02, -2.90089604e-02,\n","         -1.25298323e-02,  3.73641737e-02,  1.45500908e-02, -8.92970115e-02,\n","         -5.66667458e-03,  1.18048294e-02,  1.48320660e-01, -3.01068369e-03,\n","          3.12293731e-02,  1.63574778e-02,  1.33104222e-02,  6.20237552e-03,\n","          4.12822217e-02, -1.92943588e-02,  1.46975974e-04,  1.25061227e-02,\n","          4.81469817e-02,  9.65286698e-03,  2.11142339e-02, -2.48228898e-03,\n","          1.48439649e-02,  2.26217993e-02, -1.11322163e-03, -2.40939185e-02,\n","         -9.02827270e-03, -2.73634475e-02, -2.57379711e-02,  6.80219457e-02,\n","         -6.75368006e-04,  3.86389084e-02, -3.43065076e-02,  5.73633984e-02,\n","         -5.91257075e-03,  3.96261215e-02,  2.66927527e-03,  4.43086065e-02,\n","         -4.10006288e-03,  1.08326878e-02, -2.43261419e-02, -2.81301723e-03,\n","         -2.48145387e-02, -3.43435779e-02,  1.84908696e-02,  6.74108556e-03,\n","         -3.95936146e-02, -3.17801982e-02,  2.37179119e-02, -4.71149907e-02,\n","          2.79924507e-07,  1.23620974e-02, -2.95126308e-02,  1.37103880e-02,\n","          5.79985417e-03, -1.56977642e-02,  1.53956085e-03,  4.44341600e-02,\n","          5.05852103e-02, -1.23070134e-03, -2.24524643e-02, -1.94147211e-02,\n","          3.06341331e-02, -3.08307167e-02,  1.72229689e-02,  1.77847967e-02,\n","         -1.25594372e-02, -2.30832435e-02,  8.29358399e-02,  3.68292420e-03,\n","         -1.83245284e-03,  5.22550233e-02, -2.84631047e-02, -5.58212437e-02,\n","         -4.50611338e-02, -7.19258329e-03, -6.47943048e-03, -3.08414176e-02,\n","          1.35782585e-02, -1.05106283e-03, -9.16011631e-03,  2.82972660e-02,\n","         -2.65486408e-02,  5.24807088e-02,  6.11405587e-03, -4.00571413e-02,\n","         -2.82973647e-02,  2.29500812e-02, -5.93024604e-02, -2.20876448e-02,\n","         -4.13870327e-02,  5.62005676e-02,  5.98054379e-02, -1.88976079e-02,\n","         -6.31388649e-02, -1.59234013e-02, -4.39983513e-03,  3.44757061e-03,\n","          1.28477871e-01,  4.14162353e-02, -3.74711156e-02, -1.35490559e-02,\n","          1.24214040e-02,  1.16750905e-02,  2.36480450e-03, -1.10410126e-02,\n","          5.90929836e-02,  3.49255726e-02, -3.43810208e-02,  3.43911014e-02,\n","          5.60172834e-02, -3.41355912e-02, -1.05728926e-02,  9.53233801e-03,\n","          1.28590399e-02, -5.24582202e-03,  8.19502212e-03, -4.65004519e-02,\n","          2.56272807e-34, -5.30559644e-02,  4.01622243e-02, -4.06770557e-02,\n","         -5.62329255e-02,  1.20808361e-02, -8.78201891e-03, -9.09224432e-03,\n","          4.16994914e-02,  1.60412840e-03, -8.99688061e-03, -2.07995865e-02]),\n","  'score': tensor(0.0072)}]"]},"metadata":{},"execution_count":68}],"source":["query = random.choice(query_list)\n","print(f\"Query: {query}\")\n","\n","# Answer query with context and return context\n","answer, context_items = ask(query=query,\n","                            temperature=0.7,\n","                            max_new_tokens=512,\n","                            return_answer_only=False)\n","\n","print(f\"Answer:\\n\")\n","print_wrapped(answer)\n","print(f\"Context items:\")\n","context_items"]},{"cell_type":"markdown","metadata":{"id":"GcW2Qor-3jeU"},"source":["## Try the model with different number of examples in prompt_formatter(Multiple shot prompt engineering) and comment on the answers retrieved by RAG. You can also try different examples but its optional."]},{"cell_type":"markdown","source":["I tried removing n_example in prompt formatter to test.\n","When n_exmaple = 3, the output of RAG will have ** or -- a segment-like format, and the segments will be filled from several different dimensions according to the problem. The content is relatively rich.\n","\n","When n_exmaple = 1, the RAG output will be filled from the imitation example format. But the interesting thing is that its output is the longest. Maybe it's a case of extending CoT with fewer samples?\n","\n","When n_exmaple = 0, RAG has no special format. The output is more like the retrieved text. Very little information is extended."],"metadata":{"id":"OBpXr7GAn4oP"}},{"cell_type":"markdown","metadata":{"id":"NQzfN1fkmBih"},"source":["# 2. LoRA Adapter FineTuning (30 Points)"]},{"cell_type":"markdown","metadata":{"id":"OyCKwVfhmWEY"},"source":["[LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) was introduced at the end of 2021 in the paper [LoRA: Low-rank adaptation of Large Language Models.](https://arxiv.org/pdf/2106.09685.pdf)\n","\n","To make fine-tuning more efficient, LoRA’s approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesn’t receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n","\n","Models that want to support LoRA fine-tuning need to provide adapters. There is a list on [Hugging Face](https://huggingface.co/docs/peft/index#supported-models) with the supported models which covers popular open source models and it is growing.\n","\n","You can also read [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)\n","\n","Advantages of LoRA-\n","There are some additional advantages that LoRA provides. One is to use quantization which leads to memory reduction. Read the blog post on Hugging Face [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) for more details.\n","\n","Some people claim that LoRA might also help to reduce [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114) which happens when too much fine-tuning has been done. However, while the original weights are frozen, at inference time the additional parameters are utilized which lead to other results which is the whole point of fine-tuning.\n","\n","![LoRA Adapter](https://heidloff.net/assets/img/2023/08/lora.png)"]},{"cell_type":"code","source":["!pip uninstall -y torch torchvision torchaudio\n","!pip install -q --index-url https://download.pytorch.org/whl/cu124 \\\n","        torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vpKGERbYrga4","executionInfo":{"status":"ok","timestamp":1746234807494,"user_tz":240,"elapsed":51604,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"65ab48e2-305b-45d2-e7de-5e2bc1a35076"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping torch as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m110.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFGPx8lPnGED"},"outputs":[],"source":["## DONOT CHANGE THE CODE\n","\n","!pip install datasets\n","import os\n","!pip install peft\n","if \"COLAB_GPU\" in os.environ:\n","    print(\"[INFO] Running in Google Colab, installing requirements.\")\n","    #!pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n","    !pip install tqdm # for progress bars\n","    !pip install sentence-transformers # for embedding models\n","    !pip install accelerate # for quantization model loading"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qZDiokGmGUN"},"outputs":[],"source":["## DONOT CHANGE THE CODE\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n","from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n","import torch\n","from datasets import load_dataset\n","import os\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W0R9W0BxmeKb"},"outputs":[],"source":["device = \"cuda\"\n","model_name_or_path = \"bigscience/bloomz-560m\"\n","tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n","peft_config = PromptTuningConfig(\n","    task_type=TaskType.CAUSAL_LM,\n","    prompt_tuning_init=PromptTuningInit.TEXT,\n","    num_virtual_tokens=20,\n","    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n","    tokenizer_name_or_path=model_name_or_path,\n",")\n","\n","dataset_name = \"twitter_complaints\"\n","checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n","    \"/\", \"_\"\n",")\n","text_column = \"Tweet text\"\n","label_column = \"text_label\"\n","max_length = 64\n","lr = 5e-2\n","num_epochs = 12\n","batch_size = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ZDExgJamjQb","executionInfo":{"status":"ok","timestamp":1746237318619,"user_tz":240,"elapsed":2970,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"247e8382-f0e1-4c05-fe5e-0bacb9b4c6c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["3\n"]}],"source":["## DONOT CHANGE THE CODE\n","\n","#load data\n","dataset = load_dataset(\"ought/raft\", dataset_name)\n","dataset[\"train\"][0]\n","\n","classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n","dataset = dataset.map(\n","    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n","    batched=True,\n","    num_proc=1,\n",")\n","dataset[\"train\"][0]\n","{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2, \"text_label\": \"no complaint\"}\n","\n","## preprocess\n","tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n","if tokenizer.pad_token_id is None:\n","    tokenizer.pad_token_id = tokenizer.eos_token_id\n","target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n","print(target_max_length)\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBOjLWOMm4PO"},"outputs":[],"source":["## DONOT CHANGE THE CODE\n","\n","def preprocess_function(examples):\n","    batch_size = len(examples[text_column])\n","    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n","    targets = [str(x) for x in examples[label_column]]\n","    model_inputs = tokenizer(inputs)\n","    labels = tokenizer(targets)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n","        # print(i, sample_input_ids, label_input_ids)\n","        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n","        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n","        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n","    # print(model_inputs)\n","    for i in range(batch_size):\n","        sample_input_ids = model_inputs[\"input_ids\"][i]\n","        label_input_ids = labels[\"input_ids\"][i]\n","        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n","            max_length - len(sample_input_ids)\n","        ) + sample_input_ids\n","        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n","            \"attention_mask\"\n","        ][i]\n","        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n","        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n","        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n","        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BdG8zpfHm4_Q","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f17a1027739648f182c9cf460eb67366","be8e36a1c1a54e239399e08e092310d8","3032127b502b4ce8a6aca675b27b4f97","4cc38d850ed241b98361438217b04654","b4c38f2de5e14b7280273755632d7c1b","8b34ebf8b5ff4b9e905014b8f7879106","9f4378d22c3340858392f3434935f29c","5b110924135d4255b7d9023c975d274f","1e2a579758c94ff6b1fe3d8ea6c67bba","e07cc28e7cb64dad8a0ed62ce238174b","6bf6155010504dc8bbce08d129df26ff","a16d4436048d46999130dd077d8904c7","f2a61a1fb9a84bbaa905d76e446ccd91","b044ee6da12a4d6f853da1fabc380f0b","328cc6ae15e44e0093f30f6c29df3e2b","954d20b1cbfe44a7b69bebbbfa3bd322","fc2016071d824ac587b3f77c3a860ef7","2017a070f23c484ab8c6867244d069ba","6481cda38de348558a5ca285b38eaf30","f047f3e344cf4d19a3e5f962d51422a8","0ee7c026b8bb495f85d02e7e53fd3fd9","f1b63fbc472348b8a4d65a5bf34105d3"]},"executionInfo":{"status":"ok","timestamp":1746237319871,"user_tz":240,"elapsed":1235,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"babc098e-51e3-4bbc-e78a-111b5d12ccb6"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f17a1027739648f182c9cf460eb67366"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16d4436048d46999130dd077d8904c7"}},"metadata":{}}],"source":["## DONOT CHANGE THE CODE\n","\n","processed_datasets = dataset.map(\n","    preprocess_function,\n","    batched=True,\n","    num_proc=1,\n","    remove_columns=dataset[\"train\"].column_names,\n","    load_from_cache_file=False,\n","    desc=\"Running tokenizer on dataset\",\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IlMkAYRhm6-t"},"outputs":[],"source":["train_dataset = processed_datasets[\"train\"]\n","eval_dataset = processed_datasets[\"test\"]\n","\n","\n","train_dataloader = DataLoader(\n","    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",")\n","eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmy80lgim9h2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746237327004,"user_tz":240,"elapsed":2401,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"26c41c6c-89d2-47a3-974d-7e45f49ce8a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0037\n","None\n"]}],"source":["model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n","model = get_peft_model(model, peft_config)\n","print(model.print_trainable_parameters())\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZxuczmbnrth"},"outputs":[],"source":["optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n","lr_scheduler = get_linear_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=0,\n","    num_training_steps=(len(train_dataloader) * num_epochs),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IVeZRGNknwW8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746237709538,"user_tz":240,"elapsed":382207,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"a6497e42-d3e0-4b7c-b2a1-17e49d3b5c3a"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.19it/s]\n","100%|██████████| 850/850 [00:30<00:00, 27.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=0: train_ppl=tensor(97269.8906, device='cuda:0') train_epoch_loss=tensor(11.4852, device='cuda:0') eval_ppl=tensor(28961.2246, device='cuda:0') eval_epoch_loss=tensor(10.2737, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.84it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.05it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=1: train_ppl=tensor(39.3209, device='cuda:0') train_epoch_loss=tensor(3.6718, device='cuda:0') eval_ppl=tensor(30158.4199, device='cuda:0') eval_epoch_loss=tensor(10.3142, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.81it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=2: train_ppl=tensor(13.3951, device='cuda:0') train_epoch_loss=tensor(2.5949, device='cuda:0') eval_ppl=tensor(20983.0332, device='cuda:0') eval_epoch_loss=tensor(9.9515, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.70it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=3: train_ppl=tensor(4.5059, device='cuda:0') train_epoch_loss=tensor(1.5054, device='cuda:0') eval_ppl=tensor(13316.3516, device='cuda:0') eval_epoch_loss=tensor(9.4967, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.80it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=4: train_ppl=tensor(2.2838, device='cuda:0') train_epoch_loss=tensor(0.8258, device='cuda:0') eval_ppl=tensor(12754.4590, device='cuda:0') eval_epoch_loss=tensor(9.4536, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.68it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.04it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=5: train_ppl=tensor(1.6411, device='cuda:0') train_epoch_loss=tensor(0.4953, device='cuda:0') eval_ppl=tensor(9251.7793, device='cuda:0') eval_epoch_loss=tensor(9.1326, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.67it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=6: train_ppl=tensor(1.4188, device='cuda:0') train_epoch_loss=tensor(0.3498, device='cuda:0') eval_ppl=tensor(10803.5713, device='cuda:0') eval_epoch_loss=tensor(9.2876, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.97it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=7: train_ppl=tensor(1.3661, device='cuda:0') train_epoch_loss=tensor(0.3119, device='cuda:0') eval_ppl=tensor(9476.9170, device='cuda:0') eval_epoch_loss=tensor(9.1566, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.93it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=8: train_ppl=tensor(1.2724, device='cuda:0') train_epoch_loss=tensor(0.2409, device='cuda:0') eval_ppl=tensor(13298.8252, device='cuda:0') eval_epoch_loss=tensor(9.4954, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.86it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=9: train_ppl=tensor(1.3482, device='cuda:0') train_epoch_loss=tensor(0.2988, device='cuda:0') eval_ppl=tensor(13481.9834, device='cuda:0') eval_epoch_loss=tensor(9.5091, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.73it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=10: train_ppl=tensor(1.2506, device='cuda:0') train_epoch_loss=tensor(0.2236, device='cuda:0') eval_ppl=tensor(12503.0537, device='cuda:0') eval_epoch_loss=tensor(9.4337, device='cuda:0')\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 13/13 [00:00<00:00, 13.85it/s]\n","100%|██████████| 850/850 [00:30<00:00, 28.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["epoch=11: train_ppl=tensor(1.2271, device='cuda:0') train_epoch_loss=tensor(0.2046, device='cuda:0') eval_ppl=tensor(13015.1992, device='cuda:0') eval_epoch_loss=tensor(9.4739, device='cuda:0')\n"]}],"source":["model = model.to(device)\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        outputs = model(**batch)\n","        loss = outputs.loss\n","        total_loss += loss.detach().float()\n","        loss.backward()\n","        optimizer.step()\n","        lr_scheduler.step()\n","        optimizer.zero_grad()\n","\n","    model.eval()\n","    eval_loss = 0\n","    eval_preds = []\n","    for step, batch in enumerate(tqdm(eval_dataloader)):\n","        batch = {k: v.to(device) for k, v in batch.items()}\n","        with torch.no_grad():\n","            outputs = model(**batch)\n","        loss = outputs.loss\n","        eval_loss += loss.detach().float()\n","        eval_preds.extend(\n","            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n","        )\n","\n","    eval_epoch_loss = eval_loss / len(eval_dataloader)\n","    eval_ppl = torch.exp(eval_epoch_loss)\n","    train_epoch_loss = total_loss / len(train_dataloader)\n","    train_ppl = torch.exp(train_epoch_loss)\n","    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n","    model.save_pretrained(\n","    f\"lora_{peft_config.num_virtual_tokens}vt_lr{lr}_ep{num_epochs}_bs{batch_size}\"\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rrP8_f4Bn0EP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1746237727240,"user_tz":240,"elapsed":85,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"37d49f23-e969-4486-9223-cb4384f6b6b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : complaint']\n"]}],"source":["# Test the Tuned Model\n","inputs = tokenizer(\n","    f'{text_column} : {\"@nationalgridus I have no water and the bill is current and paid. Can you do something about this?\"} Label : ',\n","    return_tensors=\"pt\",\n",")\n","\n","model.to(device)\n","\n","with torch.no_grad():\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","    outputs = model.generate(\n","        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n","    )\n","    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"g-sDhzaE1XFE"},"source":["# Train with 3 different Hyper-parameter Settings and explain the differences in output generation. Also try the best model with 5 different texts from https://huggingface.co/datasets/ought/raft/viewer?views%5B%5D=ade_corpus_v2_train\n","\n","\n","\n"]},{"cell_type":"markdown","source":["the first lora training, using heper :\n","\n","\"max_length = 64；lr = 3e-2；num_epochs = 10；batch_size = 8\",\n","the output is :\n","\n","['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : no complaintestado/no complaintestado/no complaint']\n","\n","the second lora training, using heper :\n","\n","\"max_length = 64；lr = 2e-2；num_epochs = 12；batch_size = 32\"\n","\n","['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : no complaintstderrstderrstderrstderrstderrstderrstderrstderr']\n","\n","the third lora training, using heper :\n","\n","\"max_length = 64；lr = 5e-2；num_epochs = 12；batch_size = 4\"\n","\n","['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : complaint']\n","\n","we can see the third lora is the best which has the correct label. High lr + small batch_size makes each step have more gradient noise and adds more regularization to get the best generated text. It is worth mentioning that the second set of hyperparameters with low lr + large batch_size makes learning extremely slow, so it is not effectively trained within 12 epochs, making the perplexity very high."],"metadata":{"id":"73lPq7Xd6X77"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UE29Odj_1Y1x"},"outputs":[],"source":[]},{"cell_type":"code","source":["!zip -r /content/lora_and_notebook.zip \\\n","   /content/lora_20vt_lr0.02_ep12_bs32 \\\n","   /content/lora_20vt_lr0.03_ep10_bs8 \\\n","   /content/lora_20vt_lr0.05_ep12_bs4 \\\n","   \"/content/drive/MyDrive/23789HW12/HW12.ipynb\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXdLsUpM56ju","executionInfo":{"status":"ok","timestamp":1746239690310,"user_tz":240,"elapsed":1310,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"e1b7c6cf-aa07-48ae-f60e-33dd9edc5c64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["  adding: content/lora_20vt_lr0.02_ep12_bs32/ (stored 0%)\n","  adding: content/lora_20vt_lr0.02_ep12_bs32/README.md (deflated 66%)\n","  adding: content/lora_20vt_lr0.02_ep12_bs32/adapter_config.json (deflated 44%)\n","  adding: content/lora_20vt_lr0.02_ep12_bs32/adapter_model.safetensors (deflated 9%)\n","  adding: content/lora_20vt_lr0.03_ep10_bs8/ (stored 0%)\n","  adding: content/lora_20vt_lr0.03_ep10_bs8/README.md (deflated 66%)\n","  adding: content/lora_20vt_lr0.03_ep10_bs8/adapter_config.json (deflated 44%)\n","  adding: content/lora_20vt_lr0.03_ep10_bs8/adapter_model.safetensors (deflated 8%)\n","  adding: content/lora_20vt_lr0.05_ep12_bs4/ (stored 0%)\n","  adding: content/lora_20vt_lr0.05_ep12_bs4/README.md (deflated 66%)\n","  adding: content/lora_20vt_lr0.05_ep12_bs4/adapter_config.json (deflated 44%)\n","  adding: content/lora_20vt_lr0.05_ep12_bs4/adapter_model.safetensors (deflated 8%)\n","  adding: content/drive/MyDrive/23789HW12/HW12.ipynb (deflated 77%)\n"]}]},{"cell_type":"code","source":["!apt-get install texlive texlive-xetex texlive-latex-extra pandoc"],"metadata":{"id":"DRbie-yA_zEq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --clear-output /content/drive/MyDrive/23789HW12/HW12.ipynb"],"metadata":{"id":"Qx_PE8afAyfg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!jupyter nbconvert --to pdf /content/drive/MyDrive/23789HW12/HW12.ipynb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gp_Rm6L8Dd_p","executionInfo":{"status":"ok","timestamp":1746241078483,"user_tz":240,"elapsed":5415,"user":{"displayName":"Qiyao Lin","userId":"17603726599540428157"}},"outputId":"93a38c55-f79c-4f08-baf0-a7f33975b8f0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[NbConvertApp] Converting notebook /content/drive/MyDrive/23789HW12/HW12.ipynb to pdf\n","[NbConvertApp] Writing 127343 bytes to notebook.tex\n","[NbConvertApp] Building PDF\n","[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n","[NbConvertApp] CRITICAL | xelatex failed: ['xelatex', 'notebook.tex', '-quiet']\n","b\"This is XeTeX, Version 3.141592653-2.6-0.999993 (TeX Live 2022/dev/Debian) (preloaded format=xelatex)\\n restricted \\\\write18 enabled.\\nentering extended mode\\n(./notebook.tex\\nLaTeX2e <2021-11-15> patch level 1\\nL3 programming layer <2022-01-21>\\n(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\\nDocument Class: article 2021/10/04 v1.4n Standard LaTeX document class\\n(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\\n(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\\nex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\\n) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/pgf.revision.tex)))\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\\nex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\\nf))))\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\\ntex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\\ntex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\\n.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\\nric.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\\ne.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\\n.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\\ntex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\\n.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\\ntex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\\nthmetics.code.tex)))\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\\nx)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\\ncode.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\\n.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\\nx)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\\node.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\\ns.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\\n)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\\nex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\\n.code.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\\nx)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\\n)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\\n\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\\ntex))\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\\nx)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\\node.tex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\\ntex)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code.tex))\\n)\\n(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\\n) (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\\n)\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\\n.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\\n.sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\\nLibrary (tcolorbox): 'tcbbreakable.code.tex' version '5.0.2'\\n(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdfcol.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/infwarerr/infwarerr.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/iftex/iftex.sty))))\\n(/usr/share/texlive/texmf-dist/tex/latex/parskip/parskip.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/kvoptions/kvoptions.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/kvsetkeys/kvsetkeys.sty)))\\n(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/iftex/ifvtex.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\\nFor additional information on amsmath, use the `?' option.\\n(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/l3packages/xparse/xparse.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-xetex.def\\n(|extractbb --version))))\\n(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.cfg)))\\n(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/base/fix-cm.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\\n(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math-table.tex)))\\n(/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/grffile/grffile.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\\n(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\\n(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\\n(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/kvdefinekeys/kvdefinekeys.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/pdfescape/pdfescape.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/hycolor/hycolor.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/letltxmacro/letltxmacro.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/auxhook/auxhook.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref-langpatches.def)\\n(/usr/share/texlive/texmf-dist/tex/generic/intcalc/intcalc.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/etexcmds/etexcmds.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\\n(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/bitset/bitset.sty\\n(/usr/share/texlive/texmf-dist/tex/generic/bigintcalc/bigintcalc.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/base/atbegshi-ltx.sty))\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\\n(/usr/share/texlive/texmf-dist/tex/generic/stringenc/stringenc.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/rerunfilecheck/rerunfilecheck.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/base/atveryend-ltx.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/uniquecounter/uniquecounter.sty)))\\n(/usr/share/texlive/texmf-dist/tex/latex/titling/titling.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/tools/array.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/tools/calc.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/soul/soul.sty)\\n(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\\nNo file notebook.aux.\\n(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\\n*geometry* driver: auto-detecting\\n*geometry* detected driver: xetex\\n*geometry* verbose mode - [ preamble ] result:\\n* driver: xetex\\n* paper: <default>\\n* layout: <same size as paper>\\n* layoutoffset:(h,v)=(0.0pt,0.0pt)\\n* modes: \\n* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\\n* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\\n* \\\\paperwidth=614.295pt\\n* \\\\paperheight=794.96999pt\\n* \\\\textwidth=469.75502pt\\n* \\\\textheight=650.43001pt\\n* \\\\oddsidemargin=0.0pt\\n* \\\\evensidemargin=0.0pt\\n* \\\\topmargin=-37.0pt\\n* \\\\headheight=12.0pt\\n* \\\\headsep=25.0pt\\n* \\\\topskip=11.0pt\\n* \\\\footskip=30.0pt\\n* \\\\marginparwidth=59.0pt\\n* \\\\marginparsep=10.0pt\\n* \\\\columnsep=10.0pt\\n* \\\\skip\\\\footins=10.0pt plus 4.0pt minus 2.0pt\\n* \\\\hoffset=0.0pt\\n* \\\\voffset=0.0pt\\n* \\\\mag=1000\\n* \\\\@twocolumnfalse\\n* \\\\@twosidefalse\\n* \\\\@mparswitchfalse\\n* \\\\@reversemarginfalse\\n* (1in=72.27pt=25.4mm, 1cm=28.453pt)\\n\\n(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\\n(/usr/share/texlive/texmf-dist/tex/latex/refcount/refcount.sty)\\n(/usr/share/texlive/texmf-dist/tex/generic/gettitlestring/gettitlestring.sty))\\n\\nPackage hyperref Warning: Rerun to get /PageLabels entry.\\n\\n(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd)\\n(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd)\\n(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\\n\\nLaTeX Warning: No \\\\author given.\\n\\n[1] [2] [3] [4] [5]\\nOverfull \\\\hbox (66.02701pt too wide) in paragraph at lines 759--762\\n[][]\\\\TU/lmtt/m/n/10.95 sentence_transformers.util.dot_score[][]|  \\n\\nOverfull \\\\hbox (74.81985pt too wide) in paragraph at lines 773--777\\n[][][]\\\\TU/lmtt/m/n/10.95 torch.nn.functional.cosine_similarity[][]\\\\TU/lmr/m/n/1\\n0.95 , \\n\\nOverfull \\\\hbox (25.78577pt too wide) in paragraph at lines 773--777\\n\\\\TU/lmtt/m/n/10.95 scipy.spatial.distance.cosine[][] \\n\\nOverfull \\\\hbox (54.52953pt too wide) in paragraph at lines 773--777\\n[][]\\\\TU/lmtt/m/n/10.95 sentence_transformers.util.cos_sim[][]|  \\n[6] [7]\\nUnderfull \\\\hbox (badness 3769) in paragraph at lines 901--903\\n[]\\\\TU/lmr/m/n/10.95 The good news is that as mentioned before, the outputs of o\\nur embedding model\\n[8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18]\\nUnderfull \\\\hbox (badness 10000) in paragraph at lines 1443--1443\\n[]\\\\TU/lmr/bx/n/12 Try the model with different number of examples in\\n\\nUnderfull \\\\hbox (badness 3029) in paragraph at lines 1443--1443\\n\\\\TU/lmr/bx/n/12 prompt_formatter(Multiple shot prompt engineering) and comment\\n\\nLaTeX Warning: File `https://heidloff.net/assets/img/2023/08/lora.png' not foun\\nd on input line 1500.\\n\\n! Unable to load picture or PDF file 'https://heidloff.net/assets/img/2023/08/l\\nora.png'.\\n<to be read again> \\n                   }\\nl.1500 ...eidloff.net/assets/img/2023/08/lora.png}\\n                                                  \\n? \\n! Emergency stop.\\n<to be read again> \\n                   }\\nl.1500 ...eidloff.net/assets/img/2023/08/lora.png}\\n                                                  \\nOutput written on notebook.pdf (18 pages).\\nTranscript written on notebook.log.\\n\"\n","Traceback (most recent call last):\n","  File \"/usr/local/bin/jupyter-nbconvert\", line 10, in <module>\n","    sys.exit(main())\n","             ^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/jupyter_core/application.py\", line 283, in launch_instance\n","    super().launch_instance(argv=argv, **kwargs)\n","  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 420, in start\n","    self.convert_notebooks()\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 597, in convert_notebooks\n","    self.convert_single_notebook(notebook_filename)\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 563, in convert_single_notebook\n","    output, resources = self.export_single_notebook(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/nbconvertapp.py\", line 487, in export_single_notebook\n","    output, resources = self.exporter.from_filename(\n","                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/templateexporter.py\", line 390, in from_filename\n","    return super().from_filename(filename, resources, **kw)  # type:ignore[return-value]\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 201, in from_filename\n","    return self.from_file(f, resources=resources, **kw)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/templateexporter.py\", line 396, in from_file\n","    return super().from_file(file_stream, resources, **kw)  # type:ignore[return-value]\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/exporter.py\", line 220, in from_file\n","    return self.from_notebook_node(\n","           ^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/pdf.py\", line 197, in from_notebook_node\n","    self.run_latex(tex_file)\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/pdf.py\", line 166, in run_latex\n","    return self.run_command(\n","           ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/nbconvert/exporters/pdf.py\", line 156, in run_command\n","    raise raise_on_failure(msg)\n","nbconvert.exporters.pdf.LatexFailed: PDF creating failed, captured latex output:\n","Failed to run \"['xelatex', 'notebook.tex', '-quiet']\" command:\n","This is XeTeX, Version 3.141592653-2.6-0.999993 (TeX Live 2022/dev/Debian) (preloaded format=xelatex)\n"," restricted \\write18 enabled.\n","entering extended mode\n","(./notebook.tex\n","LaTeX2e <2021-11-15> patch level 1\n","L3 programming layer <2022-01-21>\n","(/usr/share/texlive/texmf-dist/tex/latex/base/article.cls\n","Document Class: article 2021/10/04 v1.4n Standard LaTeX document class\n","(/usr/share/texlive/texmf-dist/tex/latex/base/size11.clo))\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcolorbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgf.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/utilities/pgfrcs.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-common-lists.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfutil-latex.def\n",") (/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfrcs.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/pgf.revision.tex)))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/basiclayer/pgfcore.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphicx.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/keyval.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/graphics.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics/trig.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/graphics.cfg)\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-def/xetex.def)))\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/systemlayer/pgfsys.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeys.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/utilities/pgfkeysfiltered.code.t\n","ex)) (/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgf.cfg)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-xetex.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-dvipdfmx.def\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsys-common-pdf.de\n","f))))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsyssoftpath.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/systemlayer/pgfsysprotocol.code.\n","tex)) (/usr/share/texlive/texmf-dist/tex/latex/xcolor/xcolor.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/graphics-cfg/color.cfg))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcore.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmath.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathcalc.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathutil.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathparser.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.code.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.basic.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.trigonomet\n","ric.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.random.cod\n","e.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.comparison\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.base.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.round.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.misc.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfunctions.integerari\n","thmetics.code.tex)))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfmathfloat.code.tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/math/pgfint.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepoints.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathconstruct.\n","code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathusage.code\n",".tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorescopes.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoregraphicstate.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransformation\n","s.code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorequick.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreobjects.code.t\n","ex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepathprocessing\n",".code.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorearrows.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreshade.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreimage.code.tex\n","\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoreexternal.code.\n","tex))\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorelayers.code.te\n","x)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcoretransparency.c\n","ode.tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorepatterns.code.\n","tex)\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/basiclayer/pgfcorerdf.code.tex))\n",")\n","(/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleshapes.code.tex\n",") (/usr/share/texlive/texmf-dist/tex/generic/pgf/modules/pgfmoduleplot.code.tex\n",")\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-0-65\n",".sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/pgf/compatibility/pgfcomp-version-1-18\n",".sty)) (/usr/share/texlive/texmf-dist/tex/latex/tools/verbatim.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/environ/environ.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/trimspaces/trimspaces.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/etoolbox/etoolbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tcolorbox/tcbbreakable.code.tex\n","Library (tcolorbox): 'tcbbreakable.code.tex' version '5.0.2'\n","(/usr/share/texlive/texmf-dist/tex/generic/oberdiek/pdfcol.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/ltxcmds/ltxcmds.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/infwarerr/infwarerr.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/iftex/iftex.sty))))\n","(/usr/share/texlive/texmf-dist/tex/latex/parskip/parskip.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/kvoptions/kvoptions.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/kvsetkeys/kvsetkeys.sty)))\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/caption3.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/float/float.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/enumerate.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/geometry/geometry.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/iftex/ifvtex.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsmath.sty\n","For additional information on amsmath, use the `?' option.\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amstext.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsgen.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsbsy.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/amsmath/amsopn.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amssymb.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/amsfonts.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/textcomp.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/upquote/upquote.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/eurosym/eurosym.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/l3packages/xparse/xparse.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/l3kernel/expl3.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/l3backend/l3backend-xetex.def\n","(|extractbb --version))))\n","(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec-xetex.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/fontenc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/fontspec/fontspec.cfg)))\n","(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math-xetex.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/l3packages/l3keys2e/l3keys2e.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/base/fix-cm.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/ts1enc.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/unicode-math/unicode-math-table.tex)))\n","(/usr/share/texlive/texmf-dist/tex/latex/fancyvrb/fancyvrb.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/grffile/grffile.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjustbox.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/xkeyval/xkeyval.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkeyval.tex\n","(/usr/share/texlive/texmf-dist/tex/generic/xkeyval/xkvutils.tex)))\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/adjcalc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/trimclip.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/collectbox/collectbox.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/adjustbox/tc-xetex.def))\n","(/usr/share/texlive/texmf-dist/tex/latex/ifoddpage/ifoddpage.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/varwidth/varwidth.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/pdftexcmds/pdftexcmds.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/kvdefinekeys/kvdefinekeys.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/pdfescape/pdfescape.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/hycolor/hycolor.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/letltxmacro/letltxmacro.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/auxhook/auxhook.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/pd1enc.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hyperref-langpatches.def)\n","(/usr/share/texlive/texmf-dist/tex/generic/intcalc/intcalc.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/etexcmds/etexcmds.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/puenc.def)\n","(/usr/share/texlive/texmf-dist/tex/latex/url/url.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/bitset/bitset.sty\n","(/usr/share/texlive/texmf-dist/tex/generic/bigintcalc/bigintcalc.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/base/atbegshi-ltx.sty))\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/hxetex.def\n","(/usr/share/texlive/texmf-dist/tex/generic/stringenc/stringenc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/rerunfilecheck/rerunfilecheck.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/base/atveryend-ltx.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/uniquecounter/uniquecounter.sty)))\n","(/usr/share/texlive/texmf-dist/tex/latex/titling/titling.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/longtable.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/booktabs/booktabs.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/array.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/tools/calc.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/enumitem/enumitem.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/ulem/ulem.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/soul/soul.sty)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/mathrsfs.sty)\n","No file notebook.aux.\n","(/usr/share/texlive/texmf-dist/tex/latex/caption/ltcaption.sty)\n","*geometry* driver: auto-detecting\n","*geometry* detected driver: xetex\n","*geometry* verbose mode - [ preamble ] result:\n","* driver: xetex\n","* paper: <default>\n","* layout: <same size as paper>\n","* layoutoffset:(h,v)=(0.0pt,0.0pt)\n","* modes: \n","* h-part:(L,W,R)=(72.26999pt, 469.75502pt, 72.26999pt)\n","* v-part:(T,H,B)=(72.26999pt, 650.43001pt, 72.26999pt)\n","* \\paperwidth=614.295pt\n","* \\paperheight=794.96999pt\n","* \\textwidth=469.75502pt\n","* \\textheight=650.43001pt\n","* \\oddsidemargin=0.0pt\n","* \\evensidemargin=0.0pt\n","* \\topmargin=-37.0pt\n","* \\headheight=12.0pt\n","* \\headsep=25.0pt\n","* \\topskip=11.0pt\n","* \\footskip=30.0pt\n","* \\marginparwidth=59.0pt\n","* \\marginparsep=10.0pt\n","* \\columnsep=10.0pt\n","* \\skip\\footins=10.0pt plus 4.0pt minus 2.0pt\n","* \\hoffset=0.0pt\n","* \\voffset=0.0pt\n","* \\mag=1000\n","* \\@twocolumnfalse\n","* \\@twosidefalse\n","* \\@mparswitchfalse\n","* \\@reversemarginfalse\n","* (1in=72.27pt=25.4mm, 1cm=28.453pt)\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/hyperref/nameref.sty\n","(/usr/share/texlive/texmf-dist/tex/latex/refcount/refcount.sty)\n","(/usr/share/texlive/texmf-dist/tex/generic/gettitlestring/gettitlestring.sty))\n","\n","Package hyperref Warning: Rerun to get /PageLabels entry.\n","\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsa.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/amsfonts/umsb.fd)\n","(/usr/share/texlive/texmf-dist/tex/latex/jknapltx/ursfs.fd)\n","\n","LaTeX Warning: No \\author given.\n","\n","[1] [2] [3] [4] [5]\n","Overfull \\hbox (66.02701pt too wide) in paragraph at lines 759--762\n","[][]\\TU/lmtt/m/n/10.95 sentence_transformers.util.dot_score[][]|  \n","\n","Overfull \\hbox (74.81985pt too wide) in paragraph at lines 773--777\n","[][][]\\TU/lmtt/m/n/10.95 torch.nn.functional.cosine_similarity[][]\\TU/lmr/m/n/1\n","0.95 , \n","\n","Overfull \\hbox (25.78577pt too wide) in paragraph at lines 773--777\n","\\TU/lmtt/m/n/10.95 scipy.spatial.distance.cosine[][] \n","\n","Overfull \\hbox (54.52953pt too wide) in paragraph at lines 773--777\n","[][]\\TU/lmtt/m/n/10.95 sentence_transformers.util.cos_sim[][]|  \n","[6] [7]\n","Underfull \\hbox (badness 3769) in paragraph at lines 901--903\n","[]\\TU/lmr/m/n/10.95 The good news is that as mentioned before, the outputs of o\n","ur embedding model\n","[8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [18]\n","Underfull \\hbox (badness 10000) in paragraph at lines 1443--1443\n","[]\\TU/lmr/bx/n/12 Try the model with different number of examples in\n","\n","Underfull \\hbox (badness 3029) in paragraph at lines 1443--1443\n","\\TU/lmr/bx/n/12 prompt_formatter(Multiple shot prompt engineering) and comment\n","\n","LaTeX Warning: File `https://heidloff.net/assets/img/2023/08/lora.png' not foun\n","d on input line 1500.\n","\n","! Unable to load picture or PDF file 'https://heidloff.net/assets/img/2023/08/l\n","ora.png'.\n","<to be read again> \n","                   }\n","l.1500 ...eidloff.net/assets/img/2023/08/lora.png}\n","                                                  \n","? \n","! Emergency stop.\n","<to be read again> \n","                   }\n","l.1500 ...eidloff.net/assets/img/2023/08/lora.png}\n","                                                  \n","Output written on notebook.pdf (18 pages).\n","Transcript written on notebook.log.\n","\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f17a1027739648f182c9cf460eb67366":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be8e36a1c1a54e239399e08e092310d8","IPY_MODEL_3032127b502b4ce8a6aca675b27b4f97","IPY_MODEL_4cc38d850ed241b98361438217b04654"],"layout":"IPY_MODEL_b4c38f2de5e14b7280273755632d7c1b"}},"be8e36a1c1a54e239399e08e092310d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b34ebf8b5ff4b9e905014b8f7879106","placeholder":"​","style":"IPY_MODEL_9f4378d22c3340858392f3434935f29c","value":"Running tokenizer on dataset: 100%"}},"3032127b502b4ce8a6aca675b27b4f97":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5b110924135d4255b7d9023c975d274f","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1e2a579758c94ff6b1fe3d8ea6c67bba","value":50}},"4cc38d850ed241b98361438217b04654":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e07cc28e7cb64dad8a0ed62ce238174b","placeholder":"​","style":"IPY_MODEL_6bf6155010504dc8bbce08d129df26ff","value":" 50/50 [00:00&lt;00:00, 1819.43 examples/s]"}},"b4c38f2de5e14b7280273755632d7c1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b34ebf8b5ff4b9e905014b8f7879106":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f4378d22c3340858392f3434935f29c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5b110924135d4255b7d9023c975d274f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e2a579758c94ff6b1fe3d8ea6c67bba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e07cc28e7cb64dad8a0ed62ce238174b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6bf6155010504dc8bbce08d129df26ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a16d4436048d46999130dd077d8904c7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2a61a1fb9a84bbaa905d76e446ccd91","IPY_MODEL_b044ee6da12a4d6f853da1fabc380f0b","IPY_MODEL_328cc6ae15e44e0093f30f6c29df3e2b"],"layout":"IPY_MODEL_954d20b1cbfe44a7b69bebbbfa3bd322"}},"f2a61a1fb9a84bbaa905d76e446ccd91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc2016071d824ac587b3f77c3a860ef7","placeholder":"​","style":"IPY_MODEL_2017a070f23c484ab8c6867244d069ba","value":"Running tokenizer on dataset: 100%"}},"b044ee6da12a4d6f853da1fabc380f0b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6481cda38de348558a5ca285b38eaf30","max":3399,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f047f3e344cf4d19a3e5f962d51422a8","value":3399}},"328cc6ae15e44e0093f30f6c29df3e2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ee7c026b8bb495f85d02e7e53fd3fd9","placeholder":"​","style":"IPY_MODEL_f1b63fbc472348b8a4d65a5bf34105d3","value":" 3399/3399 [00:00&lt;00:00, 6199.99 examples/s]"}},"954d20b1cbfe44a7b69bebbbfa3bd322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc2016071d824ac587b3f77c3a860ef7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2017a070f23c484ab8c6867244d069ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6481cda38de348558a5ca285b38eaf30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f047f3e344cf4d19a3e5f962d51422a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0ee7c026b8bb495f85d02e7e53fd3fd9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1b63fbc472348b8a4d65a5bf34105d3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e39d902d85744551a959f388ea132f60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d7a37d89c4ba4e83b7db2c898376e695","IPY_MODEL_022ae34744cf44e4b7a327e85db49297","IPY_MODEL_64fbf831e77d4ee0b2b0b4db133e212e"],"layout":"IPY_MODEL_bcb71150d9f1483fb75603684dc8d897"}},"d7a37d89c4ba4e83b7db2c898376e695":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_366bdae22a4047149ea7e85abcc1fb9a","placeholder":"​","style":"IPY_MODEL_84c5e2bf81154c92ba27f33da78238e3","value":"Fetching 12 files: 100%"}},"022ae34744cf44e4b7a327e85db49297":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c5e9d2634be4a7eb39e6883cef86df1","max":12,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0940e01a7b2a4922bd37669ccafc43b0","value":12}},"64fbf831e77d4ee0b2b0b4db133e212e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccc3d7a7c0be47f6a17c0f4c909c450b","placeholder":"​","style":"IPY_MODEL_8d99cd73d0e842e38e9ea56d3fb00483","value":" 12/12 [00:04&lt;00:00,  2.74it/s]"}},"bcb71150d9f1483fb75603684dc8d897":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"366bdae22a4047149ea7e85abcc1fb9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84c5e2bf81154c92ba27f33da78238e3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c5e9d2634be4a7eb39e6883cef86df1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0940e01a7b2a4922bd37669ccafc43b0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ccc3d7a7c0be47f6a17c0f4c909c450b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d99cd73d0e842e38e9ea56d3fb00483":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"864e532029864e5b9cc164df1d4a390d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6e406332a934913b66a89d8d3874764","IPY_MODEL_4835b396934d4cd79d3c9e58adf51a89","IPY_MODEL_92960dd91f784936bb68957af8eb2ff0"],"layout":"IPY_MODEL_bd3ffcbd17414f7da83b0fd57fc666b1"}},"e6e406332a934913b66a89d8d3874764":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a2de276b8f6d4475ba40a7fe9cc969db","placeholder":"​","style":"IPY_MODEL_a0d0f5fecafa4577bc03f30719b8ff2d","value":"Loading checkpoint shards: 100%"}},"4835b396934d4cd79d3c9e58adf51a89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0f8ad477d444faf9167802f7510fd47","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fa91a3ae28324f018ba6a50c00da948b","value":2}},"92960dd91f784936bb68957af8eb2ff0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6cdfb7c6a124ad3abbae41a31dec9f2","placeholder":"​","style":"IPY_MODEL_1abece0922ad4dbba211c4c87816b3d7","value":" 2/2 [00:02&lt;00:00,  2.47s/it]"}},"bd3ffcbd17414f7da83b0fd57fc666b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a2de276b8f6d4475ba40a7fe9cc969db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0d0f5fecafa4577bc03f30719b8ff2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0f8ad477d444faf9167802f7510fd47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa91a3ae28324f018ba6a50c00da948b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6cdfb7c6a124ad3abbae41a31dec9f2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1abece0922ad4dbba211c4c87816b3d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}